cuda memory allocated: 504626176
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1172, acc: 0.3625
loss: 1.0991, acc: 0.3937
loss: 1.0658, acc: 0.4396
loss: 1.0232, acc: 0.4859
loss: 0.9754, acc: 0.5275
loss: 0.9376, acc: 0.5615
loss: 0.9058, acc: 0.5821
loss: 0.8749, acc: 0.6000
loss: 0.8515, acc: 0.6201
loss: 0.8193, acc: 0.6381
loss: 0.7891, acc: 0.6557
loss: 0.7793, acc: 0.6599
loss: 0.7555, acc: 0.6726
loss: 0.7509, acc: 0.6768
> val_acc: 0.7680, val_f1: 0.7069
>> saved: peft/roberta_lora/lap14//acc_0.768_f1_0.7069_230828-0424
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5111, acc: 0.8281
loss: 0.5385, acc: 0.7946
loss: 0.5310, acc: 0.7760
loss: 0.5246, acc: 0.7886
loss: 0.5296, acc: 0.7812
loss: 0.5241, acc: 0.7836
loss: 0.5382, acc: 0.7754
loss: 0.5371, acc: 0.7753
loss: 0.5287, acc: 0.7812
loss: 0.5240, acc: 0.7832
loss: 0.5198, acc: 0.7873
loss: 0.5076, acc: 0.7928
loss: 0.5008, acc: 0.7979
loss: 0.5001, acc: 0.7980
loss: 0.4985, acc: 0.8003
> val_acc: 0.7853, val_f1: 0.7417
>> saved: peft/roberta_lora/lap14//acc_0.7853_f1_0.7417_230828-0424
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4808, acc: 0.8281
loss: 0.4892, acc: 0.8090
loss: 0.4432, acc: 0.8259
loss: 0.4302, acc: 0.8339
loss: 0.4241, acc: 0.8346
loss: 0.4300, acc: 0.8362
loss: 0.4408, acc: 0.8327
loss: 0.4434, acc: 0.8285
loss: 0.4480, acc: 0.8303
loss: 0.4370, acc: 0.8329
loss: 0.4474, acc: 0.8299
loss: 0.4638, acc: 0.8210
loss: 0.4643, acc: 0.8193
loss: 0.4694, acc: 0.8175
> val_acc: 0.7868, val_f1: 0.7276
>> saved: peft/roberta_lora/lap14//acc_0.7868_f1_0.7276_230828-0424
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2012, acc: 0.9688
loss: 0.2572, acc: 0.9219
loss: 0.3702, acc: 0.8750
loss: 0.3487, acc: 0.8789
loss: 0.3224, acc: 0.8824
loss: 0.3112, acc: 0.8846
loss: 0.3237, acc: 0.8841
loss: 0.3377, acc: 0.8776
loss: 0.3344, acc: 0.8780
loss: 0.3299, acc: 0.8798
loss: 0.3348, acc: 0.8762
loss: 0.3409, acc: 0.8717
loss: 0.3537, acc: 0.8642
loss: 0.3544, acc: 0.8622
loss: 0.3517, acc: 0.8640
> val_acc: 0.8103, val_f1: 0.7717
>> saved: peft/roberta_lora/lap14//acc_0.8103_f1_0.7717_230828-0425
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2257, acc: 0.9167
loss: 0.2623, acc: 0.8984
loss: 0.2742, acc: 0.8870
loss: 0.2999, acc: 0.8785
loss: 0.3093, acc: 0.8764
loss: 0.3170, acc: 0.8739
loss: 0.3204, acc: 0.8731
loss: 0.3143, acc: 0.8766
loss: 0.3210, acc: 0.8750
loss: 0.3216, acc: 0.8724
loss: 0.3286, acc: 0.8726
loss: 0.3453, acc: 0.8675
loss: 0.3543, acc: 0.8656
loss: 0.3575, acc: 0.8617
loss: 0.3525, acc: 0.8647
> val_acc: 0.7994, val_f1: 0.7573
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2060, acc: 0.9437
loss: 0.2634, acc: 0.9094
loss: 0.2639, acc: 0.9042
loss: 0.2654, acc: 0.9000
loss: 0.2590, acc: 0.9038
loss: 0.2870, acc: 0.8990
loss: 0.3176, acc: 0.8830
loss: 0.3280, acc: 0.8805
loss: 0.3295, acc: 0.8785
loss: 0.3290, acc: 0.8788
loss: 0.3266, acc: 0.8790
loss: 0.3199, acc: 0.8823
loss: 0.3183, acc: 0.8827
loss: 0.3149, acc: 0.8835
> val_acc: 0.8041, val_f1: 0.7595
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2907, acc: 0.9219
loss: 0.2377, acc: 0.9196
loss: 0.2211, acc: 0.9219
loss: 0.2519, acc: 0.9099
loss: 0.2592, acc: 0.9048
loss: 0.2495, acc: 0.9086
loss: 0.2468, acc: 0.9082
loss: 0.2528, acc: 0.9071
loss: 0.2507, acc: 0.9070
loss: 0.2479, acc: 0.9043
loss: 0.2422, acc: 0.9056
loss: 0.2489, acc: 0.9046
loss: 0.2508, acc: 0.9027
loss: 0.2577, acc: 0.9002
loss: 0.2562, acc: 0.9023
> val_acc: 0.8103, val_f1: 0.7742
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1510, acc: 0.9453
loss: 0.1342, acc: 0.9444
loss: 0.1535, acc: 0.9375
loss: 0.1672, acc: 0.9342
loss: 0.1779, acc: 0.9323
loss: 0.1820, acc: 0.9310
loss: 0.2022, acc: 0.9274
loss: 0.2055, acc: 0.9271
loss: 0.2064, acc: 0.9268
loss: 0.2133, acc: 0.9222
loss: 0.2124, acc: 0.9230
loss: 0.2126, acc: 0.9221
loss: 0.2122, acc: 0.9204
loss: 0.2107, acc: 0.9198
> val_acc: 0.8182, val_f1: 0.7865
>> saved: peft/roberta_lora/lap14//acc_0.8182_f1_0.7865_230828-0426
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1780, acc: 0.9062
loss: 0.2438, acc: 0.9010
loss: 0.2342, acc: 0.9062
loss: 0.2084, acc: 0.9141
loss: 0.2110, acc: 0.9152
loss: 0.2041, acc: 0.9207
loss: 0.2054, acc: 0.9173
loss: 0.2027, acc: 0.9201
loss: 0.2200, acc: 0.9184
loss: 0.2246, acc: 0.9164
loss: 0.2275, acc: 0.9142
loss: 0.2229, acc: 0.9163
loss: 0.2246, acc: 0.9170
loss: 0.2240, acc: 0.9186
loss: 0.2259, acc: 0.9173
> val_acc: 0.8025, val_f1: 0.7581
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1801, acc: 0.9167
loss: 0.1534, acc: 0.9375
loss: 0.1611, acc: 0.9351
loss: 0.1477, acc: 0.9410
loss: 0.1541, acc: 0.9389
loss: 0.1928, acc: 0.9286
loss: 0.1931, acc: 0.9309
loss: 0.1952, acc: 0.9309
loss: 0.1920, acc: 0.9310
loss: 0.1913, acc: 0.9310
loss: 0.1849, acc: 0.9340
loss: 0.1886, acc: 0.9321
loss: 0.1822, acc: 0.9350
loss: 0.1914, acc: 0.9315
loss: 0.1872, acc: 0.9326
> val_acc: 0.7900, val_f1: 0.7448
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2036, acc: 0.9125
loss: 0.1728, acc: 0.9281
loss: 0.1595, acc: 0.9354
loss: 0.1632, acc: 0.9422
loss: 0.1511, acc: 0.9450
loss: 0.1434, acc: 0.9490
loss: 0.1395, acc: 0.9509
loss: 0.1403, acc: 0.9500
loss: 0.1433, acc: 0.9479
loss: 0.1731, acc: 0.9381
loss: 0.1834, acc: 0.9335
loss: 0.1811, acc: 0.9344
loss: 0.1835, acc: 0.9337
loss: 0.1867, acc: 0.9335
> val_acc: 0.8041, val_f1: 0.7609
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.2363, acc: 0.9219
loss: 0.2015, acc: 0.9330
loss: 0.1868, acc: 0.9375
loss: 0.1988, acc: 0.9338
loss: 0.1757, acc: 0.9389
loss: 0.1990, acc: 0.9317
loss: 0.1995, acc: 0.9326
loss: 0.1932, acc: 0.9333
loss: 0.1952, acc: 0.9323
loss: 0.1959, acc: 0.9315
loss: 0.1898, acc: 0.9327
loss: 0.1842, acc: 0.9353
loss: 0.1834, acc: 0.9360
loss: 0.1796, acc: 0.9375
loss: 0.1765, acc: 0.9379
> val_acc: 0.8166, val_f1: 0.7812
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1410, acc: 0.9531
loss: 0.1354, acc: 0.9583
loss: 0.1464, acc: 0.9487
loss: 0.1453, acc: 0.9424
loss: 0.1314, acc: 0.9466
loss: 0.1352, acc: 0.9472
loss: 0.1543, acc: 0.9393
loss: 0.1736, acc: 0.9335
loss: 0.1835, acc: 0.9290
loss: 0.1791, acc: 0.9279
loss: 0.1883, acc: 0.9248
loss: 0.2033, acc: 0.9184
loss: 0.2099, acc: 0.9180
loss: 0.2095, acc: 0.9189
> val_acc: 0.7774, val_f1: 0.7146
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8182, test_f1: 0.7865
cuda memory allocated: 560526848
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1660, acc: 0.3375
loss: 1.1064, acc: 0.3719
loss: 1.0587, acc: 0.4292
loss: 1.0029, acc: 0.4875
loss: 0.9443, acc: 0.5413
loss: 0.9038, acc: 0.5687
loss: 0.8720, acc: 0.5938
loss: 0.8472, acc: 0.6156
loss: 0.8367, acc: 0.6194
loss: 0.8176, acc: 0.6331
loss: 0.7943, acc: 0.6443
loss: 0.7787, acc: 0.6542
loss: 0.7710, acc: 0.6601
loss: 0.7615, acc: 0.6656
> val_acc: 0.7727, val_f1: 0.7196
>> saved: peft/roberta_lora/lap14//acc_0.7727_f1_0.7196_230828-0428
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5423, acc: 0.7969
loss: 0.5575, acc: 0.7902
loss: 0.6175, acc: 0.7786
loss: 0.6201, acc: 0.7629
loss: 0.6023, acc: 0.7713
loss: 0.6017, acc: 0.7697
loss: 0.5897, acc: 0.7773
loss: 0.5854, acc: 0.7796
loss: 0.5933, acc: 0.7723
loss: 0.5880, acc: 0.7739
loss: 0.5748, acc: 0.7806
loss: 0.5667, acc: 0.7823
loss: 0.5613, acc: 0.7843
loss: 0.5532, acc: 0.7873
loss: 0.5538, acc: 0.7856
> val_acc: 0.7947, val_f1: 0.7502
>> saved: peft/roberta_lora/lap14//acc_0.7947_f1_0.7502_230828-0428
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5383, acc: 0.7656
loss: 0.4566, acc: 0.8090
loss: 0.5154, acc: 0.7879
loss: 0.5150, acc: 0.7944
loss: 0.5070, acc: 0.7969
loss: 0.4835, acc: 0.8114
loss: 0.4840, acc: 0.8097
loss: 0.4962, acc: 0.8021
loss: 0.4945, acc: 0.8054
loss: 0.4927, acc: 0.8087
loss: 0.4905, acc: 0.8079
loss: 0.4874, acc: 0.8077
loss: 0.4764, acc: 0.8140
loss: 0.4757, acc: 0.8148
> val_acc: 0.7947, val_f1: 0.7560
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2671, acc: 0.9375
loss: 0.3845, acc: 0.8490
loss: 0.4104, acc: 0.8466
loss: 0.3814, acc: 0.8516
loss: 0.3793, acc: 0.8542
loss: 0.3892, acc: 0.8570
loss: 0.3849, acc: 0.8569
loss: 0.3789, acc: 0.8576
loss: 0.3824, acc: 0.8537
loss: 0.3739, acc: 0.8594
loss: 0.3806, acc: 0.8560
loss: 0.3788, acc: 0.8566
loss: 0.3825, acc: 0.8540
loss: 0.3947, acc: 0.8475
loss: 0.3924, acc: 0.8490
> val_acc: 0.8088, val_f1: 0.7674
>> saved: peft/roberta_lora/lap14//acc_0.8088_f1_0.7674_230828-0429
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4050, acc: 0.8229
loss: 0.3545, acc: 0.8594
loss: 0.3486, acc: 0.8582
loss: 0.3491, acc: 0.8594
loss: 0.3486, acc: 0.8587
loss: 0.3492, acc: 0.8571
loss: 0.3519, acc: 0.8608
loss: 0.3446, acc: 0.8610
loss: 0.3588, acc: 0.8561
loss: 0.3529, acc: 0.8620
loss: 0.3468, acc: 0.8644
loss: 0.3469, acc: 0.8669
loss: 0.3485, acc: 0.8671
loss: 0.3525, acc: 0.8649
loss: 0.3469, acc: 0.8664
> val_acc: 0.8276, val_f1: 0.7997
>> saved: peft/roberta_lora/lap14//acc_0.8276_f1_0.7997_230828-0429
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2142, acc: 0.9313
loss: 0.2140, acc: 0.9250
loss: 0.2669, acc: 0.9062
loss: 0.3096, acc: 0.8859
loss: 0.2950, acc: 0.8938
loss: 0.2891, acc: 0.8917
loss: 0.2967, acc: 0.8875
loss: 0.3098, acc: 0.8812
loss: 0.3020, acc: 0.8847
loss: 0.3018, acc: 0.8812
loss: 0.3034, acc: 0.8818
loss: 0.3048, acc: 0.8797
loss: 0.3074, acc: 0.8793
loss: 0.3082, acc: 0.8790
> val_acc: 0.7931, val_f1: 0.7488
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2494, acc: 0.9219
loss: 0.2646, acc: 0.9018
loss: 0.2280, acc: 0.9089
loss: 0.2243, acc: 0.9099
loss: 0.2276, acc: 0.9134
loss: 0.2489, acc: 0.9086
loss: 0.2452, acc: 0.9082
loss: 0.2607, acc: 0.9037
loss: 0.2574, acc: 0.9025
loss: 0.2645, acc: 0.9009
loss: 0.2685, acc: 0.9002
loss: 0.2703, acc: 0.8997
loss: 0.2686, acc: 0.8997
loss: 0.2745, acc: 0.8974
loss: 0.2728, acc: 0.8963
> val_acc: 0.7962, val_f1: 0.7522
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2378, acc: 0.8906
loss: 0.2026, acc: 0.9132
loss: 0.2135, acc: 0.9062
loss: 0.2206, acc: 0.9079
loss: 0.2408, acc: 0.9036
loss: 0.2382, acc: 0.9041
loss: 0.2561, acc: 0.8998
loss: 0.2584, acc: 0.8990
loss: 0.2627, acc: 0.8963
loss: 0.2636, acc: 0.8986
loss: 0.2644, acc: 0.8999
loss: 0.2571, acc: 0.9025
loss: 0.2558, acc: 0.8999
loss: 0.2666, acc: 0.8954
> val_acc: 0.8088, val_f1: 0.7720
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1864, acc: 0.8750
loss: 0.3289, acc: 0.8750
loss: 0.3223, acc: 0.8864
loss: 0.3078, acc: 0.8809
loss: 0.2963, acc: 0.8884
loss: 0.2747, acc: 0.8942
loss: 0.2788, acc: 0.8931
loss: 0.2646, acc: 0.8967
loss: 0.2546, acc: 0.8986
loss: 0.2520, acc: 0.8967
loss: 0.2593, acc: 0.8946
loss: 0.2583, acc: 0.8956
loss: 0.2563, acc: 0.8975
loss: 0.2505, acc: 0.9006
loss: 0.2503, acc: 0.9005
> val_acc: 0.8229, val_f1: 0.7842
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2197, acc: 0.8958
loss: 0.1891, acc: 0.9297
loss: 0.1779, acc: 0.9399
loss: 0.2131, acc: 0.9253
loss: 0.2173, acc: 0.9253
loss: 0.2085, acc: 0.9275
loss: 0.2070, acc: 0.9261
loss: 0.2031, acc: 0.9276
loss: 0.2085, acc: 0.9251
loss: 0.2082, acc: 0.9258
loss: 0.2153, acc: 0.9263
loss: 0.2262, acc: 0.9219
loss: 0.2440, acc: 0.9132
loss: 0.2444, acc: 0.9131
loss: 0.2424, acc: 0.9154
> val_acc: 0.8072, val_f1: 0.7683
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8276, test_f1: 0.7997
cuda memory allocated: 543618560
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1435, acc: 0.4625
loss: 1.1215, acc: 0.4188
loss: 1.0767, acc: 0.4458
loss: 1.0132, acc: 0.4984
loss: 0.9661, acc: 0.5400
loss: 0.9123, acc: 0.5729
loss: 0.8662, acc: 0.6018
loss: 0.8225, acc: 0.6266
loss: 0.7964, acc: 0.6417
loss: 0.7787, acc: 0.6519
loss: 0.7695, acc: 0.6591
loss: 0.7566, acc: 0.6677
loss: 0.7545, acc: 0.6716
loss: 0.7476, acc: 0.6763
> val_acc: 0.7539, val_f1: 0.6770
>> saved: peft/roberta_lora/lap14//acc_0.7539_f1_0.677_230828-0431
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5151, acc: 0.7969
loss: 0.5071, acc: 0.7991
loss: 0.5232, acc: 0.7812
loss: 0.5209, acc: 0.7831
loss: 0.5590, acc: 0.7798
loss: 0.5572, acc: 0.7824
loss: 0.5424, acc: 0.7861
loss: 0.5294, acc: 0.7948
loss: 0.5385, acc: 0.7879
loss: 0.5387, acc: 0.7886
loss: 0.5316, acc: 0.7903
loss: 0.5396, acc: 0.7867
loss: 0.5339, acc: 0.7898
loss: 0.5329, acc: 0.7892
loss: 0.5411, acc: 0.7873
> val_acc: 0.7665, val_f1: 0.7140
>> saved: peft/roberta_lora/lap14//acc_0.7665_f1_0.714_230828-0432
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4799, acc: 0.8047
loss: 0.4624, acc: 0.8229
loss: 0.4574, acc: 0.8192
loss: 0.4716, acc: 0.8158
loss: 0.4526, acc: 0.8268
loss: 0.4602, acc: 0.8276
loss: 0.4514, acc: 0.8290
loss: 0.4456, acc: 0.8301
loss: 0.4556, acc: 0.8217
loss: 0.4604, acc: 0.8202
loss: 0.4715, acc: 0.8160
loss: 0.4726, acc: 0.8167
loss: 0.4732, acc: 0.8130
loss: 0.4699, acc: 0.8134
> val_acc: 0.7821, val_f1: 0.7390
>> saved: peft/roberta_lora/lap14//acc_0.7821_f1_0.739_230828-0432
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1944, acc: 0.9688
loss: 0.3620, acc: 0.8594
loss: 0.3470, acc: 0.8693
loss: 0.3606, acc: 0.8652
loss: 0.3648, acc: 0.8616
loss: 0.3873, acc: 0.8558
loss: 0.4077, acc: 0.8438
loss: 0.4107, acc: 0.8438
loss: 0.4113, acc: 0.8445
loss: 0.4026, acc: 0.8478
loss: 0.3922, acc: 0.8536
loss: 0.3886, acc: 0.8521
loss: 0.3988, acc: 0.8458
loss: 0.4023, acc: 0.8428
loss: 0.4119, acc: 0.8371
> val_acc: 0.7853, val_f1: 0.7272
>> saved: peft/roberta_lora/lap14//acc_0.7853_f1_0.7272_230828-0432
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4123, acc: 0.8542
loss: 0.3706, acc: 0.8750
loss: 0.3749, acc: 0.8726
loss: 0.3581, acc: 0.8733
loss: 0.3800, acc: 0.8628
loss: 0.3746, acc: 0.8605
loss: 0.3864, acc: 0.8542
loss: 0.3892, acc: 0.8528
loss: 0.3956, acc: 0.8510
loss: 0.4091, acc: 0.8431
loss: 0.4018, acc: 0.8467
loss: 0.3935, acc: 0.8491
loss: 0.3940, acc: 0.8467
loss: 0.3955, acc: 0.8447
loss: 0.3829, acc: 0.8492
> val_acc: 0.8213, val_f1: 0.7920
>> saved: peft/roberta_lora/lap14//acc_0.8213_f1_0.792_230828-0433
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3857, acc: 0.8500
loss: 0.3353, acc: 0.8688
loss: 0.3347, acc: 0.8646
loss: 0.3632, acc: 0.8531
loss: 0.3378, acc: 0.8700
loss: 0.3235, acc: 0.8771
loss: 0.3031, acc: 0.8830
loss: 0.3120, acc: 0.8805
loss: 0.3127, acc: 0.8778
loss: 0.3045, acc: 0.8788
loss: 0.3157, acc: 0.8744
loss: 0.3185, acc: 0.8745
loss: 0.3218, acc: 0.8721
loss: 0.3227, acc: 0.8728
> val_acc: 0.8088, val_f1: 0.7709
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2264, acc: 0.9062
loss: 0.1846, acc: 0.9375
loss: 0.2478, acc: 0.9010
loss: 0.2443, acc: 0.9044
loss: 0.2592, acc: 0.8977
loss: 0.2623, acc: 0.8935
loss: 0.2574, acc: 0.8975
loss: 0.2707, acc: 0.8894
loss: 0.2967, acc: 0.8728
loss: 0.3224, acc: 0.8670
loss: 0.3304, acc: 0.8678
loss: 0.3402, acc: 0.8640
loss: 0.3377, acc: 0.8659
loss: 0.3359, acc: 0.8671
loss: 0.3392, acc: 0.8659
> val_acc: 0.8135, val_f1: 0.7752
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3350, acc: 0.8750
loss: 0.2754, acc: 0.8889
loss: 0.2595, acc: 0.8973
loss: 0.2591, acc: 0.8997
loss: 0.2521, acc: 0.9023
loss: 0.2504, acc: 0.9030
loss: 0.2446, acc: 0.9044
loss: 0.2531, acc: 0.9038
loss: 0.2528, acc: 0.9034
loss: 0.2688, acc: 0.8992
loss: 0.2645, acc: 0.9016
loss: 0.2626, acc: 0.9031
loss: 0.2647, acc: 0.9014
loss: 0.2672, acc: 0.9013
> val_acc: 0.8072, val_f1: 0.7781
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1589, acc: 0.9375
loss: 0.1897, acc: 0.9323
loss: 0.1801, acc: 0.9403
loss: 0.2011, acc: 0.9336
loss: 0.2338, acc: 0.9226
loss: 0.2459, acc: 0.9183
loss: 0.2555, acc: 0.9103
loss: 0.2490, acc: 0.9141
loss: 0.2397, acc: 0.9154
loss: 0.2436, acc: 0.9137
loss: 0.2414, acc: 0.9148
loss: 0.2435, acc: 0.9146
loss: 0.2490, acc: 0.9109
loss: 0.2422, acc: 0.9129
loss: 0.2433, acc: 0.9124
> val_acc: 0.8166, val_f1: 0.7882
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3111, acc: 0.8854
loss: 0.2340, acc: 0.9023
loss: 0.2193, acc: 0.9014
loss: 0.2065, acc: 0.9132
loss: 0.2361, acc: 0.9022
loss: 0.2360, acc: 0.9018
loss: 0.2423, acc: 0.8996
loss: 0.2324, acc: 0.9046
loss: 0.2276, acc: 0.9092
loss: 0.2157, acc: 0.9147
loss: 0.2088, acc: 0.9192
loss: 0.2071, acc: 0.9203
loss: 0.2051, acc: 0.9206
loss: 0.2037, acc: 0.9219
loss: 0.2060, acc: 0.9214
> val_acc: 0.8103, val_f1: 0.7667
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8213, test_f1: 0.7920
>> test_acc: 0.8182, test_f1: 0.7865
>> test_acc: 0.8276, test_f1: 0.7997
>> test_acc: 0.8213, test_f1: 0.7920

>> avg_test_acc: 0.8224, avg_test_f1: 0.7927
>> max_test_acc: 0.8276, max_test_f1: 0.7997
