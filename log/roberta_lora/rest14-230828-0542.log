cuda memory allocated: 504626176
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0532, acc: 0.5750
loss: 1.0208, acc: 0.5938
loss: 1.0080, acc: 0.5771
loss: 0.9386, acc: 0.6078
loss: 0.8903, acc: 0.6362
loss: 0.8488, acc: 0.6562
loss: 0.8377, acc: 0.6571
loss: 0.8243, acc: 0.6609
loss: 0.8110, acc: 0.6681
loss: 0.7914, acc: 0.6725
loss: 0.7778, acc: 0.6767
loss: 0.7679, acc: 0.6844
loss: 0.7578, acc: 0.6889
loss: 0.7449, acc: 0.6924
loss: 0.7445, acc: 0.6921
loss: 0.7326, acc: 0.6965
loss: 0.7200, acc: 0.7029
loss: 0.7139, acc: 0.7045
loss: 0.7105, acc: 0.7043
loss: 0.7012, acc: 0.7091
loss: 0.6961, acc: 0.7107
loss: 0.6905, acc: 0.7139
> val_acc: 0.8357, val_f1: 0.7307
>> saved: peft/roberta_lora/rest14//acc_0.8357_f1_0.7307_230828-0543
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6267, acc: 0.7656
loss: 0.6083, acc: 0.7634
loss: 0.5756, acc: 0.7656
loss: 0.5983, acc: 0.7610
loss: 0.5855, acc: 0.7685
loss: 0.5692, acc: 0.7720
loss: 0.5760, acc: 0.7744
loss: 0.5762, acc: 0.7736
loss: 0.5631, acc: 0.7783
loss: 0.5472, acc: 0.7872
loss: 0.5387, acc: 0.7915
loss: 0.5275, acc: 0.7933
loss: 0.5227, acc: 0.7979
loss: 0.5172, acc: 0.7999
loss: 0.5118, acc: 0.8012
loss: 0.5094, acc: 0.8003
loss: 0.5092, acc: 0.7995
loss: 0.5073, acc: 0.8003
loss: 0.5026, acc: 0.8020
loss: 0.4988, acc: 0.8025
loss: 0.4979, acc: 0.8036
loss: 0.4966, acc: 0.8032
loss: 0.4956, acc: 0.8022
> val_acc: 0.8179, val_f1: 0.6566
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4439, acc: 0.8203
loss: 0.4790, acc: 0.8021
loss: 0.4566, acc: 0.8237
loss: 0.4600, acc: 0.8141
loss: 0.4686, acc: 0.8086
loss: 0.4688, acc: 0.8114
loss: 0.4534, acc: 0.8217
loss: 0.4556, acc: 0.8237
loss: 0.4487, acc: 0.8281
loss: 0.4448, acc: 0.8278
loss: 0.4528, acc: 0.8241
loss: 0.4454, acc: 0.8263
loss: 0.4529, acc: 0.8208
loss: 0.4490, acc: 0.8225
loss: 0.4473, acc: 0.8214
loss: 0.4476, acc: 0.8204
loss: 0.4468, acc: 0.8222
loss: 0.4437, acc: 0.8234
loss: 0.4487, acc: 0.8215
loss: 0.4496, acc: 0.8204
loss: 0.4503, acc: 0.8209
loss: 0.4464, acc: 0.8222
> val_acc: 0.8464, val_f1: 0.7410
>> saved: peft/roberta_lora/rest14//acc_0.8464_f1_0.741_230828-0544
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4331, acc: 0.8125
loss: 0.4967, acc: 0.8021
loss: 0.4462, acc: 0.8210
loss: 0.4158, acc: 0.8281
loss: 0.3895, acc: 0.8438
loss: 0.3788, acc: 0.8534
loss: 0.3731, acc: 0.8569
loss: 0.3834, acc: 0.8524
loss: 0.3867, acc: 0.8506
loss: 0.3942, acc: 0.8485
loss: 0.4016, acc: 0.8487
loss: 0.3997, acc: 0.8477
loss: 0.4115, acc: 0.8427
loss: 0.4073, acc: 0.8428
loss: 0.4060, acc: 0.8433
loss: 0.4012, acc: 0.8446
loss: 0.3974, acc: 0.8465
loss: 0.3947, acc: 0.8470
loss: 0.3879, acc: 0.8486
loss: 0.3836, acc: 0.8503
loss: 0.3822, acc: 0.8496
loss: 0.3808, acc: 0.8496
loss: 0.3868, acc: 0.8460
> val_acc: 0.8482, val_f1: 0.7554
>> saved: peft/roberta_lora/rest14//acc_0.8482_f1_0.7554_230828-0544
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2849, acc: 0.8958
loss: 0.2710, acc: 0.9023
loss: 0.2708, acc: 0.8966
loss: 0.2997, acc: 0.8924
loss: 0.3544, acc: 0.8736
loss: 0.3648, acc: 0.8672
loss: 0.3638, acc: 0.8646
loss: 0.3707, acc: 0.8643
loss: 0.3662, acc: 0.8670
loss: 0.3693, acc: 0.8665
loss: 0.3656, acc: 0.8667
loss: 0.3605, acc: 0.8696
loss: 0.3536, acc: 0.8700
loss: 0.3572, acc: 0.8681
loss: 0.3583, acc: 0.8660
loss: 0.3614, acc: 0.8646
loss: 0.3690, acc: 0.8626
loss: 0.3668, acc: 0.8640
loss: 0.3667, acc: 0.8639
loss: 0.3657, acc: 0.8629
loss: 0.3580, acc: 0.8662
loss: 0.3582, acc: 0.8649
loss: 0.3671, acc: 0.8609
> val_acc: 0.8438, val_f1: 0.7470
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.4180, acc: 0.8375
loss: 0.3672, acc: 0.8594
loss: 0.3437, acc: 0.8604
loss: 0.3324, acc: 0.8719
loss: 0.3417, acc: 0.8700
loss: 0.3460, acc: 0.8604
loss: 0.3596, acc: 0.8562
loss: 0.3516, acc: 0.8570
loss: 0.3455, acc: 0.8562
loss: 0.3395, acc: 0.8612
loss: 0.3375, acc: 0.8653
loss: 0.3364, acc: 0.8661
loss: 0.3380, acc: 0.8663
loss: 0.3394, acc: 0.8634
loss: 0.3365, acc: 0.8638
loss: 0.3350, acc: 0.8645
loss: 0.3302, acc: 0.8662
loss: 0.3235, acc: 0.8691
loss: 0.3278, acc: 0.8658
loss: 0.3331, acc: 0.8625
loss: 0.3314, acc: 0.8637
loss: 0.3297, acc: 0.8651
> val_acc: 0.8598, val_f1: 0.8025
>> saved: peft/roberta_lora/rest14//acc_0.8598_f1_0.8025_230828-0545
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.3214, acc: 0.8594
loss: 0.2989, acc: 0.8661
loss: 0.2788, acc: 0.8724
loss: 0.2791, acc: 0.8860
loss: 0.2944, acc: 0.8821
loss: 0.2982, acc: 0.8808
loss: 0.2987, acc: 0.8828
loss: 0.3106, acc: 0.8809
loss: 0.2970, acc: 0.8862
loss: 0.2962, acc: 0.8856
loss: 0.2857, acc: 0.8888
loss: 0.2978, acc: 0.8854
loss: 0.2978, acc: 0.8861
loss: 0.3011, acc: 0.8867
loss: 0.2974, acc: 0.8880
loss: 0.2956, acc: 0.8896
loss: 0.2896, acc: 0.8914
loss: 0.2958, acc: 0.8879
loss: 0.2956, acc: 0.8872
loss: 0.2913, acc: 0.8901
loss: 0.2955, acc: 0.8885
loss: 0.2955, acc: 0.8879
loss: 0.2969, acc: 0.8876
> val_acc: 0.8688, val_f1: 0.7982
>> saved: peft/roberta_lora/rest14//acc_0.8688_f1_0.7982_230828-0546
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2209, acc: 0.9375
loss: 0.2521, acc: 0.9062
loss: 0.2255, acc: 0.9152
loss: 0.2398, acc: 0.9062
loss: 0.2447, acc: 0.9036
loss: 0.2613, acc: 0.8987
loss: 0.2483, acc: 0.9053
loss: 0.2381, acc: 0.9087
loss: 0.2271, acc: 0.9134
loss: 0.2215, acc: 0.9139
loss: 0.2260, acc: 0.9120
loss: 0.2225, acc: 0.9121
loss: 0.2245, acc: 0.9106
loss: 0.2300, acc: 0.9085
loss: 0.2315, acc: 0.9105
loss: 0.2308, acc: 0.9110
loss: 0.2336, acc: 0.9096
loss: 0.2409, acc: 0.9066
loss: 0.2501, acc: 0.9023
loss: 0.2529, acc: 0.9012
loss: 0.2580, acc: 0.8999
loss: 0.2567, acc: 0.8997
> val_acc: 0.8759, val_f1: 0.8105
>> saved: peft/roberta_lora/rest14//acc_0.8759_f1_0.8105_230828-0546
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2957, acc: 0.9062
loss: 0.2508, acc: 0.9010
loss: 0.2486, acc: 0.9091
loss: 0.2333, acc: 0.9043
loss: 0.2415, acc: 0.9062
loss: 0.2290, acc: 0.9135
loss: 0.2252, acc: 0.9143
loss: 0.2266, acc: 0.9149
loss: 0.2193, acc: 0.9184
loss: 0.2173, acc: 0.9192
loss: 0.2144, acc: 0.9173
loss: 0.2147, acc: 0.9146
loss: 0.2248, acc: 0.9114
loss: 0.2287, acc: 0.9077
loss: 0.2333, acc: 0.9062
loss: 0.2316, acc: 0.9075
loss: 0.2330, acc: 0.9062
loss: 0.2427, acc: 0.9030
loss: 0.2418, acc: 0.9049
loss: 0.2439, acc: 0.9033
loss: 0.2437, acc: 0.9041
loss: 0.2422, acc: 0.9051
loss: 0.2472, acc: 0.9032
> val_acc: 0.8643, val_f1: 0.8148
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2381, acc: 0.8958
loss: 0.2236, acc: 0.9102
loss: 0.2288, acc: 0.9111
loss: 0.2402, acc: 0.9080
loss: 0.2414, acc: 0.9117
loss: 0.2442, acc: 0.9129
loss: 0.2411, acc: 0.9119
loss: 0.2376, acc: 0.9153
loss: 0.2334, acc: 0.9193
loss: 0.2419, acc: 0.9154
loss: 0.2421, acc: 0.9145
loss: 0.2431, acc: 0.9106
loss: 0.2398, acc: 0.9117
loss: 0.2429, acc: 0.9118
loss: 0.2442, acc: 0.9105
loss: 0.2483, acc: 0.9075
loss: 0.2448, acc: 0.9078
loss: 0.2498, acc: 0.9066
loss: 0.2488, acc: 0.9062
loss: 0.2522, acc: 0.9047
loss: 0.2539, acc: 0.9047
loss: 0.2596, acc: 0.9019
loss: 0.2590, acc: 0.9024
> val_acc: 0.8313, val_f1: 0.7199
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2146, acc: 0.9187
loss: 0.2008, acc: 0.9281
loss: 0.2135, acc: 0.9271
loss: 0.2304, acc: 0.9187
loss: 0.2659, acc: 0.9113
loss: 0.2617, acc: 0.9115
loss: 0.2556, acc: 0.9143
loss: 0.2559, acc: 0.9141
loss: 0.2631, acc: 0.9125
loss: 0.2831, acc: 0.9075
loss: 0.2779, acc: 0.9080
loss: 0.2741, acc: 0.9078
loss: 0.2693, acc: 0.9096
loss: 0.2698, acc: 0.9085
loss: 0.2749, acc: 0.9071
loss: 0.2768, acc: 0.9066
loss: 0.2729, acc: 0.9081
loss: 0.2711, acc: 0.9076
loss: 0.2757, acc: 0.9056
loss: 0.2710, acc: 0.9066
loss: 0.2717, acc: 0.9051
loss: 0.2720, acc: 0.9048
> val_acc: 0.8545, val_f1: 0.7742
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1599, acc: 0.9219
loss: 0.2608, acc: 0.8973
loss: 0.2490, acc: 0.8958
loss: 0.2495, acc: 0.8989
loss: 0.2317, acc: 0.9176
loss: 0.2139, acc: 0.9213
loss: 0.2173, acc: 0.9219
loss: 0.2184, acc: 0.9223
loss: 0.2292, acc: 0.9182
loss: 0.2166, acc: 0.9242
loss: 0.2148, acc: 0.9243
loss: 0.2054, acc: 0.9276
loss: 0.2016, acc: 0.9264
loss: 0.2025, acc: 0.9254
loss: 0.2049, acc: 0.9240
loss: 0.2070, acc: 0.9241
loss: 0.2088, acc: 0.9226
loss: 0.2074, acc: 0.9221
loss: 0.2106, acc: 0.9226
loss: 0.2126, acc: 0.9207
loss: 0.2128, acc: 0.9203
loss: 0.2168, acc: 0.9197
loss: 0.2195, acc: 0.9194
> val_acc: 0.8473, val_f1: 0.7751
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.2367, acc: 0.9062
loss: 0.2514, acc: 0.8854
loss: 0.2232, acc: 0.9085
loss: 0.2241, acc: 0.9095
loss: 0.2467, acc: 0.9049
loss: 0.2427, acc: 0.9062
loss: 0.2320, acc: 0.9108
loss: 0.2372, acc: 0.9071
loss: 0.2372, acc: 0.9070
loss: 0.2412, acc: 0.9069
loss: 0.2338, acc: 0.9080
loss: 0.2290, acc: 0.9089
loss: 0.2248, acc: 0.9121
loss: 0.2222, acc: 0.9121
loss: 0.2162, acc: 0.9139
loss: 0.2101, acc: 0.9153
loss: 0.2111, acc: 0.9156
loss: 0.2246, acc: 0.9115
loss: 0.2317, acc: 0.9089
loss: 0.2308, acc: 0.9091
loss: 0.2330, acc: 0.9078
loss: 0.2323, acc: 0.9077
> val_acc: 0.8473, val_f1: 0.7911
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8759, test_f1: 0.8105
cuda memory allocated: 560526848
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0899, acc: 0.5625
loss: 1.0589, acc: 0.5750
loss: 1.0325, acc: 0.5708
loss: 0.9852, acc: 0.5875
loss: 0.9334, acc: 0.6100
loss: 0.8908, acc: 0.6292
loss: 0.8598, acc: 0.6429
loss: 0.8310, acc: 0.6555
loss: 0.8049, acc: 0.6660
loss: 0.7881, acc: 0.6750
loss: 0.7719, acc: 0.6841
loss: 0.7627, acc: 0.6875
loss: 0.7487, acc: 0.6957
loss: 0.7358, acc: 0.7000
loss: 0.7275, acc: 0.7037
loss: 0.7232, acc: 0.7035
loss: 0.7152, acc: 0.7074
loss: 0.7122, acc: 0.7083
loss: 0.7127, acc: 0.7076
loss: 0.7060, acc: 0.7109
loss: 0.6999, acc: 0.7131
loss: 0.6879, acc: 0.7182
> val_acc: 0.8080, val_f1: 0.6648
>> saved: peft/roberta_lora/rest14//acc_0.808_f1_0.6648_230828-0550
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6819, acc: 0.7188
loss: 0.5694, acc: 0.7723
loss: 0.5770, acc: 0.7708
loss: 0.5466, acc: 0.7831
loss: 0.5497, acc: 0.7713
loss: 0.5413, acc: 0.7789
loss: 0.5430, acc: 0.7725
loss: 0.5424, acc: 0.7762
loss: 0.5499, acc: 0.7753
loss: 0.5380, acc: 0.7819
loss: 0.5450, acc: 0.7788
loss: 0.5422, acc: 0.7818
loss: 0.5357, acc: 0.7873
loss: 0.5363, acc: 0.7864
loss: 0.5320, acc: 0.7882
loss: 0.5303, acc: 0.7877
loss: 0.5263, acc: 0.7881
loss: 0.5309, acc: 0.7852
loss: 0.5270, acc: 0.7874
loss: 0.5244, acc: 0.7880
loss: 0.5222, acc: 0.7886
loss: 0.5147, acc: 0.7926
loss: 0.5088, acc: 0.7941
> val_acc: 0.8589, val_f1: 0.7846
>> saved: peft/roberta_lora/rest14//acc_0.8589_f1_0.7846_230828-0550
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4289, acc: 0.8281
loss: 0.3771, acc: 0.8611
loss: 0.4340, acc: 0.8415
loss: 0.4391, acc: 0.8388
loss: 0.4490, acc: 0.8255
loss: 0.4324, acc: 0.8330
loss: 0.4411, acc: 0.8217
loss: 0.4438, acc: 0.8189
loss: 0.4476, acc: 0.8175
loss: 0.4514, acc: 0.8151
loss: 0.4519, acc: 0.8142
loss: 0.4540, acc: 0.8125
loss: 0.4516, acc: 0.8130
loss: 0.4443, acc: 0.8175
loss: 0.4375, acc: 0.8201
loss: 0.4393, acc: 0.8200
loss: 0.4351, acc: 0.8237
loss: 0.4399, acc: 0.8206
loss: 0.4441, acc: 0.8185
loss: 0.4438, acc: 0.8194
loss: 0.4450, acc: 0.8182
loss: 0.4417, acc: 0.8214
> val_acc: 0.8500, val_f1: 0.7590
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2569, acc: 0.9062
loss: 0.3070, acc: 0.8802
loss: 0.3399, acc: 0.8665
loss: 0.3277, acc: 0.8711
loss: 0.3336, acc: 0.8690
loss: 0.3332, acc: 0.8666
loss: 0.3474, acc: 0.8619
loss: 0.3489, acc: 0.8602
loss: 0.3637, acc: 0.8567
loss: 0.3573, acc: 0.8573
loss: 0.3597, acc: 0.8572
loss: 0.3708, acc: 0.8555
loss: 0.3792, acc: 0.8499
loss: 0.3894, acc: 0.8456
loss: 0.3869, acc: 0.8468
loss: 0.3864, acc: 0.8479
loss: 0.3899, acc: 0.8465
loss: 0.3939, acc: 0.8445
loss: 0.3986, acc: 0.8407
loss: 0.3969, acc: 0.8411
loss: 0.3965, acc: 0.8416
loss: 0.4004, acc: 0.8370
loss: 0.3958, acc: 0.8398
> val_acc: 0.8598, val_f1: 0.7712
>> saved: peft/roberta_lora/rest14//acc_0.8598_f1_0.7712_230828-0551
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2732, acc: 0.8958
loss: 0.2831, acc: 0.9023
loss: 0.2783, acc: 0.8990
loss: 0.2932, acc: 0.8941
loss: 0.2945, acc: 0.8940
loss: 0.3091, acc: 0.8839
loss: 0.3133, acc: 0.8816
loss: 0.3168, acc: 0.8824
loss: 0.3199, acc: 0.8779
loss: 0.3257, acc: 0.8783
loss: 0.3301, acc: 0.8791
loss: 0.3278, acc: 0.8815
loss: 0.3243, acc: 0.8795
loss: 0.3245, acc: 0.8801
loss: 0.3256, acc: 0.8793
loss: 0.3308, acc: 0.8762
loss: 0.3361, acc: 0.8712
loss: 0.3368, acc: 0.8707
loss: 0.3354, acc: 0.8716
loss: 0.3348, acc: 0.8709
loss: 0.3347, acc: 0.8704
loss: 0.3348, acc: 0.8707
loss: 0.3375, acc: 0.8695
> val_acc: 0.8661, val_f1: 0.7903
>> saved: peft/roberta_lora/rest14//acc_0.8661_f1_0.7903_230828-0552
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2610, acc: 0.8875
loss: 0.2393, acc: 0.9031
loss: 0.2782, acc: 0.8917
loss: 0.2949, acc: 0.8797
loss: 0.2920, acc: 0.8825
loss: 0.2926, acc: 0.8844
loss: 0.2934, acc: 0.8821
loss: 0.2912, acc: 0.8844
loss: 0.2944, acc: 0.8826
loss: 0.2891, acc: 0.8831
loss: 0.2979, acc: 0.8812
loss: 0.2966, acc: 0.8828
loss: 0.2973, acc: 0.8832
loss: 0.3043, acc: 0.8808
loss: 0.3018, acc: 0.8829
loss: 0.3001, acc: 0.8828
loss: 0.3048, acc: 0.8798
loss: 0.3067, acc: 0.8795
loss: 0.3122, acc: 0.8783
loss: 0.3129, acc: 0.8775
loss: 0.3115, acc: 0.8777
loss: 0.3100, acc: 0.8787
> val_acc: 0.8625, val_f1: 0.7939
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2469, acc: 0.9375
loss: 0.2274, acc: 0.9152
loss: 0.2716, acc: 0.9036
loss: 0.2579, acc: 0.9044
loss: 0.2518, acc: 0.9020
loss: 0.2670, acc: 0.8877
loss: 0.2592, acc: 0.8906
loss: 0.2676, acc: 0.8885
loss: 0.2643, acc: 0.8914
loss: 0.2640, acc: 0.8936
loss: 0.2711, acc: 0.8912
loss: 0.2703, acc: 0.8898
loss: 0.2728, acc: 0.8906
loss: 0.2751, acc: 0.8913
loss: 0.2758, acc: 0.8924
loss: 0.2814, acc: 0.8896
loss: 0.2824, acc: 0.8880
loss: 0.2839, acc: 0.8865
loss: 0.2822, acc: 0.8865
loss: 0.2780, acc: 0.8889
loss: 0.2780, acc: 0.8885
loss: 0.2732, acc: 0.8902
loss: 0.2759, acc: 0.8903
> val_acc: 0.8277, val_f1: 0.7120
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3522, acc: 0.8359
loss: 0.2958, acc: 0.8611
loss: 0.2864, acc: 0.8795
loss: 0.2790, acc: 0.8816
loss: 0.2792, acc: 0.8815
loss: 0.2694, acc: 0.8847
loss: 0.2548, acc: 0.8915
loss: 0.2485, acc: 0.8966
loss: 0.2433, acc: 0.9013
loss: 0.2452, acc: 0.9043
loss: 0.2452, acc: 0.9039
loss: 0.2512, acc: 0.8994
loss: 0.2552, acc: 0.8975
loss: 0.2563, acc: 0.8972
loss: 0.2585, acc: 0.8974
loss: 0.2634, acc: 0.8932
loss: 0.2679, acc: 0.8925
loss: 0.2706, acc: 0.8897
loss: 0.2736, acc: 0.8886
loss: 0.2757, acc: 0.8870
loss: 0.2757, acc: 0.8879
loss: 0.2757, acc: 0.8893
> val_acc: 0.8688, val_f1: 0.8005
>> saved: peft/roberta_lora/rest14//acc_0.8688_f1_0.8005_230828-0553
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1932, acc: 0.9375
loss: 0.2583, acc: 0.9010
loss: 0.2196, acc: 0.9148
loss: 0.2068, acc: 0.9141
loss: 0.2055, acc: 0.9211
loss: 0.2032, acc: 0.9171
loss: 0.2223, acc: 0.9133
loss: 0.2168, acc: 0.9141
loss: 0.2360, acc: 0.9108
loss: 0.2399, acc: 0.9096
loss: 0.2424, acc: 0.9112
loss: 0.2396, acc: 0.9113
loss: 0.2386, acc: 0.9134
loss: 0.2442, acc: 0.9119
loss: 0.2442, acc: 0.9124
loss: 0.2459, acc: 0.9128
loss: 0.2425, acc: 0.9147
loss: 0.2424, acc: 0.9161
loss: 0.2433, acc: 0.9152
loss: 0.2451, acc: 0.9147
loss: 0.2507, acc: 0.9121
loss: 0.2534, acc: 0.9104
loss: 0.2614, acc: 0.9093
> val_acc: 0.8491, val_f1: 0.7475
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3280, acc: 0.8750
loss: 0.2299, acc: 0.9180
loss: 0.2242, acc: 0.9183
loss: 0.2254, acc: 0.9149
loss: 0.2300, acc: 0.9117
loss: 0.2371, acc: 0.9062
loss: 0.2404, acc: 0.9081
loss: 0.2318, acc: 0.9120
loss: 0.2306, acc: 0.9135
loss: 0.2323, acc: 0.9128
loss: 0.2271, acc: 0.9133
loss: 0.2294, acc: 0.9122
loss: 0.2245, acc: 0.9132
loss: 0.2299, acc: 0.9118
loss: 0.2347, acc: 0.9097
loss: 0.2384, acc: 0.9079
loss: 0.2369, acc: 0.9096
loss: 0.2391, acc: 0.9091
loss: 0.2373, acc: 0.9099
loss: 0.2350, acc: 0.9117
loss: 0.2347, acc: 0.9117
loss: 0.2356, acc: 0.9117
loss: 0.2343, acc: 0.9124
> val_acc: 0.8446, val_f1: 0.7721
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2843, acc: 0.8562
loss: 0.2308, acc: 0.8938
loss: 0.2268, acc: 0.9000
loss: 0.1999, acc: 0.9156
loss: 0.1880, acc: 0.9237
loss: 0.1970, acc: 0.9250
loss: 0.2121, acc: 0.9214
loss: 0.2143, acc: 0.9203
loss: 0.2183, acc: 0.9187
loss: 0.2252, acc: 0.9150
loss: 0.2327, acc: 0.9108
loss: 0.2359, acc: 0.9104
loss: 0.2410, acc: 0.9091
loss: 0.2415, acc: 0.9071
loss: 0.2405, acc: 0.9062
loss: 0.2411, acc: 0.9070
loss: 0.2398, acc: 0.9081
loss: 0.2436, acc: 0.9076
loss: 0.2442, acc: 0.9069
loss: 0.2443, acc: 0.9062
loss: 0.2437, acc: 0.9060
loss: 0.2455, acc: 0.9068
> val_acc: 0.8562, val_f1: 0.7789
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1432, acc: 0.9688
loss: 0.2366, acc: 0.9196
loss: 0.2222, acc: 0.9271
loss: 0.2072, acc: 0.9301
loss: 0.2086, acc: 0.9304
loss: 0.2162, acc: 0.9282
loss: 0.2100, acc: 0.9258
loss: 0.2110, acc: 0.9198
loss: 0.2033, acc: 0.9196
loss: 0.2086, acc: 0.9195
loss: 0.2074, acc: 0.9213
loss: 0.2076, acc: 0.9194
loss: 0.2090, acc: 0.9194
loss: 0.2110, acc: 0.9179
loss: 0.2158, acc: 0.9171
loss: 0.2247, acc: 0.9127
loss: 0.2240, acc: 0.9131
loss: 0.2235, acc: 0.9127
loss: 0.2230, acc: 0.9134
loss: 0.2297, acc: 0.9108
loss: 0.2332, acc: 0.9105
loss: 0.2313, acc: 0.9118
loss: 0.2351, acc: 0.9107
> val_acc: 0.8509, val_f1: 0.7734
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1463, acc: 0.9609
loss: 0.1421, acc: 0.9653
loss: 0.1301, acc: 0.9621
loss: 0.1404, acc: 0.9539
loss: 0.1447, acc: 0.9544
loss: 0.1492, acc: 0.9504
loss: 0.1647, acc: 0.9403
loss: 0.1721, acc: 0.9383
loss: 0.1755, acc: 0.9375
loss: 0.1837, acc: 0.9318
loss: 0.1832, acc: 0.9329
loss: 0.1876, acc: 0.9311
loss: 0.1862, acc: 0.9331
loss: 0.1893, acc: 0.9321
loss: 0.1930, acc: 0.9295
loss: 0.1926, acc: 0.9292
loss: 0.1959, acc: 0.9282
loss: 0.1955, acc: 0.9280
loss: 0.1966, acc: 0.9275
loss: 0.1985, acc: 0.9265
loss: 0.1988, acc: 0.9267
loss: 0.1989, acc: 0.9266
> val_acc: 0.8402, val_f1: 0.7347
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8688, test_f1: 0.8005
cuda memory allocated: 543618560
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 0.9574, acc: 0.6125
loss: 0.9563, acc: 0.6000
loss: 0.9300, acc: 0.6146
loss: 0.8644, acc: 0.6406
loss: 0.8267, acc: 0.6575
loss: 0.7975, acc: 0.6729
loss: 0.7884, acc: 0.6723
loss: 0.7766, acc: 0.6805
loss: 0.7651, acc: 0.6854
loss: 0.7499, acc: 0.6906
loss: 0.7517, acc: 0.6966
loss: 0.7445, acc: 0.6974
loss: 0.7382, acc: 0.7000
loss: 0.7236, acc: 0.7067
loss: 0.7132, acc: 0.7117
loss: 0.7017, acc: 0.7172
loss: 0.6952, acc: 0.7191
loss: 0.6862, acc: 0.7219
loss: 0.6839, acc: 0.7253
loss: 0.6797, acc: 0.7266
loss: 0.6713, acc: 0.7307
loss: 0.6672, acc: 0.7324
> val_acc: 0.8330, val_f1: 0.7173
>> saved: peft/roberta_lora/rest14//acc_0.833_f1_0.7173_230828-0556
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5020, acc: 0.8125
loss: 0.5195, acc: 0.7812
loss: 0.5368, acc: 0.7682
loss: 0.5081, acc: 0.7868
loss: 0.4966, acc: 0.7912
loss: 0.5031, acc: 0.7928
loss: 0.5169, acc: 0.7861
loss: 0.5196, acc: 0.7855
loss: 0.5115, acc: 0.7879
loss: 0.5021, acc: 0.7926
loss: 0.4891, acc: 0.7993
loss: 0.5052, acc: 0.7917
loss: 0.5105, acc: 0.7893
loss: 0.4989, acc: 0.7957
loss: 0.4984, acc: 0.7943
loss: 0.5031, acc: 0.7930
loss: 0.5014, acc: 0.7973
loss: 0.4988, acc: 0.7992
loss: 0.4982, acc: 0.7986
loss: 0.5006, acc: 0.7977
loss: 0.5006, acc: 0.7981
loss: 0.5014, acc: 0.7991
loss: 0.5059, acc: 0.7960
> val_acc: 0.8071, val_f1: 0.6575
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3616, acc: 0.8438
loss: 0.4389, acc: 0.8264
loss: 0.4217, acc: 0.8393
loss: 0.3946, acc: 0.8536
loss: 0.4180, acc: 0.8411
loss: 0.4158, acc: 0.8405
loss: 0.4038, acc: 0.8410
loss: 0.4066, acc: 0.8365
loss: 0.4038, acc: 0.8395
loss: 0.4105, acc: 0.8348
loss: 0.4278, acc: 0.8252
loss: 0.4264, acc: 0.8273
loss: 0.4222, acc: 0.8271
loss: 0.4210, acc: 0.8297
loss: 0.4226, acc: 0.8307
loss: 0.4330, acc: 0.8248
loss: 0.4280, acc: 0.8263
loss: 0.4275, acc: 0.8258
loss: 0.4252, acc: 0.8271
loss: 0.4213, acc: 0.8295
loss: 0.4221, acc: 0.8293
loss: 0.4236, acc: 0.8300
> val_acc: 0.8116, val_f1: 0.6616
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4460, acc: 0.8438
loss: 0.3943, acc: 0.8594
loss: 0.4009, acc: 0.8551
loss: 0.3860, acc: 0.8535
loss: 0.3864, acc: 0.8542
loss: 0.3904, acc: 0.8486
loss: 0.3916, acc: 0.8508
loss: 0.3845, acc: 0.8533
loss: 0.3842, acc: 0.8498
loss: 0.3884, acc: 0.8451
loss: 0.3809, acc: 0.8480
loss: 0.3674, acc: 0.8504
loss: 0.3682, acc: 0.8499
loss: 0.3653, acc: 0.8518
loss: 0.3716, acc: 0.8504
loss: 0.3674, acc: 0.8540
loss: 0.3754, acc: 0.8515
loss: 0.3774, acc: 0.8496
loss: 0.3760, acc: 0.8503
loss: 0.3748, acc: 0.8496
loss: 0.3746, acc: 0.8509
loss: 0.3815, acc: 0.8467
loss: 0.3854, acc: 0.8454
> val_acc: 0.8455, val_f1: 0.7479
>> saved: peft/roberta_lora/rest14//acc_0.8455_f1_0.7479_230828-0558
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3366, acc: 0.8750
loss: 0.3747, acc: 0.8516
loss: 0.3271, acc: 0.8678
loss: 0.3192, acc: 0.8767
loss: 0.2977, acc: 0.8859
loss: 0.3045, acc: 0.8862
loss: 0.3123, acc: 0.8769
loss: 0.3102, acc: 0.8791
loss: 0.3100, acc: 0.8779
loss: 0.3071, acc: 0.8796
loss: 0.3243, acc: 0.8715
loss: 0.3317, acc: 0.8702
loss: 0.3336, acc: 0.8686
loss: 0.3278, acc: 0.8722
loss: 0.3361, acc: 0.8694
loss: 0.3469, acc: 0.8658
loss: 0.3451, acc: 0.8675
loss: 0.3484, acc: 0.8675
loss: 0.3478, acc: 0.8679
loss: 0.3498, acc: 0.8658
loss: 0.3472, acc: 0.8680
loss: 0.3459, acc: 0.8689
loss: 0.3417, acc: 0.8711
> val_acc: 0.8545, val_f1: 0.7576
>> saved: peft/roberta_lora/rest14//acc_0.8545_f1_0.7576_230828-0558
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2514, acc: 0.9000
loss: 0.2650, acc: 0.9031
loss: 0.2521, acc: 0.9000
loss: 0.2320, acc: 0.9078
loss: 0.2306, acc: 0.9050
loss: 0.2349, acc: 0.9052
loss: 0.2278, acc: 0.9107
loss: 0.2439, acc: 0.9062
loss: 0.2765, acc: 0.8917
loss: 0.2810, acc: 0.8894
loss: 0.2826, acc: 0.8892
loss: 0.2812, acc: 0.8911
loss: 0.2820, acc: 0.8933
loss: 0.2865, acc: 0.8902
loss: 0.2947, acc: 0.8850
loss: 0.2922, acc: 0.8855
loss: 0.2969, acc: 0.8824
loss: 0.2981, acc: 0.8816
loss: 0.3013, acc: 0.8809
loss: 0.3044, acc: 0.8803
loss: 0.3051, acc: 0.8810
loss: 0.3066, acc: 0.8807
> val_acc: 0.8554, val_f1: 0.7801
>> saved: peft/roberta_lora/rest14//acc_0.8554_f1_0.7801_230828-0559
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2029, acc: 0.9219
loss: 0.2088, acc: 0.9286
loss: 0.2247, acc: 0.9297
loss: 0.2323, acc: 0.9246
loss: 0.2586, acc: 0.9105
loss: 0.2682, acc: 0.9039
loss: 0.2637, acc: 0.9053
loss: 0.2612, acc: 0.9046
loss: 0.2752, acc: 0.8973
loss: 0.2798, acc: 0.8956
loss: 0.2897, acc: 0.8894
loss: 0.2840, acc: 0.8925
loss: 0.2793, acc: 0.8947
loss: 0.2758, acc: 0.8965
loss: 0.2788, acc: 0.8971
loss: 0.2864, acc: 0.8916
loss: 0.2877, acc: 0.8906
loss: 0.2865, acc: 0.8908
loss: 0.2862, acc: 0.8913
loss: 0.2916, acc: 0.8898
loss: 0.2912, acc: 0.8900
loss: 0.2903, acc: 0.8908
loss: 0.2940, acc: 0.8906
> val_acc: 0.8473, val_f1: 0.7716
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2456, acc: 0.8984
loss: 0.2270, acc: 0.9167
loss: 0.2176, acc: 0.9219
loss: 0.2240, acc: 0.9211
loss: 0.2226, acc: 0.9193
loss: 0.2247, acc: 0.9127
loss: 0.2273, acc: 0.9053
loss: 0.2371, acc: 0.8990
loss: 0.2349, acc: 0.9020
loss: 0.2530, acc: 0.8960
loss: 0.2629, acc: 0.8924
loss: 0.2695, acc: 0.8914
loss: 0.2806, acc: 0.8892
loss: 0.2824, acc: 0.8872
loss: 0.2815, acc: 0.8868
loss: 0.2825, acc: 0.8849
loss: 0.2838, acc: 0.8850
loss: 0.2854, acc: 0.8834
loss: 0.2808, acc: 0.8850
loss: 0.2798, acc: 0.8867
loss: 0.2782, acc: 0.8879
loss: 0.2767, acc: 0.8890
> val_acc: 0.8670, val_f1: 0.7920
>> saved: peft/roberta_lora/rest14//acc_0.867_f1_0.792_230828-0600
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2439, acc: 0.8750
loss: 0.1484, acc: 0.9375
loss: 0.1476, acc: 0.9489
loss: 0.1887, acc: 0.9336
loss: 0.2054, acc: 0.9271
loss: 0.2070, acc: 0.9267
loss: 0.1889, acc: 0.9335
loss: 0.1963, acc: 0.9332
loss: 0.1987, acc: 0.9306
loss: 0.2038, acc: 0.9287
loss: 0.2079, acc: 0.9271
loss: 0.2064, acc: 0.9286
loss: 0.2021, acc: 0.9293
loss: 0.2017, acc: 0.9280
loss: 0.2015, acc: 0.9274
loss: 0.2088, acc: 0.9256
loss: 0.2155, acc: 0.9236
loss: 0.2238, acc: 0.9204
loss: 0.2295, acc: 0.9176
loss: 0.2280, acc: 0.9176
loss: 0.2269, acc: 0.9180
loss: 0.2319, acc: 0.9151
loss: 0.2326, acc: 0.9144
> val_acc: 0.8634, val_f1: 0.8067
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2640, acc: 0.8854
loss: 0.2351, acc: 0.9102
loss: 0.2397, acc: 0.9062
loss: 0.2344, acc: 0.9132
loss: 0.2231, acc: 0.9185
loss: 0.2325, acc: 0.9152
loss: 0.2302, acc: 0.9176
loss: 0.2143, acc: 0.9219
loss: 0.2126, acc: 0.9222
loss: 0.2111, acc: 0.9245
loss: 0.2096, acc: 0.9245
loss: 0.2087, acc: 0.9278
loss: 0.2081, acc: 0.9281
loss: 0.2188, acc: 0.9233
loss: 0.2200, acc: 0.9217
loss: 0.2243, acc: 0.9199
loss: 0.2196, acc: 0.9224
loss: 0.2213, acc: 0.9212
loss: 0.2281, acc: 0.9173
loss: 0.2271, acc: 0.9174
loss: 0.2317, acc: 0.9154
loss: 0.2320, acc: 0.9149
loss: 0.2303, acc: 0.9149
> val_acc: 0.8616, val_f1: 0.7786
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0973, acc: 0.9688
loss: 0.1615, acc: 0.9437
loss: 0.1579, acc: 0.9437
loss: 0.1694, acc: 0.9391
loss: 0.1653, acc: 0.9387
loss: 0.1702, acc: 0.9375
loss: 0.1604, acc: 0.9411
loss: 0.1596, acc: 0.9422
loss: 0.1716, acc: 0.9361
loss: 0.1773, acc: 0.9306
loss: 0.1758, acc: 0.9324
loss: 0.1719, acc: 0.9333
loss: 0.1692, acc: 0.9351
loss: 0.1732, acc: 0.9321
loss: 0.1737, acc: 0.9325
loss: 0.1839, acc: 0.9289
loss: 0.1868, acc: 0.9261
loss: 0.1884, acc: 0.9250
loss: 0.1891, acc: 0.9253
loss: 0.1906, acc: 0.9241
loss: 0.1960, acc: 0.9220
loss: 0.2017, acc: 0.9210
> val_acc: 0.8598, val_f1: 0.7890
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.2605, acc: 0.9062
loss: 0.1782, acc: 0.9420
loss: 0.1608, acc: 0.9453
loss: 0.1757, acc: 0.9412
loss: 0.1828, acc: 0.9403
loss: 0.2037, acc: 0.9317
loss: 0.2025, acc: 0.9297
loss: 0.2042, acc: 0.9291
loss: 0.2014, acc: 0.9301
loss: 0.2154, acc: 0.9249
loss: 0.2084, acc: 0.9261
loss: 0.2145, acc: 0.9243
loss: 0.2187, acc: 0.9219
loss: 0.2227, acc: 0.9207
loss: 0.2226, acc: 0.9188
loss: 0.2171, acc: 0.9213
loss: 0.2147, acc: 0.9226
loss: 0.2206, acc: 0.9206
loss: 0.2237, acc: 0.9185
loss: 0.2268, acc: 0.9162
loss: 0.2280, acc: 0.9151
loss: 0.2244, acc: 0.9162
loss: 0.2231, acc: 0.9166
> val_acc: 0.8554, val_f1: 0.7769
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.2877, acc: 0.9141
loss: 0.2422, acc: 0.9167
loss: 0.1980, acc: 0.9353
loss: 0.1901, acc: 0.9424
loss: 0.1872, acc: 0.9427
loss: 0.1926, acc: 0.9375
loss: 0.1839, acc: 0.9393
loss: 0.1845, acc: 0.9399
loss: 0.1810, acc: 0.9403
loss: 0.1869, acc: 0.9356
loss: 0.1957, acc: 0.9317
loss: 0.1997, acc: 0.9285
loss: 0.1936, acc: 0.9307
loss: 0.1899, acc: 0.9330
loss: 0.1972, acc: 0.9316
loss: 0.1985, acc: 0.9320
loss: 0.1945, acc: 0.9338
loss: 0.1954, acc: 0.9336
loss: 0.1979, acc: 0.9332
loss: 0.1997, acc: 0.9306
loss: 0.1973, acc: 0.9315
loss: 0.1960, acc: 0.9329
> val_acc: 0.8482, val_f1: 0.7730
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8670, test_f1: 0.7920
>> test_acc: 0.8759, test_f1: 0.8105
>> test_acc: 0.8688, test_f1: 0.8005
>> test_acc: 0.8670, test_f1: 0.7920

>> avg_test_acc: 0.8705, avg_test_f1: 0.8010
>> max_test_acc: 0.8759, max_test_f1: 0.8105
