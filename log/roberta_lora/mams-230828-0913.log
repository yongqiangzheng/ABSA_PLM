cuda memory allocated: 504626176
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1528, acc: 0.3937
loss: 1.1307, acc: 0.3688
loss: 1.1141, acc: 0.3875
loss: 1.0971, acc: 0.3953
loss: 1.0915, acc: 0.3975
loss: 1.0739, acc: 0.4135
loss: 1.0576, acc: 0.4223
loss: 1.0247, acc: 0.4453
loss: 1.0035, acc: 0.4674
loss: 0.9882, acc: 0.4763
loss: 0.9760, acc: 0.4858
loss: 0.9578, acc: 0.5062
loss: 0.9450, acc: 0.5178
loss: 0.9403, acc: 0.5277
loss: 0.9225, acc: 0.5408
loss: 0.9094, acc: 0.5496
loss: 0.8986, acc: 0.5581
loss: 0.8888, acc: 0.5646
loss: 0.8742, acc: 0.5720
loss: 0.8686, acc: 0.5766
loss: 0.8642, acc: 0.5807
loss: 0.8588, acc: 0.5847
loss: 0.8519, acc: 0.5913
loss: 0.8391, acc: 0.6005
loss: 0.8311, acc: 0.6062
loss: 0.8234, acc: 0.6120
loss: 0.8182, acc: 0.6167
loss: 0.8099, acc: 0.6230
loss: 0.8001, acc: 0.6293
loss: 0.7980, acc: 0.6321
loss: 0.7944, acc: 0.6347
loss: 0.7902, acc: 0.6381
loss: 0.7814, acc: 0.6436
loss: 0.7736, acc: 0.6482
loss: 0.7692, acc: 0.6514
loss: 0.7637, acc: 0.6547
loss: 0.7583, acc: 0.6583
loss: 0.7541, acc: 0.6607
loss: 0.7489, acc: 0.6635
loss: 0.7455, acc: 0.6652
loss: 0.7421, acc: 0.6672
loss: 0.7384, acc: 0.6699
loss: 0.7366, acc: 0.6715
loss: 0.7333, acc: 0.6727
loss: 0.7300, acc: 0.6740
loss: 0.7297, acc: 0.6745
loss: 0.7261, acc: 0.6765
loss: 0.7217, acc: 0.6788
loss: 0.7195, acc: 0.6801
loss: 0.7182, acc: 0.6811
loss: 0.7161, acc: 0.6828
loss: 0.7121, acc: 0.6850
loss: 0.7093, acc: 0.6867
loss: 0.7065, acc: 0.6876
loss: 0.7040, acc: 0.6887
loss: 0.7013, acc: 0.6904
loss: 0.6999, acc: 0.6917
loss: 0.6972, acc: 0.6937
loss: 0.6948, acc: 0.6944
loss: 0.6930, acc: 0.6954
loss: 0.6906, acc: 0.6965
loss: 0.6913, acc: 0.6970
loss: 0.6890, acc: 0.6984
loss: 0.6850, acc: 0.7004
loss: 0.6823, acc: 0.7020
loss: 0.6803, acc: 0.7031
loss: 0.6790, acc: 0.7037
loss: 0.6779, acc: 0.7051
loss: 0.6763, acc: 0.7066
loss: 0.6748, acc: 0.7073
> val_acc: 0.7913, val_f1: 0.7805
>> saved: peft/roberta_lora/mams//acc_0.7913_f1_0.7805_230828-0915
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5028, acc: 0.7875
loss: 0.5154, acc: 0.8000
loss: 0.4901, acc: 0.8063
loss: 0.4640, acc: 0.8172
loss: 0.4938, acc: 0.8075
loss: 0.5049, acc: 0.8031
loss: 0.4785, acc: 0.8161
loss: 0.4926, acc: 0.8094
loss: 0.4936, acc: 0.8069
loss: 0.5059, acc: 0.7994
loss: 0.5117, acc: 0.7989
loss: 0.5122, acc: 0.7990
loss: 0.5064, acc: 0.8005
loss: 0.5101, acc: 0.7960
loss: 0.5034, acc: 0.7983
loss: 0.5038, acc: 0.7977
loss: 0.5017, acc: 0.7967
loss: 0.5041, acc: 0.7958
loss: 0.5008, acc: 0.7964
loss: 0.4960, acc: 0.7997
loss: 0.4974, acc: 0.7985
loss: 0.4933, acc: 0.8009
loss: 0.5002, acc: 0.7989
loss: 0.4981, acc: 0.8003
loss: 0.5004, acc: 0.8005
loss: 0.5006, acc: 0.8007
loss: 0.4978, acc: 0.8021
loss: 0.5020, acc: 0.7984
loss: 0.5045, acc: 0.7976
loss: 0.5019, acc: 0.7992
loss: 0.5033, acc: 0.7984
loss: 0.5024, acc: 0.7977
loss: 0.5065, acc: 0.7953
loss: 0.5092, acc: 0.7943
loss: 0.5081, acc: 0.7950
loss: 0.5086, acc: 0.7946
loss: 0.5114, acc: 0.7929
loss: 0.5131, acc: 0.7926
loss: 0.5120, acc: 0.7928
loss: 0.5120, acc: 0.7923
loss: 0.5133, acc: 0.7918
loss: 0.5124, acc: 0.7921
loss: 0.5110, acc: 0.7930
loss: 0.5098, acc: 0.7939
loss: 0.5087, acc: 0.7940
loss: 0.5070, acc: 0.7946
loss: 0.5055, acc: 0.7960
loss: 0.5054, acc: 0.7964
loss: 0.5065, acc: 0.7960
loss: 0.5053, acc: 0.7963
loss: 0.5052, acc: 0.7964
loss: 0.5071, acc: 0.7960
loss: 0.5069, acc: 0.7965
loss: 0.5050, acc: 0.7972
loss: 0.5050, acc: 0.7974
loss: 0.5042, acc: 0.7977
loss: 0.5042, acc: 0.7980
loss: 0.5035, acc: 0.7984
loss: 0.5037, acc: 0.7986
loss: 0.5031, acc: 0.7990
loss: 0.5002, acc: 0.8001
loss: 0.5004, acc: 0.8003
loss: 0.5014, acc: 0.8002
loss: 0.5018, acc: 0.7994
loss: 0.5024, acc: 0.7998
loss: 0.5031, acc: 0.7991
loss: 0.5023, acc: 0.7994
loss: 0.5023, acc: 0.7994
loss: 0.5007, acc: 0.8005
loss: 0.5018, acc: 0.8009
> val_acc: 0.8056, val_f1: 0.7973
>> saved: peft/roberta_lora/mams//acc_0.8056_f1_0.7973_230828-0916
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4150, acc: 0.8438
loss: 0.4658, acc: 0.8125
loss: 0.4562, acc: 0.8208
loss: 0.4369, acc: 0.8281
loss: 0.4377, acc: 0.8300
loss: 0.4474, acc: 0.8260
loss: 0.4552, acc: 0.8241
loss: 0.4421, acc: 0.8313
loss: 0.4314, acc: 0.8354
loss: 0.4521, acc: 0.8269
loss: 0.4555, acc: 0.8261
loss: 0.4651, acc: 0.8219
loss: 0.4579, acc: 0.8255
loss: 0.4561, acc: 0.8250
loss: 0.4577, acc: 0.8263
loss: 0.4534, acc: 0.8289
loss: 0.4563, acc: 0.8265
loss: 0.4553, acc: 0.8278
loss: 0.4545, acc: 0.8273
loss: 0.4526, acc: 0.8281
loss: 0.4557, acc: 0.8274
loss: 0.4578, acc: 0.8256
loss: 0.4566, acc: 0.8264
loss: 0.4553, acc: 0.8266
loss: 0.4563, acc: 0.8260
loss: 0.4542, acc: 0.8267
loss: 0.4574, acc: 0.8250
loss: 0.4600, acc: 0.8232
loss: 0.4591, acc: 0.8237
loss: 0.4586, acc: 0.8237
loss: 0.4610, acc: 0.8224
loss: 0.4571, acc: 0.8238
loss: 0.4583, acc: 0.8233
loss: 0.4573, acc: 0.8232
loss: 0.4609, acc: 0.8213
loss: 0.4633, acc: 0.8205
loss: 0.4614, acc: 0.8216
loss: 0.4583, acc: 0.8230
loss: 0.4575, acc: 0.8234
loss: 0.4564, acc: 0.8239
loss: 0.4579, acc: 0.8229
loss: 0.4582, acc: 0.8229
loss: 0.4568, acc: 0.8235
loss: 0.4565, acc: 0.8244
loss: 0.4559, acc: 0.8250
loss: 0.4570, acc: 0.8243
loss: 0.4568, acc: 0.8250
loss: 0.4545, acc: 0.8255
loss: 0.4569, acc: 0.8246
loss: 0.4569, acc: 0.8247
loss: 0.4552, acc: 0.8257
loss: 0.4549, acc: 0.8257
loss: 0.4577, acc: 0.8250
loss: 0.4563, acc: 0.8262
loss: 0.4585, acc: 0.8245
loss: 0.4573, acc: 0.8252
loss: 0.4559, acc: 0.8257
loss: 0.4566, acc: 0.8255
loss: 0.4570, acc: 0.8252
loss: 0.4564, acc: 0.8260
loss: 0.4568, acc: 0.8259
loss: 0.4583, acc: 0.8252
loss: 0.4587, acc: 0.8251
loss: 0.4586, acc: 0.8251
loss: 0.4591, acc: 0.8249
loss: 0.4597, acc: 0.8245
loss: 0.4600, acc: 0.8242
loss: 0.4620, acc: 0.8236
loss: 0.4626, acc: 0.8232
loss: 0.4625, acc: 0.8232
> val_acc: 0.8101, val_f1: 0.8005
>> saved: peft/roberta_lora/mams//acc_0.8101_f1_0.8005_230828-0918
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4137, acc: 0.8750
loss: 0.3787, acc: 0.8594
loss: 0.3950, acc: 0.8521
loss: 0.3810, acc: 0.8516
loss: 0.4049, acc: 0.8337
loss: 0.4146, acc: 0.8271
loss: 0.4143, acc: 0.8286
loss: 0.4403, acc: 0.8219
loss: 0.4284, acc: 0.8292
loss: 0.4248, acc: 0.8294
loss: 0.4342, acc: 0.8301
loss: 0.4339, acc: 0.8313
loss: 0.4313, acc: 0.8337
loss: 0.4290, acc: 0.8326
loss: 0.4255, acc: 0.8354
loss: 0.4267, acc: 0.8340
loss: 0.4274, acc: 0.8335
loss: 0.4288, acc: 0.8319
loss: 0.4266, acc: 0.8332
loss: 0.4265, acc: 0.8344
loss: 0.4286, acc: 0.8324
loss: 0.4319, acc: 0.8313
loss: 0.4355, acc: 0.8302
loss: 0.4348, acc: 0.8310
loss: 0.4342, acc: 0.8315
loss: 0.4311, acc: 0.8317
loss: 0.4311, acc: 0.8310
loss: 0.4319, acc: 0.8310
loss: 0.4310, acc: 0.8325
loss: 0.4321, acc: 0.8327
loss: 0.4317, acc: 0.8325
loss: 0.4301, acc: 0.8328
loss: 0.4297, acc: 0.8328
loss: 0.4270, acc: 0.8331
loss: 0.4254, acc: 0.8327
loss: 0.4238, acc: 0.8337
loss: 0.4239, acc: 0.8333
loss: 0.4250, acc: 0.8324
loss: 0.4269, acc: 0.8322
loss: 0.4276, acc: 0.8311
loss: 0.4282, acc: 0.8311
loss: 0.4278, acc: 0.8321
loss: 0.4266, acc: 0.8323
loss: 0.4281, acc: 0.8322
loss: 0.4286, acc: 0.8328
loss: 0.4285, acc: 0.8326
loss: 0.4293, acc: 0.8320
loss: 0.4291, acc: 0.8320
loss: 0.4303, acc: 0.8316
loss: 0.4306, acc: 0.8313
loss: 0.4332, acc: 0.8299
loss: 0.4329, acc: 0.8303
loss: 0.4322, acc: 0.8305
loss: 0.4335, acc: 0.8296
loss: 0.4357, acc: 0.8292
loss: 0.4354, acc: 0.8297
loss: 0.4358, acc: 0.8300
loss: 0.4381, acc: 0.8291
loss: 0.4384, acc: 0.8293
loss: 0.4391, acc: 0.8285
loss: 0.4384, acc: 0.8287
loss: 0.4376, acc: 0.8287
loss: 0.4399, acc: 0.8280
loss: 0.4405, acc: 0.8275
loss: 0.4404, acc: 0.8276
loss: 0.4401, acc: 0.8275
loss: 0.4404, acc: 0.8272
loss: 0.4408, acc: 0.8271
loss: 0.4420, acc: 0.8262
loss: 0.4422, acc: 0.8257
> val_acc: 0.8146, val_f1: 0.8042
>> saved: peft/roberta_lora/mams//acc_0.8146_f1_0.8042_230828-0919
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4675, acc: 0.8688
loss: 0.4187, acc: 0.8750
loss: 0.4364, acc: 0.8562
loss: 0.4334, acc: 0.8547
loss: 0.4404, acc: 0.8538
loss: 0.4423, acc: 0.8500
loss: 0.4379, acc: 0.8500
loss: 0.4298, acc: 0.8531
loss: 0.4314, acc: 0.8528
loss: 0.4349, acc: 0.8500
loss: 0.4367, acc: 0.8455
loss: 0.4273, acc: 0.8490
loss: 0.4222, acc: 0.8505
loss: 0.4240, acc: 0.8496
loss: 0.4218, acc: 0.8504
loss: 0.4234, acc: 0.8500
loss: 0.4230, acc: 0.8485
loss: 0.4151, acc: 0.8517
loss: 0.4206, acc: 0.8503
loss: 0.4226, acc: 0.8491
loss: 0.4241, acc: 0.8458
loss: 0.4289, acc: 0.8438
loss: 0.4319, acc: 0.8408
loss: 0.4314, acc: 0.8401
loss: 0.4315, acc: 0.8395
loss: 0.4288, acc: 0.8401
loss: 0.4301, acc: 0.8394
loss: 0.4292, acc: 0.8384
loss: 0.4284, acc: 0.8388
loss: 0.4271, acc: 0.8392
loss: 0.4263, acc: 0.8395
loss: 0.4268, acc: 0.8391
loss: 0.4246, acc: 0.8403
loss: 0.4241, acc: 0.8406
loss: 0.4228, acc: 0.8414
loss: 0.4216, acc: 0.8410
loss: 0.4214, acc: 0.8409
loss: 0.4231, acc: 0.8403
loss: 0.4219, acc: 0.8409
loss: 0.4233, acc: 0.8402
loss: 0.4250, acc: 0.8393
loss: 0.4211, acc: 0.8412
loss: 0.4219, acc: 0.8416
loss: 0.4245, acc: 0.8408
loss: 0.4267, acc: 0.8397
loss: 0.4259, acc: 0.8398
loss: 0.4241, acc: 0.8402
loss: 0.4255, acc: 0.8402
loss: 0.4263, acc: 0.8401
loss: 0.4245, acc: 0.8405
loss: 0.4227, acc: 0.8406
loss: 0.4239, acc: 0.8398
loss: 0.4254, acc: 0.8397
loss: 0.4235, acc: 0.8399
loss: 0.4225, acc: 0.8400
loss: 0.4236, acc: 0.8396
loss: 0.4232, acc: 0.8397
loss: 0.4226, acc: 0.8402
loss: 0.4232, acc: 0.8398
loss: 0.4228, acc: 0.8402
loss: 0.4222, acc: 0.8402
loss: 0.4236, acc: 0.8395
loss: 0.4243, acc: 0.8396
loss: 0.4243, acc: 0.8395
loss: 0.4255, acc: 0.8385
loss: 0.4243, acc: 0.8391
loss: 0.4256, acc: 0.8386
loss: 0.4250, acc: 0.8387
loss: 0.4245, acc: 0.8391
loss: 0.4249, acc: 0.8386
> val_acc: 0.8206, val_f1: 0.8137
>> saved: peft/roberta_lora/mams//acc_0.8206_f1_0.8137_230828-0921
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3878, acc: 0.8250
loss: 0.3787, acc: 0.8469
loss: 0.3703, acc: 0.8521
loss: 0.3636, acc: 0.8594
loss: 0.3582, acc: 0.8588
loss: 0.3614, acc: 0.8562
loss: 0.3817, acc: 0.8527
loss: 0.3819, acc: 0.8562
loss: 0.3756, acc: 0.8597
loss: 0.3657, acc: 0.8631
loss: 0.3568, acc: 0.8642
loss: 0.3618, acc: 0.8620
loss: 0.3647, acc: 0.8596
loss: 0.3743, acc: 0.8567
loss: 0.3747, acc: 0.8562
loss: 0.3745, acc: 0.8574
loss: 0.3823, acc: 0.8533
loss: 0.3812, acc: 0.8552
loss: 0.3828, acc: 0.8539
loss: 0.3834, acc: 0.8528
loss: 0.3927, acc: 0.8485
loss: 0.3896, acc: 0.8500
loss: 0.3940, acc: 0.8484
loss: 0.3967, acc: 0.8474
loss: 0.3984, acc: 0.8452
loss: 0.4009, acc: 0.8452
loss: 0.4013, acc: 0.8456
loss: 0.3984, acc: 0.8473
loss: 0.3999, acc: 0.8470
loss: 0.4004, acc: 0.8469
loss: 0.4051, acc: 0.8448
loss: 0.4043, acc: 0.8449
loss: 0.4032, acc: 0.8451
loss: 0.4074, acc: 0.8428
loss: 0.4081, acc: 0.8420
loss: 0.4050, acc: 0.8436
loss: 0.4042, acc: 0.8441
loss: 0.4038, acc: 0.8449
loss: 0.4032, acc: 0.8452
loss: 0.4013, acc: 0.8461
loss: 0.4017, acc: 0.8462
loss: 0.4014, acc: 0.8461
loss: 0.3998, acc: 0.8471
loss: 0.4001, acc: 0.8466
loss: 0.3980, acc: 0.8475
loss: 0.4009, acc: 0.8469
loss: 0.4018, acc: 0.8465
loss: 0.4011, acc: 0.8470
loss: 0.4042, acc: 0.8457
loss: 0.4068, acc: 0.8444
loss: 0.4065, acc: 0.8440
loss: 0.4060, acc: 0.8439
loss: 0.4073, acc: 0.8434
loss: 0.4058, acc: 0.8436
loss: 0.4056, acc: 0.8438
loss: 0.4055, acc: 0.8436
loss: 0.4072, acc: 0.8425
loss: 0.4076, acc: 0.8418
loss: 0.4078, acc: 0.8421
loss: 0.4066, acc: 0.8423
loss: 0.4044, acc: 0.8433
loss: 0.4059, acc: 0.8432
loss: 0.4067, acc: 0.8433
loss: 0.4079, acc: 0.8424
loss: 0.4072, acc: 0.8429
loss: 0.4068, acc: 0.8428
loss: 0.4067, acc: 0.8432
loss: 0.4081, acc: 0.8425
loss: 0.4075, acc: 0.8428
loss: 0.4074, acc: 0.8428
> val_acc: 0.7965, val_f1: 0.7919
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.3360, acc: 0.8688
loss: 0.3145, acc: 0.8656
loss: 0.3559, acc: 0.8500
loss: 0.3600, acc: 0.8562
loss: 0.3733, acc: 0.8538
loss: 0.3799, acc: 0.8490
loss: 0.3756, acc: 0.8473
loss: 0.3662, acc: 0.8531
loss: 0.3622, acc: 0.8576
loss: 0.3570, acc: 0.8606
loss: 0.3533, acc: 0.8602
loss: 0.3542, acc: 0.8604
loss: 0.3520, acc: 0.8630
loss: 0.3475, acc: 0.8643
loss: 0.3476, acc: 0.8658
loss: 0.3497, acc: 0.8648
loss: 0.3553, acc: 0.8614
loss: 0.3593, acc: 0.8601
loss: 0.3563, acc: 0.8612
loss: 0.3545, acc: 0.8612
loss: 0.3553, acc: 0.8613
loss: 0.3586, acc: 0.8602
loss: 0.3606, acc: 0.8592
loss: 0.3602, acc: 0.8591
loss: 0.3622, acc: 0.8578
loss: 0.3623, acc: 0.8587
loss: 0.3638, acc: 0.8586
loss: 0.3649, acc: 0.8580
loss: 0.3633, acc: 0.8586
loss: 0.3620, acc: 0.8600
loss: 0.3629, acc: 0.8599
loss: 0.3634, acc: 0.8594
loss: 0.3665, acc: 0.8581
loss: 0.3673, acc: 0.8575
loss: 0.3691, acc: 0.8557
loss: 0.3680, acc: 0.8559
loss: 0.3669, acc: 0.8566
loss: 0.3684, acc: 0.8551
loss: 0.3687, acc: 0.8550
loss: 0.3683, acc: 0.8552
loss: 0.3704, acc: 0.8547
loss: 0.3703, acc: 0.8549
loss: 0.3711, acc: 0.8541
loss: 0.3688, acc: 0.8553
loss: 0.3671, acc: 0.8557
loss: 0.3664, acc: 0.8562
loss: 0.3669, acc: 0.8560
loss: 0.3690, acc: 0.8552
loss: 0.3692, acc: 0.8548
loss: 0.3706, acc: 0.8542
loss: 0.3717, acc: 0.8537
loss: 0.3719, acc: 0.8541
loss: 0.3711, acc: 0.8547
loss: 0.3694, acc: 0.8557
loss: 0.3703, acc: 0.8555
loss: 0.3701, acc: 0.8557
loss: 0.3692, acc: 0.8566
loss: 0.3691, acc: 0.8565
loss: 0.3700, acc: 0.8565
loss: 0.3702, acc: 0.8566
loss: 0.3708, acc: 0.8566
loss: 0.3705, acc: 0.8570
loss: 0.3703, acc: 0.8572
loss: 0.3700, acc: 0.8573
loss: 0.3709, acc: 0.8572
loss: 0.3702, acc: 0.8574
loss: 0.3689, acc: 0.8577
loss: 0.3683, acc: 0.8583
loss: 0.3678, acc: 0.8589
loss: 0.3679, acc: 0.8587
> val_acc: 0.8161, val_f1: 0.8132
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3589, acc: 0.8625
loss: 0.3643, acc: 0.8688
loss: 0.3842, acc: 0.8583
loss: 0.3459, acc: 0.8734
loss: 0.3729, acc: 0.8588
loss: 0.3583, acc: 0.8646
loss: 0.3670, acc: 0.8652
loss: 0.3587, acc: 0.8680
loss: 0.3506, acc: 0.8708
loss: 0.3438, acc: 0.8731
loss: 0.3434, acc: 0.8744
loss: 0.3551, acc: 0.8714
loss: 0.3544, acc: 0.8716
loss: 0.3584, acc: 0.8710
loss: 0.3554, acc: 0.8717
loss: 0.3577, acc: 0.8703
loss: 0.3548, acc: 0.8710
loss: 0.3544, acc: 0.8712
loss: 0.3520, acc: 0.8714
loss: 0.3524, acc: 0.8697
loss: 0.3536, acc: 0.8679
loss: 0.3525, acc: 0.8676
loss: 0.3528, acc: 0.8663
loss: 0.3542, acc: 0.8661
loss: 0.3576, acc: 0.8638
loss: 0.3539, acc: 0.8656
loss: 0.3579, acc: 0.8639
loss: 0.3603, acc: 0.8621
loss: 0.3574, acc: 0.8629
loss: 0.3562, acc: 0.8633
loss: 0.3527, acc: 0.8641
loss: 0.3535, acc: 0.8639
loss: 0.3550, acc: 0.8627
loss: 0.3557, acc: 0.8618
loss: 0.3581, acc: 0.8609
loss: 0.3568, acc: 0.8623
loss: 0.3550, acc: 0.8633
loss: 0.3582, acc: 0.8628
loss: 0.3570, acc: 0.8636
loss: 0.3583, acc: 0.8631
loss: 0.3579, acc: 0.8628
loss: 0.3570, acc: 0.8632
loss: 0.3573, acc: 0.8629
loss: 0.3582, acc: 0.8628
loss: 0.3567, acc: 0.8640
loss: 0.3547, acc: 0.8648
loss: 0.3528, acc: 0.8653
loss: 0.3555, acc: 0.8647
loss: 0.3546, acc: 0.8654
loss: 0.3533, acc: 0.8660
loss: 0.3522, acc: 0.8668
loss: 0.3526, acc: 0.8667
loss: 0.3544, acc: 0.8660
loss: 0.3526, acc: 0.8670
loss: 0.3519, acc: 0.8668
loss: 0.3517, acc: 0.8670
loss: 0.3521, acc: 0.8671
loss: 0.3520, acc: 0.8673
loss: 0.3535, acc: 0.8671
loss: 0.3542, acc: 0.8669
loss: 0.3542, acc: 0.8667
loss: 0.3552, acc: 0.8658
loss: 0.3567, acc: 0.8651
loss: 0.3569, acc: 0.8646
loss: 0.3567, acc: 0.8648
loss: 0.3573, acc: 0.8648
loss: 0.3585, acc: 0.8645
loss: 0.3589, acc: 0.8645
loss: 0.3606, acc: 0.8639
loss: 0.3619, acc: 0.8637
> val_acc: 0.8018, val_f1: 0.8009
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3725, acc: 0.8688
loss: 0.3969, acc: 0.8625
loss: 0.3444, acc: 0.8812
loss: 0.3201, acc: 0.8891
loss: 0.3373, acc: 0.8800
loss: 0.3396, acc: 0.8823
loss: 0.3356, acc: 0.8839
loss: 0.3342, acc: 0.8844
loss: 0.3315, acc: 0.8840
loss: 0.3343, acc: 0.8812
loss: 0.3350, acc: 0.8790
loss: 0.3407, acc: 0.8750
loss: 0.3440, acc: 0.8750
loss: 0.3429, acc: 0.8750
loss: 0.3453, acc: 0.8733
loss: 0.3462, acc: 0.8750
loss: 0.3414, acc: 0.8765
loss: 0.3382, acc: 0.8785
loss: 0.3384, acc: 0.8783
loss: 0.3383, acc: 0.8788
loss: 0.3408, acc: 0.8768
loss: 0.3390, acc: 0.8781
loss: 0.3429, acc: 0.8777
loss: 0.3453, acc: 0.8758
loss: 0.3464, acc: 0.8748
loss: 0.3491, acc: 0.8743
loss: 0.3515, acc: 0.8734
loss: 0.3525, acc: 0.8719
loss: 0.3528, acc: 0.8716
loss: 0.3546, acc: 0.8706
loss: 0.3533, acc: 0.8710
loss: 0.3544, acc: 0.8701
loss: 0.3532, acc: 0.8701
loss: 0.3554, acc: 0.8700
loss: 0.3568, acc: 0.8693
loss: 0.3584, acc: 0.8679
loss: 0.3587, acc: 0.8684
loss: 0.3622, acc: 0.8666
loss: 0.3634, acc: 0.8667
loss: 0.3652, acc: 0.8653
loss: 0.3650, acc: 0.8652
loss: 0.3660, acc: 0.8647
loss: 0.3651, acc: 0.8654
loss: 0.3655, acc: 0.8655
loss: 0.3675, acc: 0.8650
loss: 0.3695, acc: 0.8643
loss: 0.3700, acc: 0.8642
loss: 0.3683, acc: 0.8646
loss: 0.3678, acc: 0.8648
loss: 0.3674, acc: 0.8644
loss: 0.3693, acc: 0.8634
loss: 0.3683, acc: 0.8635
loss: 0.3679, acc: 0.8636
loss: 0.3684, acc: 0.8635
loss: 0.3673, acc: 0.8633
loss: 0.3686, acc: 0.8628
loss: 0.3698, acc: 0.8618
loss: 0.3699, acc: 0.8621
loss: 0.3702, acc: 0.8621
loss: 0.3715, acc: 0.8616
loss: 0.3731, acc: 0.8610
loss: 0.3729, acc: 0.8612
loss: 0.3713, acc: 0.8623
loss: 0.3712, acc: 0.8622
loss: 0.3720, acc: 0.8622
loss: 0.3709, acc: 0.8628
loss: 0.3710, acc: 0.8624
loss: 0.3718, acc: 0.8620
loss: 0.3730, acc: 0.8612
loss: 0.3734, acc: 0.8609
> val_acc: 0.8116, val_f1: 0.8096
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3603, acc: 0.8375
loss: 0.3734, acc: 0.8594
loss: 0.3272, acc: 0.8729
loss: 0.3109, acc: 0.8812
loss: 0.2942, acc: 0.8850
loss: 0.2868, acc: 0.8906
loss: 0.3050, acc: 0.8839
loss: 0.3048, acc: 0.8812
loss: 0.3161, acc: 0.8764
loss: 0.3234, acc: 0.8756
loss: 0.3211, acc: 0.8773
loss: 0.3139, acc: 0.8812
loss: 0.3191, acc: 0.8812
loss: 0.3115, acc: 0.8835
loss: 0.3161, acc: 0.8804
loss: 0.3180, acc: 0.8789
loss: 0.3175, acc: 0.8794
loss: 0.3172, acc: 0.8802
loss: 0.3161, acc: 0.8812
loss: 0.3131, acc: 0.8828
loss: 0.3141, acc: 0.8818
loss: 0.3127, acc: 0.8832
loss: 0.3127, acc: 0.8842
loss: 0.3165, acc: 0.8839
loss: 0.3222, acc: 0.8812
loss: 0.3269, acc: 0.8805
loss: 0.3242, acc: 0.8817
loss: 0.3238, acc: 0.8821
loss: 0.3208, acc: 0.8828
loss: 0.3219, acc: 0.8827
loss: 0.3246, acc: 0.8825
loss: 0.3273, acc: 0.8811
loss: 0.3279, acc: 0.8812
loss: 0.3269, acc: 0.8807
loss: 0.3288, acc: 0.8804
loss: 0.3279, acc: 0.8809
loss: 0.3299, acc: 0.8804
loss: 0.3294, acc: 0.8806
loss: 0.3311, acc: 0.8801
loss: 0.3297, acc: 0.8800
loss: 0.3315, acc: 0.8791
loss: 0.3345, acc: 0.8781
loss: 0.3334, acc: 0.8786
loss: 0.3328, acc: 0.8786
loss: 0.3327, acc: 0.8785
loss: 0.3337, acc: 0.8776
loss: 0.3354, acc: 0.8766
loss: 0.3360, acc: 0.8764
loss: 0.3349, acc: 0.8764
loss: 0.3333, acc: 0.8770
loss: 0.3331, acc: 0.8772
loss: 0.3348, acc: 0.8764
loss: 0.3343, acc: 0.8768
loss: 0.3335, acc: 0.8766
loss: 0.3333, acc: 0.8769
loss: 0.3331, acc: 0.8763
loss: 0.3319, acc: 0.8766
loss: 0.3349, acc: 0.8755
loss: 0.3358, acc: 0.8750
loss: 0.3361, acc: 0.8747
loss: 0.3360, acc: 0.8748
loss: 0.3362, acc: 0.8747
loss: 0.3371, acc: 0.8744
loss: 0.3377, acc: 0.8742
loss: 0.3370, acc: 0.8748
loss: 0.3382, acc: 0.8744
loss: 0.3395, acc: 0.8738
loss: 0.3397, acc: 0.8735
loss: 0.3393, acc: 0.8735
loss: 0.3404, acc: 0.8727
> val_acc: 0.8101, val_f1: 0.8070
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8413, test_f1: 0.8357
cuda memory allocated: 560920064
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0668, acc: 0.5312
loss: 1.1247, acc: 0.4594
loss: 1.1302, acc: 0.4396
loss: 1.1185, acc: 0.4203
loss: 1.1090, acc: 0.4163
loss: 1.0960, acc: 0.4188
loss: 1.0875, acc: 0.4214
loss: 1.0711, acc: 0.4313
loss: 1.0463, acc: 0.4500
loss: 1.0206, acc: 0.4719
loss: 1.0065, acc: 0.4864
loss: 0.9880, acc: 0.5031
loss: 0.9733, acc: 0.5139
loss: 0.9566, acc: 0.5259
loss: 0.9513, acc: 0.5321
loss: 0.9331, acc: 0.5449
loss: 0.9111, acc: 0.5574
loss: 0.9040, acc: 0.5632
loss: 0.8950, acc: 0.5714
loss: 0.8846, acc: 0.5787
loss: 0.8769, acc: 0.5848
loss: 0.8648, acc: 0.5918
loss: 0.8513, acc: 0.5981
loss: 0.8378, acc: 0.6049
loss: 0.8306, acc: 0.6102
loss: 0.8224, acc: 0.6147
loss: 0.8140, acc: 0.6204
loss: 0.8071, acc: 0.6248
loss: 0.8029, acc: 0.6259
loss: 0.7974, acc: 0.6306
loss: 0.7928, acc: 0.6331
loss: 0.7857, acc: 0.6373
loss: 0.7832, acc: 0.6390
loss: 0.7776, acc: 0.6430
loss: 0.7735, acc: 0.6446
loss: 0.7682, acc: 0.6476
loss: 0.7629, acc: 0.6510
loss: 0.7573, acc: 0.6546
loss: 0.7516, acc: 0.6582
loss: 0.7461, acc: 0.6613
loss: 0.7409, acc: 0.6645
loss: 0.7357, acc: 0.6670
loss: 0.7318, acc: 0.6693
loss: 0.7280, acc: 0.6720
loss: 0.7241, acc: 0.6754
loss: 0.7214, acc: 0.6769
loss: 0.7203, acc: 0.6775
loss: 0.7166, acc: 0.6797
loss: 0.7131, acc: 0.6815
loss: 0.7094, acc: 0.6840
loss: 0.7035, acc: 0.6869
loss: 0.7024, acc: 0.6875
loss: 0.6994, acc: 0.6893
loss: 0.6967, acc: 0.6903
loss: 0.6920, acc: 0.6928
loss: 0.6896, acc: 0.6949
loss: 0.6885, acc: 0.6954
loss: 0.6868, acc: 0.6966
loss: 0.6850, acc: 0.6975
loss: 0.6831, acc: 0.6986
loss: 0.6806, acc: 0.7001
loss: 0.6775, acc: 0.7020
loss: 0.6764, acc: 0.7026
loss: 0.6738, acc: 0.7046
loss: 0.6729, acc: 0.7051
loss: 0.6708, acc: 0.7059
loss: 0.6676, acc: 0.7076
loss: 0.6664, acc: 0.7085
loss: 0.6652, acc: 0.7089
loss: 0.6640, acc: 0.7099
> val_acc: 0.8018, val_f1: 0.7918
>> saved: peft/roberta_lora/mams//acc_0.8018_f1_0.7918_230828-0930
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5444, acc: 0.7812
loss: 0.5247, acc: 0.7906
loss: 0.5287, acc: 0.7979
loss: 0.5120, acc: 0.8078
loss: 0.4840, acc: 0.8213
loss: 0.4835, acc: 0.8167
loss: 0.5043, acc: 0.8063
loss: 0.5106, acc: 0.8023
loss: 0.5036, acc: 0.8000
loss: 0.5156, acc: 0.7937
loss: 0.5124, acc: 0.7966
loss: 0.5073, acc: 0.8005
loss: 0.5072, acc: 0.8014
loss: 0.5143, acc: 0.7982
loss: 0.5088, acc: 0.8004
loss: 0.5156, acc: 0.7977
loss: 0.5112, acc: 0.7985
loss: 0.5093, acc: 0.7990
loss: 0.5102, acc: 0.7990
loss: 0.5135, acc: 0.7975
loss: 0.5173, acc: 0.7958
loss: 0.5183, acc: 0.7952
loss: 0.5191, acc: 0.7954
loss: 0.5193, acc: 0.7953
loss: 0.5189, acc: 0.7950
loss: 0.5183, acc: 0.7952
loss: 0.5170, acc: 0.7956
loss: 0.5183, acc: 0.7953
loss: 0.5143, acc: 0.7959
loss: 0.5187, acc: 0.7923
loss: 0.5159, acc: 0.7937
loss: 0.5132, acc: 0.7943
loss: 0.5162, acc: 0.7928
loss: 0.5182, acc: 0.7912
loss: 0.5197, acc: 0.7893
loss: 0.5173, acc: 0.7899
loss: 0.5187, acc: 0.7899
loss: 0.5153, acc: 0.7911
loss: 0.5131, acc: 0.7923
loss: 0.5129, acc: 0.7922
loss: 0.5129, acc: 0.7918
loss: 0.5134, acc: 0.7918
loss: 0.5155, acc: 0.7914
loss: 0.5166, acc: 0.7913
loss: 0.5174, acc: 0.7907
loss: 0.5139, acc: 0.7924
loss: 0.5171, acc: 0.7914
loss: 0.5133, acc: 0.7930
loss: 0.5118, acc: 0.7927
loss: 0.5110, acc: 0.7929
loss: 0.5121, acc: 0.7926
loss: 0.5130, acc: 0.7928
loss: 0.5117, acc: 0.7937
loss: 0.5114, acc: 0.7944
loss: 0.5092, acc: 0.7953
loss: 0.5073, acc: 0.7960
loss: 0.5056, acc: 0.7965
loss: 0.5076, acc: 0.7955
loss: 0.5063, acc: 0.7962
loss: 0.5072, acc: 0.7958
loss: 0.5089, acc: 0.7953
loss: 0.5082, acc: 0.7959
loss: 0.5091, acc: 0.7955
loss: 0.5078, acc: 0.7964
loss: 0.5072, acc: 0.7968
loss: 0.5068, acc: 0.7977
loss: 0.5063, acc: 0.7980
loss: 0.5063, acc: 0.7982
loss: 0.5064, acc: 0.7982
loss: 0.5070, acc: 0.7983
> val_acc: 0.7860, val_f1: 0.7789
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5869, acc: 0.7812
loss: 0.5473, acc: 0.8000
loss: 0.5103, acc: 0.8083
loss: 0.5098, acc: 0.7969
loss: 0.5067, acc: 0.7937
loss: 0.4955, acc: 0.8031
loss: 0.4901, acc: 0.8098
loss: 0.4919, acc: 0.8117
loss: 0.4934, acc: 0.8111
loss: 0.4988, acc: 0.8087
loss: 0.4904, acc: 0.8136
loss: 0.4852, acc: 0.8156
loss: 0.4804, acc: 0.8163
loss: 0.4817, acc: 0.8143
loss: 0.4780, acc: 0.8158
loss: 0.4761, acc: 0.8172
loss: 0.4672, acc: 0.8202
loss: 0.4679, acc: 0.8201
loss: 0.4617, acc: 0.8230
loss: 0.4625, acc: 0.8231
loss: 0.4695, acc: 0.8193
loss: 0.4749, acc: 0.8162
loss: 0.4732, acc: 0.8163
loss: 0.4729, acc: 0.8161
loss: 0.4721, acc: 0.8160
loss: 0.4700, acc: 0.8161
loss: 0.4686, acc: 0.8169
loss: 0.4668, acc: 0.8176
loss: 0.4668, acc: 0.8177
loss: 0.4670, acc: 0.8167
loss: 0.4678, acc: 0.8157
loss: 0.4702, acc: 0.8150
loss: 0.4679, acc: 0.8159
loss: 0.4684, acc: 0.8158
loss: 0.4723, acc: 0.8148
loss: 0.4711, acc: 0.8155
loss: 0.4706, acc: 0.8164
loss: 0.4682, acc: 0.8176
loss: 0.4680, acc: 0.8170
loss: 0.4690, acc: 0.8159
loss: 0.4688, acc: 0.8162
loss: 0.4677, acc: 0.8167
loss: 0.4648, acc: 0.8182
loss: 0.4649, acc: 0.8182
loss: 0.4641, acc: 0.8183
loss: 0.4656, acc: 0.8175
loss: 0.4666, acc: 0.8174
loss: 0.4647, acc: 0.8184
loss: 0.4623, acc: 0.8190
loss: 0.4615, acc: 0.8195
loss: 0.4638, acc: 0.8186
loss: 0.4624, acc: 0.8196
loss: 0.4637, acc: 0.8198
loss: 0.4621, acc: 0.8208
loss: 0.4613, acc: 0.8211
loss: 0.4620, acc: 0.8206
loss: 0.4590, acc: 0.8214
loss: 0.4617, acc: 0.8206
loss: 0.4630, acc: 0.8204
loss: 0.4635, acc: 0.8202
loss: 0.4626, acc: 0.8204
loss: 0.4614, acc: 0.8205
loss: 0.4614, acc: 0.8201
loss: 0.4615, acc: 0.8203
loss: 0.4601, acc: 0.8211
loss: 0.4602, acc: 0.8203
loss: 0.4606, acc: 0.8204
loss: 0.4594, acc: 0.8204
loss: 0.4608, acc: 0.8195
loss: 0.4630, acc: 0.8185
> val_acc: 0.7808, val_f1: 0.7770
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4352, acc: 0.8625
loss: 0.4482, acc: 0.8500
loss: 0.4243, acc: 0.8583
loss: 0.4060, acc: 0.8594
loss: 0.3998, acc: 0.8538
loss: 0.4112, acc: 0.8500
loss: 0.4064, acc: 0.8554
loss: 0.4076, acc: 0.8531
loss: 0.4175, acc: 0.8431
loss: 0.4229, acc: 0.8419
loss: 0.4220, acc: 0.8403
loss: 0.4209, acc: 0.8391
loss: 0.4249, acc: 0.8375
loss: 0.4250, acc: 0.8362
loss: 0.4304, acc: 0.8333
loss: 0.4330, acc: 0.8297
loss: 0.4341, acc: 0.8298
loss: 0.4275, acc: 0.8319
loss: 0.4268, acc: 0.8332
loss: 0.4267, acc: 0.8325
loss: 0.4277, acc: 0.8327
loss: 0.4279, acc: 0.8330
loss: 0.4292, acc: 0.8329
loss: 0.4251, acc: 0.8346
loss: 0.4209, acc: 0.8365
loss: 0.4192, acc: 0.8380
loss: 0.4146, acc: 0.8391
loss: 0.4176, acc: 0.8377
loss: 0.4167, acc: 0.8381
loss: 0.4199, acc: 0.8371
loss: 0.4176, acc: 0.8379
loss: 0.4184, acc: 0.8377
loss: 0.4174, acc: 0.8377
loss: 0.4179, acc: 0.8375
loss: 0.4204, acc: 0.8368
loss: 0.4232, acc: 0.8361
loss: 0.4244, acc: 0.8363
loss: 0.4235, acc: 0.8360
loss: 0.4221, acc: 0.8365
loss: 0.4223, acc: 0.8363
loss: 0.4213, acc: 0.8370
loss: 0.4228, acc: 0.8360
loss: 0.4270, acc: 0.8344
loss: 0.4265, acc: 0.8348
loss: 0.4257, acc: 0.8353
loss: 0.4270, acc: 0.8345
loss: 0.4262, acc: 0.8350
loss: 0.4259, acc: 0.8355
loss: 0.4249, acc: 0.8360
loss: 0.4259, acc: 0.8355
loss: 0.4263, acc: 0.8354
loss: 0.4273, acc: 0.8353
loss: 0.4287, acc: 0.8344
loss: 0.4280, acc: 0.8345
loss: 0.4281, acc: 0.8348
loss: 0.4274, acc: 0.8352
loss: 0.4281, acc: 0.8348
loss: 0.4269, acc: 0.8353
loss: 0.4278, acc: 0.8357
loss: 0.4280, acc: 0.8357
loss: 0.4279, acc: 0.8359
loss: 0.4270, acc: 0.8362
loss: 0.4268, acc: 0.8362
loss: 0.4287, acc: 0.8355
loss: 0.4307, acc: 0.8353
loss: 0.4308, acc: 0.8351
loss: 0.4306, acc: 0.8355
loss: 0.4290, acc: 0.8359
loss: 0.4272, acc: 0.8367
loss: 0.4282, acc: 0.8363
> val_acc: 0.8138, val_f1: 0.8049
>> saved: peft/roberta_lora/mams//acc_0.8138_f1_0.8049_230828-0934
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.5573, acc: 0.7812
loss: 0.5131, acc: 0.8031
loss: 0.4523, acc: 0.8229
loss: 0.4338, acc: 0.8359
loss: 0.4212, acc: 0.8462
loss: 0.4180, acc: 0.8500
loss: 0.4415, acc: 0.8366
loss: 0.4262, acc: 0.8406
loss: 0.4245, acc: 0.8424
loss: 0.4237, acc: 0.8438
loss: 0.4250, acc: 0.8426
loss: 0.4248, acc: 0.8406
loss: 0.4280, acc: 0.8380
loss: 0.4242, acc: 0.8406
loss: 0.4211, acc: 0.8421
loss: 0.4168, acc: 0.8438
loss: 0.4190, acc: 0.8426
loss: 0.4166, acc: 0.8438
loss: 0.4135, acc: 0.8447
loss: 0.4143, acc: 0.8450
loss: 0.4168, acc: 0.8435
loss: 0.4181, acc: 0.8418
loss: 0.4169, acc: 0.8413
loss: 0.4167, acc: 0.8417
loss: 0.4144, acc: 0.8425
loss: 0.4125, acc: 0.8430
loss: 0.4125, acc: 0.8433
loss: 0.4095, acc: 0.8449
loss: 0.4079, acc: 0.8442
loss: 0.4077, acc: 0.8446
loss: 0.4087, acc: 0.8438
loss: 0.4079, acc: 0.8430
loss: 0.4058, acc: 0.8434
loss: 0.4071, acc: 0.8423
loss: 0.4059, acc: 0.8430
loss: 0.4054, acc: 0.8431
loss: 0.4063, acc: 0.8426
loss: 0.4033, acc: 0.8438
loss: 0.4007, acc: 0.8449
loss: 0.4022, acc: 0.8445
loss: 0.4029, acc: 0.8448
loss: 0.4032, acc: 0.8449
loss: 0.4049, acc: 0.8443
loss: 0.4033, acc: 0.8453
loss: 0.4039, acc: 0.8453
loss: 0.4037, acc: 0.8450
loss: 0.4024, acc: 0.8455
loss: 0.4012, acc: 0.8462
loss: 0.3991, acc: 0.8467
loss: 0.3995, acc: 0.8464
loss: 0.4001, acc: 0.8460
loss: 0.4031, acc: 0.8447
loss: 0.4041, acc: 0.8441
loss: 0.4047, acc: 0.8440
loss: 0.4052, acc: 0.8435
loss: 0.4070, acc: 0.8430
loss: 0.4057, acc: 0.8433
loss: 0.4064, acc: 0.8428
loss: 0.4073, acc: 0.8426
loss: 0.4074, acc: 0.8428
loss: 0.4085, acc: 0.8424
loss: 0.4073, acc: 0.8432
loss: 0.4070, acc: 0.8437
loss: 0.4056, acc: 0.8437
loss: 0.4052, acc: 0.8439
loss: 0.4052, acc: 0.8437
loss: 0.4051, acc: 0.8435
loss: 0.4052, acc: 0.8430
loss: 0.4056, acc: 0.8419
loss: 0.4052, acc: 0.8422
> val_acc: 0.8183, val_f1: 0.8117
>> saved: peft/roberta_lora/mams//acc_0.8183_f1_0.8117_230828-0936
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3361, acc: 0.8875
loss: 0.3398, acc: 0.8906
loss: 0.3458, acc: 0.8792
loss: 0.3361, acc: 0.8781
loss: 0.3352, acc: 0.8788
loss: 0.3497, acc: 0.8740
loss: 0.3484, acc: 0.8741
loss: 0.3484, acc: 0.8734
loss: 0.3419, acc: 0.8771
loss: 0.3293, acc: 0.8800
loss: 0.3510, acc: 0.8739
loss: 0.3507, acc: 0.8724
loss: 0.3538, acc: 0.8707
loss: 0.3517, acc: 0.8696
loss: 0.3531, acc: 0.8708
loss: 0.3541, acc: 0.8707
loss: 0.3552, acc: 0.8717
loss: 0.3539, acc: 0.8715
loss: 0.3577, acc: 0.8707
loss: 0.3572, acc: 0.8706
loss: 0.3596, acc: 0.8693
loss: 0.3635, acc: 0.8670
loss: 0.3659, acc: 0.8655
loss: 0.3687, acc: 0.8646
loss: 0.3702, acc: 0.8640
loss: 0.3690, acc: 0.8635
loss: 0.3685, acc: 0.8630
loss: 0.3704, acc: 0.8614
loss: 0.3727, acc: 0.8595
loss: 0.3730, acc: 0.8594
loss: 0.3763, acc: 0.8577
loss: 0.3815, acc: 0.8559
loss: 0.3816, acc: 0.8559
loss: 0.3855, acc: 0.8548
loss: 0.3886, acc: 0.8543
loss: 0.3865, acc: 0.8552
loss: 0.3872, acc: 0.8547
loss: 0.3862, acc: 0.8548
loss: 0.3869, acc: 0.8550
loss: 0.3874, acc: 0.8545
loss: 0.3877, acc: 0.8549
loss: 0.3885, acc: 0.8546
loss: 0.3883, acc: 0.8548
loss: 0.3875, acc: 0.8545
loss: 0.3874, acc: 0.8547
loss: 0.3859, acc: 0.8546
loss: 0.3855, acc: 0.8553
loss: 0.3837, acc: 0.8560
loss: 0.3836, acc: 0.8560
loss: 0.3863, acc: 0.8550
loss: 0.3856, acc: 0.8554
loss: 0.3850, acc: 0.8555
loss: 0.3839, acc: 0.8554
loss: 0.3841, acc: 0.8553
loss: 0.3878, acc: 0.8540
loss: 0.3871, acc: 0.8540
loss: 0.3872, acc: 0.8539
loss: 0.3865, acc: 0.8537
loss: 0.3876, acc: 0.8533
loss: 0.3891, acc: 0.8528
loss: 0.3889, acc: 0.8529
loss: 0.3888, acc: 0.8531
loss: 0.3890, acc: 0.8531
loss: 0.3890, acc: 0.8530
loss: 0.3904, acc: 0.8520
loss: 0.3891, acc: 0.8527
loss: 0.3893, acc: 0.8524
loss: 0.3884, acc: 0.8524
loss: 0.3879, acc: 0.8526
loss: 0.3889, acc: 0.8524
> val_acc: 0.7980, val_f1: 0.7953
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2460, acc: 0.9000
loss: 0.2622, acc: 0.8938
loss: 0.2998, acc: 0.8833
loss: 0.3383, acc: 0.8656
loss: 0.3602, acc: 0.8500
loss: 0.3728, acc: 0.8469
loss: 0.3719, acc: 0.8464
loss: 0.3782, acc: 0.8453
loss: 0.3828, acc: 0.8493
loss: 0.3783, acc: 0.8544
loss: 0.3756, acc: 0.8574
loss: 0.3695, acc: 0.8604
loss: 0.3670, acc: 0.8587
loss: 0.3594, acc: 0.8621
loss: 0.3644, acc: 0.8592
loss: 0.3676, acc: 0.8586
loss: 0.3730, acc: 0.8562
loss: 0.3712, acc: 0.8573
loss: 0.3719, acc: 0.8559
loss: 0.3717, acc: 0.8559
loss: 0.3685, acc: 0.8565
loss: 0.3687, acc: 0.8565
loss: 0.3666, acc: 0.8579
loss: 0.3635, acc: 0.8586
loss: 0.3654, acc: 0.8580
loss: 0.3639, acc: 0.8582
loss: 0.3631, acc: 0.8586
loss: 0.3627, acc: 0.8594
loss: 0.3619, acc: 0.8599
loss: 0.3616, acc: 0.8598
loss: 0.3605, acc: 0.8603
loss: 0.3584, acc: 0.8617
loss: 0.3566, acc: 0.8627
loss: 0.3580, acc: 0.8629
loss: 0.3568, acc: 0.8636
loss: 0.3591, acc: 0.8623
loss: 0.3610, acc: 0.8611
loss: 0.3620, acc: 0.8599
loss: 0.3620, acc: 0.8601
loss: 0.3605, acc: 0.8605
loss: 0.3614, acc: 0.8604
loss: 0.3625, acc: 0.8592
loss: 0.3625, acc: 0.8592
loss: 0.3619, acc: 0.8597
loss: 0.3610, acc: 0.8596
loss: 0.3637, acc: 0.8594
loss: 0.3668, acc: 0.8576
loss: 0.3670, acc: 0.8574
loss: 0.3657, acc: 0.8578
loss: 0.3670, acc: 0.8574
loss: 0.3668, acc: 0.8574
loss: 0.3659, acc: 0.8577
loss: 0.3662, acc: 0.8570
loss: 0.3668, acc: 0.8566
loss: 0.3685, acc: 0.8559
loss: 0.3693, acc: 0.8554
loss: 0.3704, acc: 0.8552
loss: 0.3712, acc: 0.8547
loss: 0.3715, acc: 0.8549
loss: 0.3705, acc: 0.8554
loss: 0.3683, acc: 0.8567
loss: 0.3685, acc: 0.8568
loss: 0.3700, acc: 0.8564
loss: 0.3688, acc: 0.8568
loss: 0.3686, acc: 0.8568
loss: 0.3701, acc: 0.8562
loss: 0.3719, acc: 0.8557
loss: 0.3722, acc: 0.8554
loss: 0.3707, acc: 0.8560
loss: 0.3713, acc: 0.8558
> val_acc: 0.8266, val_f1: 0.8198
>> saved: peft/roberta_lora/mams//acc_0.8266_f1_0.8198_230828-0939
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2496, acc: 0.9125
loss: 0.2857, acc: 0.8938
loss: 0.3055, acc: 0.8812
loss: 0.3087, acc: 0.8844
loss: 0.3034, acc: 0.8850
loss: 0.3124, acc: 0.8802
loss: 0.3214, acc: 0.8777
loss: 0.3141, acc: 0.8805
loss: 0.3207, acc: 0.8785
loss: 0.3285, acc: 0.8750
loss: 0.3309, acc: 0.8722
loss: 0.3329, acc: 0.8708
loss: 0.3355, acc: 0.8702
loss: 0.3316, acc: 0.8710
loss: 0.3346, acc: 0.8704
loss: 0.3410, acc: 0.8691
loss: 0.3465, acc: 0.8673
loss: 0.3422, acc: 0.8698
loss: 0.3443, acc: 0.8681
loss: 0.3398, acc: 0.8694
loss: 0.3441, acc: 0.8679
loss: 0.3456, acc: 0.8676
loss: 0.3418, acc: 0.8685
loss: 0.3412, acc: 0.8682
loss: 0.3432, acc: 0.8685
loss: 0.3500, acc: 0.8661
loss: 0.3506, acc: 0.8646
loss: 0.3518, acc: 0.8636
loss: 0.3518, acc: 0.8640
loss: 0.3511, acc: 0.8646
loss: 0.3514, acc: 0.8641
loss: 0.3580, acc: 0.8609
loss: 0.3595, acc: 0.8600
loss: 0.3585, acc: 0.8603
loss: 0.3582, acc: 0.8602
loss: 0.3572, acc: 0.8611
loss: 0.3594, acc: 0.8601
loss: 0.3602, acc: 0.8602
loss: 0.3589, acc: 0.8614
loss: 0.3587, acc: 0.8614
loss: 0.3574, acc: 0.8620
loss: 0.3565, acc: 0.8625
loss: 0.3569, acc: 0.8624
loss: 0.3537, acc: 0.8639
loss: 0.3558, acc: 0.8635
loss: 0.3558, acc: 0.8640
loss: 0.3568, acc: 0.8637
loss: 0.3559, acc: 0.8643
loss: 0.3538, acc: 0.8657
loss: 0.3530, acc: 0.8661
loss: 0.3521, acc: 0.8668
loss: 0.3536, acc: 0.8665
loss: 0.3545, acc: 0.8660
loss: 0.3549, acc: 0.8657
loss: 0.3548, acc: 0.8664
loss: 0.3561, acc: 0.8661
loss: 0.3559, acc: 0.8662
loss: 0.3551, acc: 0.8663
loss: 0.3554, acc: 0.8660
loss: 0.3540, acc: 0.8662
loss: 0.3543, acc: 0.8663
loss: 0.3545, acc: 0.8667
loss: 0.3535, acc: 0.8673
loss: 0.3529, acc: 0.8677
loss: 0.3530, acc: 0.8677
loss: 0.3545, acc: 0.8669
loss: 0.3543, acc: 0.8672
loss: 0.3550, acc: 0.8672
loss: 0.3562, acc: 0.8666
loss: 0.3576, acc: 0.8659
> val_acc: 0.7868, val_f1: 0.7853
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3011, acc: 0.8875
loss: 0.3525, acc: 0.8844
loss: 0.3452, acc: 0.8708
loss: 0.3368, acc: 0.8719
loss: 0.3409, acc: 0.8738
loss: 0.3445, acc: 0.8688
loss: 0.3364, acc: 0.8732
loss: 0.3201, acc: 0.8766
loss: 0.3241, acc: 0.8757
loss: 0.3167, acc: 0.8800
loss: 0.3234, acc: 0.8761
loss: 0.3261, acc: 0.8729
loss: 0.3233, acc: 0.8760
loss: 0.3237, acc: 0.8741
loss: 0.3364, acc: 0.8683
loss: 0.3377, acc: 0.8672
loss: 0.3377, acc: 0.8676
loss: 0.3374, acc: 0.8688
loss: 0.3394, acc: 0.8681
loss: 0.3365, acc: 0.8694
loss: 0.3392, acc: 0.8690
loss: 0.3398, acc: 0.8699
loss: 0.3362, acc: 0.8715
loss: 0.3355, acc: 0.8719
loss: 0.3294, acc: 0.8752
loss: 0.3288, acc: 0.8757
loss: 0.3271, acc: 0.8771
loss: 0.3272, acc: 0.8777
loss: 0.3254, acc: 0.8784
loss: 0.3248, acc: 0.8788
loss: 0.3243, acc: 0.8792
loss: 0.3244, acc: 0.8789
loss: 0.3222, acc: 0.8795
loss: 0.3246, acc: 0.8789
loss: 0.3269, acc: 0.8775
loss: 0.3274, acc: 0.8767
loss: 0.3276, acc: 0.8769
loss: 0.3292, acc: 0.8758
loss: 0.3302, acc: 0.8761
loss: 0.3308, acc: 0.8764
loss: 0.3306, acc: 0.8768
loss: 0.3316, acc: 0.8765
loss: 0.3315, acc: 0.8762
loss: 0.3313, acc: 0.8767
loss: 0.3308, acc: 0.8767
loss: 0.3323, acc: 0.8764
loss: 0.3334, acc: 0.8754
loss: 0.3345, acc: 0.8755
loss: 0.3355, acc: 0.8750
loss: 0.3367, acc: 0.8745
loss: 0.3381, acc: 0.8733
loss: 0.3389, acc: 0.8727
loss: 0.3413, acc: 0.8722
loss: 0.3411, acc: 0.8725
loss: 0.3413, acc: 0.8720
loss: 0.3424, acc: 0.8715
loss: 0.3428, acc: 0.8716
loss: 0.3426, acc: 0.8717
loss: 0.3430, acc: 0.8717
loss: 0.3430, acc: 0.8715
loss: 0.3418, acc: 0.8722
loss: 0.3413, acc: 0.8726
loss: 0.3429, acc: 0.8721
loss: 0.3433, acc: 0.8719
loss: 0.3425, acc: 0.8725
loss: 0.3410, acc: 0.8731
loss: 0.3415, acc: 0.8731
loss: 0.3406, acc: 0.8738
loss: 0.3411, acc: 0.8743
loss: 0.3405, acc: 0.8746
> val_acc: 0.8063, val_f1: 0.7985
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3551, acc: 0.8562
loss: 0.3580, acc: 0.8781
loss: 0.3231, acc: 0.8875
loss: 0.3126, acc: 0.8875
loss: 0.3224, acc: 0.8825
loss: 0.3209, acc: 0.8802
loss: 0.3263, acc: 0.8777
loss: 0.3211, acc: 0.8773
loss: 0.3154, acc: 0.8812
loss: 0.3148, acc: 0.8819
loss: 0.3109, acc: 0.8847
loss: 0.3170, acc: 0.8828
loss: 0.3268, acc: 0.8779
loss: 0.3327, acc: 0.8746
loss: 0.3326, acc: 0.8750
loss: 0.3343, acc: 0.8742
loss: 0.3371, acc: 0.8724
loss: 0.3344, acc: 0.8736
loss: 0.3318, acc: 0.8737
loss: 0.3257, acc: 0.8753
loss: 0.3226, acc: 0.8768
loss: 0.3256, acc: 0.8756
loss: 0.3273, acc: 0.8750
loss: 0.3271, acc: 0.8747
loss: 0.3276, acc: 0.8742
loss: 0.3306, acc: 0.8733
loss: 0.3325, acc: 0.8727
loss: 0.3296, acc: 0.8739
loss: 0.3286, acc: 0.8744
loss: 0.3280, acc: 0.8750
loss: 0.3283, acc: 0.8742
loss: 0.3283, acc: 0.8744
loss: 0.3277, acc: 0.8752
loss: 0.3294, acc: 0.8750
loss: 0.3296, acc: 0.8748
loss: 0.3279, acc: 0.8755
loss: 0.3289, acc: 0.8757
loss: 0.3301, acc: 0.8745
loss: 0.3296, acc: 0.8742
loss: 0.3295, acc: 0.8741
loss: 0.3306, acc: 0.8741
loss: 0.3323, acc: 0.8732
loss: 0.3329, acc: 0.8735
loss: 0.3340, acc: 0.8734
loss: 0.3343, acc: 0.8736
loss: 0.3357, acc: 0.8735
loss: 0.3358, acc: 0.8733
loss: 0.3369, acc: 0.8727
loss: 0.3352, acc: 0.8735
loss: 0.3340, acc: 0.8739
loss: 0.3350, acc: 0.8735
loss: 0.3357, acc: 0.8734
loss: 0.3344, acc: 0.8735
loss: 0.3348, acc: 0.8731
loss: 0.3341, acc: 0.8734
loss: 0.3341, acc: 0.8734
loss: 0.3340, acc: 0.8735
loss: 0.3347, acc: 0.8731
loss: 0.3347, acc: 0.8731
loss: 0.3348, acc: 0.8730
loss: 0.3352, acc: 0.8732
loss: 0.3356, acc: 0.8731
loss: 0.3362, acc: 0.8724
loss: 0.3352, acc: 0.8729
loss: 0.3353, acc: 0.8730
loss: 0.3367, acc: 0.8723
loss: 0.3359, acc: 0.8728
loss: 0.3356, acc: 0.8724
loss: 0.3346, acc: 0.8729
loss: 0.3335, acc: 0.8735
> val_acc: 0.8296, val_f1: 0.8232
>> saved: peft/roberta_lora/mams//acc_0.8296_f1_0.8232_230828-0943
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.3555, acc: 0.8875
loss: 0.3197, acc: 0.9031
loss: 0.3240, acc: 0.8917
loss: 0.2871, acc: 0.9047
loss: 0.2758, acc: 0.9087
loss: 0.2836, acc: 0.9073
loss: 0.3105, acc: 0.8929
loss: 0.3056, acc: 0.8938
loss: 0.2983, acc: 0.8965
loss: 0.3065, acc: 0.8956
loss: 0.2934, acc: 0.8994
loss: 0.2899, acc: 0.9021
loss: 0.2934, acc: 0.9000
loss: 0.2984, acc: 0.8973
loss: 0.2972, acc: 0.8971
loss: 0.2955, acc: 0.8941
loss: 0.2932, acc: 0.8941
loss: 0.2941, acc: 0.8944
loss: 0.2930, acc: 0.8944
loss: 0.2932, acc: 0.8928
loss: 0.2965, acc: 0.8905
loss: 0.2951, acc: 0.8909
loss: 0.2957, acc: 0.8916
loss: 0.2927, acc: 0.8922
loss: 0.2909, acc: 0.8930
loss: 0.2921, acc: 0.8933
loss: 0.2981, acc: 0.8900
loss: 0.2988, acc: 0.8895
loss: 0.3007, acc: 0.8901
loss: 0.2994, acc: 0.8908
loss: 0.3003, acc: 0.8901
loss: 0.2983, acc: 0.8910
loss: 0.2975, acc: 0.8915
loss: 0.2981, acc: 0.8914
loss: 0.2985, acc: 0.8914
loss: 0.2973, acc: 0.8915
loss: 0.3003, acc: 0.8902
loss: 0.3036, acc: 0.8885
loss: 0.3049, acc: 0.8875
loss: 0.3056, acc: 0.8869
loss: 0.3033, acc: 0.8880
loss: 0.3026, acc: 0.8879
loss: 0.3021, acc: 0.8876
loss: 0.3028, acc: 0.8874
loss: 0.3036, acc: 0.8865
loss: 0.3063, acc: 0.8856
loss: 0.3057, acc: 0.8858
loss: 0.3077, acc: 0.8849
loss: 0.3097, acc: 0.8846
loss: 0.3086, acc: 0.8848
loss: 0.3102, acc: 0.8839
loss: 0.3137, acc: 0.8831
loss: 0.3138, acc: 0.8824
loss: 0.3148, acc: 0.8826
loss: 0.3160, acc: 0.8825
loss: 0.3183, acc: 0.8815
loss: 0.3194, acc: 0.8814
loss: 0.3209, acc: 0.8808
loss: 0.3214, acc: 0.8802
loss: 0.3216, acc: 0.8799
loss: 0.3234, acc: 0.8792
loss: 0.3231, acc: 0.8794
loss: 0.3250, acc: 0.8791
loss: 0.3260, acc: 0.8784
loss: 0.3269, acc: 0.8781
loss: 0.3268, acc: 0.8779
loss: 0.3266, acc: 0.8782
loss: 0.3265, acc: 0.8782
loss: 0.3263, acc: 0.8782
loss: 0.3264, acc: 0.8781
> val_acc: 0.7988, val_f1: 0.7881
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.2372, acc: 0.9250
loss: 0.2935, acc: 0.9000
loss: 0.2716, acc: 0.9062
loss: 0.2790, acc: 0.8984
loss: 0.2976, acc: 0.8912
loss: 0.2981, acc: 0.8906
loss: 0.3022, acc: 0.8884
loss: 0.2900, acc: 0.8914
loss: 0.2924, acc: 0.8917
loss: 0.2912, acc: 0.8900
loss: 0.2890, acc: 0.8903
loss: 0.2908, acc: 0.8906
loss: 0.2962, acc: 0.8880
loss: 0.2971, acc: 0.8862
loss: 0.3087, acc: 0.8812
loss: 0.3116, acc: 0.8812
loss: 0.3093, acc: 0.8831
loss: 0.3053, acc: 0.8861
loss: 0.2996, acc: 0.8888
loss: 0.3021, acc: 0.8894
loss: 0.3042, acc: 0.8881
loss: 0.3113, acc: 0.8855
loss: 0.3184, acc: 0.8834
loss: 0.3220, acc: 0.8818
loss: 0.3223, acc: 0.8822
loss: 0.3229, acc: 0.8820
loss: 0.3231, acc: 0.8812
loss: 0.3209, acc: 0.8821
loss: 0.3214, acc: 0.8819
loss: 0.3257, acc: 0.8802
loss: 0.3280, acc: 0.8794
loss: 0.3300, acc: 0.8789
loss: 0.3299, acc: 0.8784
loss: 0.3288, acc: 0.8779
loss: 0.3281, acc: 0.8788
loss: 0.3303, acc: 0.8769
loss: 0.3287, acc: 0.8772
loss: 0.3276, acc: 0.8778
loss: 0.3257, acc: 0.8788
loss: 0.3282, acc: 0.8778
loss: 0.3273, acc: 0.8787
loss: 0.3299, acc: 0.8778
loss: 0.3310, acc: 0.8775
loss: 0.3312, acc: 0.8774
loss: 0.3314, acc: 0.8776
loss: 0.3322, acc: 0.8774
loss: 0.3310, acc: 0.8778
loss: 0.3311, acc: 0.8775
loss: 0.3322, acc: 0.8772
loss: 0.3318, acc: 0.8776
loss: 0.3335, acc: 0.8767
loss: 0.3337, acc: 0.8768
loss: 0.3363, acc: 0.8757
loss: 0.3357, acc: 0.8759
loss: 0.3393, acc: 0.8748
loss: 0.3402, acc: 0.8737
loss: 0.3406, acc: 0.8730
loss: 0.3419, acc: 0.8724
loss: 0.3450, acc: 0.8713
loss: 0.3451, acc: 0.8707
loss: 0.3457, acc: 0.8703
loss: 0.3450, acc: 0.8709
loss: 0.3451, acc: 0.8712
loss: 0.3457, acc: 0.8712
loss: 0.3453, acc: 0.8712
loss: 0.3449, acc: 0.8715
loss: 0.3453, acc: 0.8713
loss: 0.3455, acc: 0.8708
loss: 0.3454, acc: 0.8704
loss: 0.3456, acc: 0.8704
> val_acc: 0.7733, val_f1: 0.7718
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.3436, acc: 0.8438
loss: 0.2745, acc: 0.8906
loss: 0.2645, acc: 0.8938
loss: 0.2659, acc: 0.8938
loss: 0.2775, acc: 0.8938
loss: 0.2801, acc: 0.8958
loss: 0.2723, acc: 0.8991
loss: 0.2659, acc: 0.9031
loss: 0.2739, acc: 0.9007
loss: 0.2708, acc: 0.9000
loss: 0.2726, acc: 0.9000
loss: 0.2711, acc: 0.9000
loss: 0.2782, acc: 0.8990
loss: 0.2818, acc: 0.9000
loss: 0.2804, acc: 0.9017
loss: 0.2843, acc: 0.9004
loss: 0.2830, acc: 0.9004
loss: 0.2848, acc: 0.8993
loss: 0.2847, acc: 0.8993
loss: 0.2848, acc: 0.8994
loss: 0.2797, acc: 0.9006
loss: 0.2764, acc: 0.9020
loss: 0.2776, acc: 0.9011
loss: 0.2851, acc: 0.8982
loss: 0.2905, acc: 0.8952
loss: 0.2939, acc: 0.8930
loss: 0.2971, acc: 0.8905
loss: 0.2958, acc: 0.8900
loss: 0.2994, acc: 0.8879
loss: 0.3058, acc: 0.8862
loss: 0.3052, acc: 0.8863
loss: 0.3038, acc: 0.8875
loss: 0.3076, acc: 0.8860
loss: 0.3096, acc: 0.8847
loss: 0.3109, acc: 0.8839
loss: 0.3126, acc: 0.8828
loss: 0.3140, acc: 0.8821
loss: 0.3127, acc: 0.8824
loss: 0.3139, acc: 0.8816
loss: 0.3183, acc: 0.8800
loss: 0.3208, acc: 0.8794
loss: 0.3193, acc: 0.8798
loss: 0.3190, acc: 0.8802
loss: 0.3182, acc: 0.8804
loss: 0.3191, acc: 0.8801
loss: 0.3207, acc: 0.8796
loss: 0.3193, acc: 0.8802
loss: 0.3215, acc: 0.8793
loss: 0.3226, acc: 0.8788
loss: 0.3236, acc: 0.8788
loss: 0.3225, acc: 0.8790
loss: 0.3240, acc: 0.8784
loss: 0.3227, acc: 0.8789
loss: 0.3229, acc: 0.8786
loss: 0.3209, acc: 0.8792
loss: 0.3206, acc: 0.8796
loss: 0.3209, acc: 0.8796
loss: 0.3222, acc: 0.8794
loss: 0.3229, acc: 0.8790
loss: 0.3238, acc: 0.8788
loss: 0.3240, acc: 0.8790
loss: 0.3232, acc: 0.8795
loss: 0.3213, acc: 0.8800
loss: 0.3212, acc: 0.8798
loss: 0.3224, acc: 0.8792
loss: 0.3234, acc: 0.8787
loss: 0.3236, acc: 0.8789
loss: 0.3224, acc: 0.8796
loss: 0.3209, acc: 0.8803
loss: 0.3204, acc: 0.8804
> val_acc: 0.8191, val_f1: 0.8119
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.3357, acc: 0.8875
loss: 0.3531, acc: 0.8781
loss: 0.3375, acc: 0.8812
loss: 0.3260, acc: 0.8875
loss: 0.3267, acc: 0.8838
loss: 0.3159, acc: 0.8865
loss: 0.3085, acc: 0.8902
loss: 0.3084, acc: 0.8875
loss: 0.2984, acc: 0.8910
loss: 0.3013, acc: 0.8912
loss: 0.2997, acc: 0.8915
loss: 0.2984, acc: 0.8927
loss: 0.3104, acc: 0.8889
loss: 0.3026, acc: 0.8920
loss: 0.2996, acc: 0.8929
loss: 0.2954, acc: 0.8949
loss: 0.2906, acc: 0.8978
loss: 0.2816, acc: 0.9007
loss: 0.2832, acc: 0.9000
loss: 0.2887, acc: 0.8966
loss: 0.2932, acc: 0.8943
loss: 0.2932, acc: 0.8952
loss: 0.2921, acc: 0.8957
loss: 0.2934, acc: 0.8940
loss: 0.2912, acc: 0.8952
loss: 0.2905, acc: 0.8954
loss: 0.2883, acc: 0.8958
loss: 0.2936, acc: 0.8946
loss: 0.2942, acc: 0.8942
loss: 0.2985, acc: 0.8925
loss: 0.2975, acc: 0.8929
loss: 0.2977, acc: 0.8934
loss: 0.2991, acc: 0.8926
loss: 0.2992, acc: 0.8934
loss: 0.3013, acc: 0.8927
loss: 0.3017, acc: 0.8925
loss: 0.3035, acc: 0.8919
loss: 0.3032, acc: 0.8921
loss: 0.3071, acc: 0.8910
loss: 0.3061, acc: 0.8912
loss: 0.3049, acc: 0.8918
loss: 0.3049, acc: 0.8915
loss: 0.3046, acc: 0.8917
loss: 0.3050, acc: 0.8912
loss: 0.3053, acc: 0.8912
loss: 0.3046, acc: 0.8912
loss: 0.3040, acc: 0.8916
loss: 0.3034, acc: 0.8921
loss: 0.3046, acc: 0.8915
loss: 0.3054, acc: 0.8914
loss: 0.3047, acc: 0.8914
loss: 0.3035, acc: 0.8921
loss: 0.3027, acc: 0.8917
loss: 0.3026, acc: 0.8918
loss: 0.3036, acc: 0.8912
loss: 0.3037, acc: 0.8913
loss: 0.3027, acc: 0.8922
loss: 0.3025, acc: 0.8921
loss: 0.3020, acc: 0.8921
loss: 0.3036, acc: 0.8912
loss: 0.3043, acc: 0.8907
loss: 0.3055, acc: 0.8897
loss: 0.3058, acc: 0.8899
loss: 0.3072, acc: 0.8898
loss: 0.3070, acc: 0.8896
loss: 0.3082, acc: 0.8888
loss: 0.3103, acc: 0.8882
loss: 0.3101, acc: 0.8880
loss: 0.3102, acc: 0.8877
loss: 0.3096, acc: 0.8877
> val_acc: 0.8101, val_f1: 0.8016
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 0.2738, acc: 0.8938
loss: 0.2948, acc: 0.8875
loss: 0.2615, acc: 0.8979
loss: 0.2525, acc: 0.9062
loss: 0.2694, acc: 0.9062
loss: 0.2624, acc: 0.9094
loss: 0.2554, acc: 0.9089
loss: 0.2488, acc: 0.9102
loss: 0.2476, acc: 0.9097
loss: 0.2505, acc: 0.9069
loss: 0.2469, acc: 0.9068
loss: 0.2436, acc: 0.9073
loss: 0.2496, acc: 0.9072
loss: 0.2535, acc: 0.9067
loss: 0.2533, acc: 0.9058
loss: 0.2579, acc: 0.9043
loss: 0.2630, acc: 0.9018
loss: 0.2670, acc: 0.9007
loss: 0.2706, acc: 0.8997
loss: 0.2758, acc: 0.8978
loss: 0.2792, acc: 0.8967
loss: 0.2794, acc: 0.8972
loss: 0.2828, acc: 0.8957
loss: 0.2838, acc: 0.8951
loss: 0.2852, acc: 0.8950
loss: 0.2893, acc: 0.8925
loss: 0.2910, acc: 0.8919
loss: 0.2938, acc: 0.8902
loss: 0.2926, acc: 0.8905
loss: 0.2964, acc: 0.8894
loss: 0.2943, acc: 0.8905
loss: 0.2924, acc: 0.8912
loss: 0.2920, acc: 0.8919
loss: 0.2931, acc: 0.8917
loss: 0.2926, acc: 0.8920
loss: 0.2901, acc: 0.8931
loss: 0.2917, acc: 0.8922
loss: 0.2922, acc: 0.8918
loss: 0.2952, acc: 0.8899
loss: 0.2972, acc: 0.8891
loss: 0.2960, acc: 0.8893
loss: 0.2953, acc: 0.8896
loss: 0.2995, acc: 0.8875
loss: 0.2986, acc: 0.8881
loss: 0.3002, acc: 0.8879
loss: 0.2991, acc: 0.8880
loss: 0.2981, acc: 0.8888
loss: 0.2976, acc: 0.8895
loss: 0.2977, acc: 0.8895
loss: 0.2983, acc: 0.8890
loss: 0.2967, acc: 0.8897
loss: 0.2983, acc: 0.8888
loss: 0.2995, acc: 0.8882
loss: 0.3021, acc: 0.8870
loss: 0.3025, acc: 0.8865
loss: 0.3017, acc: 0.8867
loss: 0.3009, acc: 0.8868
loss: 0.3005, acc: 0.8870
loss: 0.3005, acc: 0.8873
loss: 0.3006, acc: 0.8871
loss: 0.3003, acc: 0.8872
loss: 0.2986, acc: 0.8877
loss: 0.3013, acc: 0.8867
loss: 0.3025, acc: 0.8864
loss: 0.3028, acc: 0.8863
loss: 0.3032, acc: 0.8861
loss: 0.3032, acc: 0.8863
loss: 0.3032, acc: 0.8866
loss: 0.3036, acc: 0.8864
loss: 0.3034, acc: 0.8866
> val_acc: 0.8153, val_f1: 0.8090
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8353, test_f1: 0.8280
cuda memory allocated: 542569984
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1394, acc: 0.3812
loss: 1.1211, acc: 0.4281
loss: 1.1096, acc: 0.4021
loss: 1.0986, acc: 0.3984
loss: 1.0861, acc: 0.4150
loss: 1.0802, acc: 0.4125
loss: 1.0768, acc: 0.4152
loss: 1.0713, acc: 0.4109
loss: 1.0621, acc: 0.4222
loss: 1.0604, acc: 0.4238
loss: 1.0583, acc: 0.4290
loss: 1.0550, acc: 0.4302
loss: 1.0472, acc: 0.4341
loss: 1.0431, acc: 0.4326
loss: 1.0391, acc: 0.4317
loss: 1.0314, acc: 0.4367
loss: 1.0254, acc: 0.4437
loss: 1.0166, acc: 0.4493
loss: 1.0109, acc: 0.4579
loss: 0.9979, acc: 0.4675
loss: 0.9911, acc: 0.4765
loss: 0.9809, acc: 0.4852
loss: 0.9694, acc: 0.4957
loss: 0.9617, acc: 0.5021
loss: 0.9535, acc: 0.5092
loss: 0.9387, acc: 0.5197
loss: 0.9318, acc: 0.5262
loss: 0.9214, acc: 0.5324
loss: 0.9086, acc: 0.5412
loss: 0.9016, acc: 0.5475
loss: 0.8948, acc: 0.5528
loss: 0.8905, acc: 0.5568
loss: 0.8846, acc: 0.5608
loss: 0.8771, acc: 0.5651
loss: 0.8679, acc: 0.5730
loss: 0.8585, acc: 0.5800
loss: 0.8527, acc: 0.5853
loss: 0.8448, acc: 0.5898
loss: 0.8427, acc: 0.5923
loss: 0.8359, acc: 0.5966
loss: 0.8287, acc: 0.6012
loss: 0.8241, acc: 0.6048
loss: 0.8186, acc: 0.6081
loss: 0.8124, acc: 0.6129
loss: 0.8053, acc: 0.6175
loss: 0.7995, acc: 0.6213
loss: 0.7970, acc: 0.6243
loss: 0.7917, acc: 0.6281
loss: 0.7876, acc: 0.6306
loss: 0.7870, acc: 0.6314
loss: 0.7854, acc: 0.6331
loss: 0.7816, acc: 0.6355
loss: 0.7783, acc: 0.6376
loss: 0.7729, acc: 0.6411
loss: 0.7681, acc: 0.6439
loss: 0.7682, acc: 0.6448
loss: 0.7675, acc: 0.6457
loss: 0.7649, acc: 0.6473
loss: 0.7602, acc: 0.6497
loss: 0.7575, acc: 0.6519
loss: 0.7553, acc: 0.6537
loss: 0.7525, acc: 0.6554
loss: 0.7486, acc: 0.6578
loss: 0.7457, acc: 0.6596
loss: 0.7430, acc: 0.6615
loss: 0.7395, acc: 0.6635
loss: 0.7389, acc: 0.6650
loss: 0.7358, acc: 0.6664
loss: 0.7345, acc: 0.6671
loss: 0.7325, acc: 0.6681
> val_acc: 0.7943, val_f1: 0.7848
>> saved: peft/roberta_lora/mams//acc_0.7943_f1_0.7848_230828-0952
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5459, acc: 0.7875
loss: 0.5393, acc: 0.7844
loss: 0.5275, acc: 0.7979
loss: 0.5189, acc: 0.7953
loss: 0.5156, acc: 0.8050
loss: 0.5138, acc: 0.8021
loss: 0.5217, acc: 0.7991
loss: 0.5199, acc: 0.8023
loss: 0.5239, acc: 0.7951
loss: 0.5302, acc: 0.7900
loss: 0.5288, acc: 0.7886
loss: 0.5238, acc: 0.7922
loss: 0.5194, acc: 0.7942
loss: 0.5154, acc: 0.7964
loss: 0.5130, acc: 0.7983
loss: 0.5131, acc: 0.8004
loss: 0.5111, acc: 0.8007
loss: 0.5205, acc: 0.7986
loss: 0.5166, acc: 0.8007
loss: 0.5217, acc: 0.7987
loss: 0.5257, acc: 0.7982
loss: 0.5238, acc: 0.7980
loss: 0.5246, acc: 0.7973
loss: 0.5236, acc: 0.7990
loss: 0.5214, acc: 0.7997
loss: 0.5187, acc: 0.8010
loss: 0.5239, acc: 0.7993
loss: 0.5239, acc: 0.7987
loss: 0.5243, acc: 0.7981
loss: 0.5258, acc: 0.7977
loss: 0.5245, acc: 0.7970
loss: 0.5252, acc: 0.7961
loss: 0.5287, acc: 0.7936
loss: 0.5288, acc: 0.7930
loss: 0.5284, acc: 0.7932
loss: 0.5275, acc: 0.7929
loss: 0.5250, acc: 0.7946
loss: 0.5254, acc: 0.7944
loss: 0.5271, acc: 0.7933
loss: 0.5260, acc: 0.7936
loss: 0.5252, acc: 0.7933
loss: 0.5225, acc: 0.7937
loss: 0.5235, acc: 0.7935
loss: 0.5230, acc: 0.7936
loss: 0.5230, acc: 0.7940
loss: 0.5206, acc: 0.7957
loss: 0.5192, acc: 0.7955
loss: 0.5203, acc: 0.7949
loss: 0.5222, acc: 0.7939
loss: 0.5208, acc: 0.7944
loss: 0.5227, acc: 0.7937
loss: 0.5246, acc: 0.7925
loss: 0.5245, acc: 0.7925
loss: 0.5240, acc: 0.7918
loss: 0.5255, acc: 0.7911
loss: 0.5242, acc: 0.7915
loss: 0.5259, acc: 0.7912
loss: 0.5270, acc: 0.7908
loss: 0.5256, acc: 0.7919
loss: 0.5248, acc: 0.7923
loss: 0.5245, acc: 0.7922
loss: 0.5271, acc: 0.7912
loss: 0.5274, acc: 0.7906
loss: 0.5273, acc: 0.7911
loss: 0.5263, acc: 0.7913
loss: 0.5255, acc: 0.7919
loss: 0.5243, acc: 0.7921
loss: 0.5237, acc: 0.7922
loss: 0.5228, acc: 0.7921
loss: 0.5221, acc: 0.7926
> val_acc: 0.8026, val_f1: 0.7991
>> saved: peft/roberta_lora/mams//acc_0.8026_f1_0.7991_230828-0953
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4749, acc: 0.7937
loss: 0.4351, acc: 0.8281
loss: 0.4459, acc: 0.8271
loss: 0.4719, acc: 0.8141
loss: 0.4803, acc: 0.8150
loss: 0.4743, acc: 0.8167
loss: 0.4580, acc: 0.8232
loss: 0.4499, acc: 0.8289
loss: 0.4631, acc: 0.8229
loss: 0.4706, acc: 0.8200
loss: 0.4686, acc: 0.8227
loss: 0.4672, acc: 0.8234
loss: 0.4644, acc: 0.8212
loss: 0.4638, acc: 0.8246
loss: 0.4573, acc: 0.8283
loss: 0.4487, acc: 0.8328
loss: 0.4562, acc: 0.8279
loss: 0.4567, acc: 0.8271
loss: 0.4573, acc: 0.8273
loss: 0.4553, acc: 0.8272
loss: 0.4520, acc: 0.8283
loss: 0.4495, acc: 0.8293
loss: 0.4514, acc: 0.8274
loss: 0.4532, acc: 0.8268
loss: 0.4542, acc: 0.8277
loss: 0.4609, acc: 0.8245
loss: 0.4597, acc: 0.8259
loss: 0.4584, acc: 0.8257
loss: 0.4579, acc: 0.8256
loss: 0.4584, acc: 0.8256
loss: 0.4604, acc: 0.8244
loss: 0.4587, acc: 0.8250
loss: 0.4585, acc: 0.8248
loss: 0.4610, acc: 0.8241
loss: 0.4630, acc: 0.8223
loss: 0.4625, acc: 0.8219
loss: 0.4665, acc: 0.8199
loss: 0.4671, acc: 0.8194
loss: 0.4680, acc: 0.8184
loss: 0.4721, acc: 0.8172
loss: 0.4736, acc: 0.8165
loss: 0.4722, acc: 0.8170
loss: 0.4696, acc: 0.8182
loss: 0.4701, acc: 0.8176
loss: 0.4680, acc: 0.8181
loss: 0.4692, acc: 0.8181
loss: 0.4689, acc: 0.8178
loss: 0.4683, acc: 0.8181
loss: 0.4682, acc: 0.8187
loss: 0.4690, acc: 0.8180
loss: 0.4704, acc: 0.8170
loss: 0.4716, acc: 0.8168
loss: 0.4720, acc: 0.8169
loss: 0.4711, acc: 0.8174
loss: 0.4710, acc: 0.8174
loss: 0.4708, acc: 0.8175
loss: 0.4676, acc: 0.8192
loss: 0.4667, acc: 0.8195
loss: 0.4680, acc: 0.8194
loss: 0.4696, acc: 0.8183
loss: 0.4711, acc: 0.8171
loss: 0.4695, acc: 0.8178
loss: 0.4724, acc: 0.8167
loss: 0.4707, acc: 0.8173
loss: 0.4704, acc: 0.8177
loss: 0.4696, acc: 0.8186
loss: 0.4709, acc: 0.8185
loss: 0.4697, acc: 0.8187
loss: 0.4722, acc: 0.8174
loss: 0.4737, acc: 0.8168
> val_acc: 0.8281, val_f1: 0.8204
>> saved: peft/roberta_lora/mams//acc_0.8281_f1_0.8204_230828-0955
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4013, acc: 0.8562
loss: 0.4516, acc: 0.8500
loss: 0.4289, acc: 0.8583
loss: 0.4007, acc: 0.8656
loss: 0.3934, acc: 0.8625
loss: 0.4093, acc: 0.8562
loss: 0.4154, acc: 0.8518
loss: 0.4236, acc: 0.8453
loss: 0.4143, acc: 0.8479
loss: 0.4209, acc: 0.8450
loss: 0.4142, acc: 0.8483
loss: 0.4101, acc: 0.8479
loss: 0.4075, acc: 0.8486
loss: 0.4085, acc: 0.8487
loss: 0.4230, acc: 0.8450
loss: 0.4237, acc: 0.8457
loss: 0.4195, acc: 0.8463
loss: 0.4102, acc: 0.8493
loss: 0.4174, acc: 0.8474
loss: 0.4169, acc: 0.8478
loss: 0.4194, acc: 0.8461
loss: 0.4197, acc: 0.8460
loss: 0.4217, acc: 0.8451
loss: 0.4280, acc: 0.8417
loss: 0.4275, acc: 0.8417
loss: 0.4284, acc: 0.8411
loss: 0.4289, acc: 0.8407
loss: 0.4273, acc: 0.8417
loss: 0.4254, acc: 0.8420
loss: 0.4266, acc: 0.8415
loss: 0.4294, acc: 0.8397
loss: 0.4300, acc: 0.8398
loss: 0.4293, acc: 0.8400
loss: 0.4304, acc: 0.8395
loss: 0.4322, acc: 0.8382
loss: 0.4329, acc: 0.8375
loss: 0.4307, acc: 0.8378
loss: 0.4325, acc: 0.8368
loss: 0.4307, acc: 0.8375
loss: 0.4281, acc: 0.8383
loss: 0.4253, acc: 0.8396
loss: 0.4257, acc: 0.8390
loss: 0.4256, acc: 0.8391
loss: 0.4265, acc: 0.8386
loss: 0.4285, acc: 0.8381
loss: 0.4272, acc: 0.8385
loss: 0.4295, acc: 0.8379
loss: 0.4298, acc: 0.8378
loss: 0.4299, acc: 0.8375
loss: 0.4297, acc: 0.8375
loss: 0.4307, acc: 0.8374
loss: 0.4311, acc: 0.8365
loss: 0.4326, acc: 0.8357
loss: 0.4333, acc: 0.8353
loss: 0.4335, acc: 0.8351
loss: 0.4305, acc: 0.8365
loss: 0.4313, acc: 0.8360
loss: 0.4322, acc: 0.8360
loss: 0.4322, acc: 0.8358
loss: 0.4316, acc: 0.8364
loss: 0.4326, acc: 0.8357
loss: 0.4337, acc: 0.8351
loss: 0.4332, acc: 0.8352
loss: 0.4347, acc: 0.8342
loss: 0.4360, acc: 0.8335
loss: 0.4353, acc: 0.8332
loss: 0.4353, acc: 0.8333
loss: 0.4343, acc: 0.8336
loss: 0.4360, acc: 0.8328
loss: 0.4362, acc: 0.8323
> val_acc: 0.8033, val_f1: 0.7925
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4410, acc: 0.8250
loss: 0.4048, acc: 0.8406
loss: 0.3939, acc: 0.8479
loss: 0.3882, acc: 0.8438
loss: 0.3767, acc: 0.8475
loss: 0.3906, acc: 0.8406
loss: 0.3911, acc: 0.8393
loss: 0.3968, acc: 0.8367
loss: 0.3917, acc: 0.8417
loss: 0.3839, acc: 0.8444
loss: 0.3962, acc: 0.8386
loss: 0.4049, acc: 0.8375
loss: 0.4100, acc: 0.8351
loss: 0.4103, acc: 0.8339
loss: 0.4136, acc: 0.8337
loss: 0.4079, acc: 0.8379
loss: 0.4055, acc: 0.8390
loss: 0.4017, acc: 0.8410
loss: 0.3997, acc: 0.8411
loss: 0.3961, acc: 0.8416
loss: 0.3979, acc: 0.8405
loss: 0.3974, acc: 0.8406
loss: 0.4022, acc: 0.8386
loss: 0.4033, acc: 0.8370
loss: 0.3980, acc: 0.8393
loss: 0.4024, acc: 0.8380
loss: 0.4044, acc: 0.8377
loss: 0.4066, acc: 0.8375
loss: 0.4077, acc: 0.8373
loss: 0.4039, acc: 0.8392
loss: 0.4015, acc: 0.8407
loss: 0.4019, acc: 0.8412
loss: 0.4022, acc: 0.8411
loss: 0.4009, acc: 0.8415
loss: 0.4013, acc: 0.8416
loss: 0.4028, acc: 0.8417
loss: 0.4024, acc: 0.8417
loss: 0.4052, acc: 0.8414
loss: 0.4057, acc: 0.8405
loss: 0.4070, acc: 0.8398
loss: 0.4063, acc: 0.8399
loss: 0.4091, acc: 0.8390
loss: 0.4093, acc: 0.8397
loss: 0.4095, acc: 0.8395
loss: 0.4110, acc: 0.8393
loss: 0.4110, acc: 0.8391
loss: 0.4115, acc: 0.8390
loss: 0.4110, acc: 0.8392
loss: 0.4090, acc: 0.8398
loss: 0.4074, acc: 0.8406
loss: 0.4068, acc: 0.8406
loss: 0.4061, acc: 0.8410
loss: 0.4077, acc: 0.8406
loss: 0.4077, acc: 0.8400
loss: 0.4099, acc: 0.8393
loss: 0.4096, acc: 0.8395
loss: 0.4093, acc: 0.8395
loss: 0.4103, acc: 0.8394
loss: 0.4089, acc: 0.8400
loss: 0.4093, acc: 0.8395
loss: 0.4103, acc: 0.8386
loss: 0.4106, acc: 0.8388
loss: 0.4099, acc: 0.8387
loss: 0.4109, acc: 0.8381
loss: 0.4121, acc: 0.8377
loss: 0.4127, acc: 0.8379
loss: 0.4126, acc: 0.8380
loss: 0.4133, acc: 0.8373
loss: 0.4123, acc: 0.8375
loss: 0.4127, acc: 0.8373
> val_acc: 0.8288, val_f1: 0.8234
>> saved: peft/roberta_lora/mams//acc_0.8288_f1_0.8234_230828-0958
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3603, acc: 0.8250
loss: 0.4244, acc: 0.8156
loss: 0.3752, acc: 0.8438
loss: 0.3806, acc: 0.8469
loss: 0.3877, acc: 0.8450
loss: 0.4011, acc: 0.8375
loss: 0.3981, acc: 0.8429
loss: 0.3866, acc: 0.8484
loss: 0.3917, acc: 0.8444
loss: 0.3887, acc: 0.8456
loss: 0.4022, acc: 0.8420
loss: 0.3950, acc: 0.8453
loss: 0.3915, acc: 0.8476
loss: 0.3861, acc: 0.8487
loss: 0.3864, acc: 0.8500
loss: 0.3904, acc: 0.8492
loss: 0.3855, acc: 0.8515
loss: 0.3837, acc: 0.8524
loss: 0.3884, acc: 0.8500
loss: 0.3882, acc: 0.8503
loss: 0.3913, acc: 0.8470
loss: 0.3882, acc: 0.8489
loss: 0.3887, acc: 0.8486
loss: 0.3872, acc: 0.8495
loss: 0.3849, acc: 0.8502
loss: 0.3830, acc: 0.8512
loss: 0.3842, acc: 0.8512
loss: 0.3863, acc: 0.8513
loss: 0.3892, acc: 0.8491
loss: 0.3874, acc: 0.8498
loss: 0.3873, acc: 0.8502
loss: 0.3868, acc: 0.8516
loss: 0.3868, acc: 0.8509
loss: 0.3877, acc: 0.8509
loss: 0.3868, acc: 0.8514
loss: 0.3874, acc: 0.8509
loss: 0.3839, acc: 0.8517
loss: 0.3895, acc: 0.8497
loss: 0.3901, acc: 0.8500
loss: 0.3894, acc: 0.8506
loss: 0.3878, acc: 0.8514
loss: 0.3906, acc: 0.8506
loss: 0.3901, acc: 0.8507
loss: 0.3886, acc: 0.8510
loss: 0.3903, acc: 0.8506
loss: 0.3891, acc: 0.8508
loss: 0.3931, acc: 0.8479
loss: 0.3948, acc: 0.8477
loss: 0.3952, acc: 0.8480
loss: 0.3949, acc: 0.8476
loss: 0.3951, acc: 0.8479
loss: 0.3942, acc: 0.8476
loss: 0.3951, acc: 0.8469
loss: 0.3952, acc: 0.8465
loss: 0.3974, acc: 0.8459
loss: 0.3975, acc: 0.8461
loss: 0.3975, acc: 0.8463
loss: 0.3971, acc: 0.8467
loss: 0.3964, acc: 0.8466
loss: 0.3943, acc: 0.8473
loss: 0.3945, acc: 0.8472
loss: 0.3929, acc: 0.8480
loss: 0.3911, acc: 0.8490
loss: 0.3909, acc: 0.8488
loss: 0.3898, acc: 0.8488
loss: 0.3912, acc: 0.8485
loss: 0.3903, acc: 0.8490
loss: 0.3891, acc: 0.8495
loss: 0.3888, acc: 0.8493
loss: 0.3883, acc: 0.8495
> val_acc: 0.8206, val_f1: 0.8132
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.4140, acc: 0.8625
loss: 0.3895, acc: 0.8750
loss: 0.3994, acc: 0.8625
loss: 0.3937, acc: 0.8656
loss: 0.3691, acc: 0.8725
loss: 0.3562, acc: 0.8781
loss: 0.3589, acc: 0.8777
loss: 0.3535, acc: 0.8797
loss: 0.3632, acc: 0.8736
loss: 0.3617, acc: 0.8750
loss: 0.3616, acc: 0.8744
loss: 0.3647, acc: 0.8719
loss: 0.3666, acc: 0.8688
loss: 0.3633, acc: 0.8696
loss: 0.3599, acc: 0.8675
loss: 0.3670, acc: 0.8648
loss: 0.3685, acc: 0.8636
loss: 0.3651, acc: 0.8646
loss: 0.3625, acc: 0.8658
loss: 0.3603, acc: 0.8669
loss: 0.3608, acc: 0.8664
loss: 0.3612, acc: 0.8668
loss: 0.3603, acc: 0.8660
loss: 0.3557, acc: 0.8674
loss: 0.3568, acc: 0.8670
loss: 0.3601, acc: 0.8666
loss: 0.3637, acc: 0.8644
loss: 0.3663, acc: 0.8629
loss: 0.3683, acc: 0.8619
loss: 0.3693, acc: 0.8617
loss: 0.3680, acc: 0.8617
loss: 0.3685, acc: 0.8613
loss: 0.3679, acc: 0.8614
loss: 0.3672, acc: 0.8608
loss: 0.3676, acc: 0.8605
loss: 0.3677, acc: 0.8608
loss: 0.3669, acc: 0.8603
loss: 0.3691, acc: 0.8584
loss: 0.3686, acc: 0.8585
loss: 0.3674, acc: 0.8588
loss: 0.3681, acc: 0.8588
loss: 0.3706, acc: 0.8574
loss: 0.3703, acc: 0.8574
loss: 0.3695, acc: 0.8577
loss: 0.3674, acc: 0.8586
loss: 0.3710, acc: 0.8576
loss: 0.3720, acc: 0.8573
loss: 0.3727, acc: 0.8573
loss: 0.3717, acc: 0.8577
loss: 0.3727, acc: 0.8570
loss: 0.3740, acc: 0.8561
loss: 0.3752, acc: 0.8558
loss: 0.3770, acc: 0.8554
loss: 0.3768, acc: 0.8557
loss: 0.3754, acc: 0.8561
loss: 0.3759, acc: 0.8558
loss: 0.3772, acc: 0.8554
loss: 0.3777, acc: 0.8553
loss: 0.3796, acc: 0.8542
loss: 0.3802, acc: 0.8542
loss: 0.3798, acc: 0.8546
loss: 0.3775, acc: 0.8553
loss: 0.3792, acc: 0.8547
loss: 0.3801, acc: 0.8546
loss: 0.3807, acc: 0.8543
loss: 0.3795, acc: 0.8547
loss: 0.3806, acc: 0.8544
loss: 0.3813, acc: 0.8541
loss: 0.3818, acc: 0.8538
loss: 0.3817, acc: 0.8541
> val_acc: 0.8273, val_f1: 0.8197
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3457, acc: 0.8500
loss: 0.2915, acc: 0.8812
loss: 0.3045, acc: 0.8708
loss: 0.3076, acc: 0.8781
loss: 0.3127, acc: 0.8800
loss: 0.3142, acc: 0.8802
loss: 0.3230, acc: 0.8741
loss: 0.3065, acc: 0.8812
loss: 0.3227, acc: 0.8771
loss: 0.3271, acc: 0.8762
loss: 0.3245, acc: 0.8761
loss: 0.3174, acc: 0.8792
loss: 0.3224, acc: 0.8779
loss: 0.3219, acc: 0.8781
loss: 0.3132, acc: 0.8821
loss: 0.3093, acc: 0.8828
loss: 0.3190, acc: 0.8801
loss: 0.3166, acc: 0.8799
loss: 0.3203, acc: 0.8786
loss: 0.3232, acc: 0.8778
loss: 0.3240, acc: 0.8765
loss: 0.3252, acc: 0.8764
loss: 0.3277, acc: 0.8758
loss: 0.3318, acc: 0.8745
loss: 0.3374, acc: 0.8718
loss: 0.3374, acc: 0.8724
loss: 0.3402, acc: 0.8708
loss: 0.3427, acc: 0.8696
loss: 0.3417, acc: 0.8705
loss: 0.3411, acc: 0.8704
loss: 0.3417, acc: 0.8704
loss: 0.3425, acc: 0.8693
loss: 0.3455, acc: 0.8686
loss: 0.3460, acc: 0.8680
loss: 0.3448, acc: 0.8688
loss: 0.3473, acc: 0.8679
loss: 0.3464, acc: 0.8681
loss: 0.3438, acc: 0.8694
loss: 0.3435, acc: 0.8697
loss: 0.3451, acc: 0.8684
loss: 0.3450, acc: 0.8686
loss: 0.3441, acc: 0.8680
loss: 0.3425, acc: 0.8689
loss: 0.3431, acc: 0.8689
loss: 0.3449, acc: 0.8681
loss: 0.3462, acc: 0.8671
loss: 0.3463, acc: 0.8670
loss: 0.3461, acc: 0.8668
loss: 0.3466, acc: 0.8666
loss: 0.3454, acc: 0.8671
loss: 0.3460, acc: 0.8673
loss: 0.3462, acc: 0.8669
loss: 0.3470, acc: 0.8662
loss: 0.3470, acc: 0.8660
loss: 0.3477, acc: 0.8659
loss: 0.3477, acc: 0.8660
loss: 0.3470, acc: 0.8668
loss: 0.3464, acc: 0.8671
loss: 0.3463, acc: 0.8671
loss: 0.3471, acc: 0.8668
loss: 0.3482, acc: 0.8666
loss: 0.3484, acc: 0.8663
loss: 0.3478, acc: 0.8668
loss: 0.3469, acc: 0.8669
loss: 0.3466, acc: 0.8670
loss: 0.3468, acc: 0.8668
loss: 0.3465, acc: 0.8666
loss: 0.3454, acc: 0.8671
loss: 0.3465, acc: 0.8669
loss: 0.3465, acc: 0.8668
> val_acc: 0.8168, val_f1: 0.8060
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3838, acc: 0.8063
loss: 0.4205, acc: 0.8125
loss: 0.3745, acc: 0.8458
loss: 0.3814, acc: 0.8484
loss: 0.3629, acc: 0.8625
loss: 0.3606, acc: 0.8594
loss: 0.3614, acc: 0.8589
loss: 0.3634, acc: 0.8586
loss: 0.3642, acc: 0.8604
loss: 0.3589, acc: 0.8631
loss: 0.3563, acc: 0.8653
loss: 0.3539, acc: 0.8661
loss: 0.3507, acc: 0.8668
loss: 0.3466, acc: 0.8679
loss: 0.3473, acc: 0.8679
loss: 0.3418, acc: 0.8711
loss: 0.3477, acc: 0.8684
loss: 0.3460, acc: 0.8701
loss: 0.3476, acc: 0.8688
loss: 0.3447, acc: 0.8716
loss: 0.3374, acc: 0.8738
loss: 0.3367, acc: 0.8741
loss: 0.3395, acc: 0.8745
loss: 0.3432, acc: 0.8740
loss: 0.3429, acc: 0.8740
loss: 0.3485, acc: 0.8726
loss: 0.3479, acc: 0.8727
loss: 0.3471, acc: 0.8737
loss: 0.3489, acc: 0.8728
loss: 0.3513, acc: 0.8719
loss: 0.3515, acc: 0.8724
loss: 0.3512, acc: 0.8730
loss: 0.3492, acc: 0.8737
loss: 0.3465, acc: 0.8735
loss: 0.3436, acc: 0.8741
loss: 0.3419, acc: 0.8748
loss: 0.3379, acc: 0.8764
loss: 0.3365, acc: 0.8775
loss: 0.3364, acc: 0.8772
loss: 0.3368, acc: 0.8770
loss: 0.3355, acc: 0.8770
loss: 0.3327, acc: 0.8777
loss: 0.3345, acc: 0.8770
loss: 0.3355, acc: 0.8760
loss: 0.3374, acc: 0.8757
loss: 0.3382, acc: 0.8754
loss: 0.3371, acc: 0.8762
loss: 0.3391, acc: 0.8749
loss: 0.3414, acc: 0.8737
loss: 0.3428, acc: 0.8730
loss: 0.3441, acc: 0.8724
loss: 0.3464, acc: 0.8714
loss: 0.3473, acc: 0.8712
loss: 0.3476, acc: 0.8712
loss: 0.3461, acc: 0.8719
loss: 0.3467, acc: 0.8717
loss: 0.3454, acc: 0.8719
loss: 0.3452, acc: 0.8712
loss: 0.3449, acc: 0.8715
loss: 0.3461, acc: 0.8710
loss: 0.3468, acc: 0.8704
loss: 0.3474, acc: 0.8698
loss: 0.3479, acc: 0.8697
loss: 0.3481, acc: 0.8698
loss: 0.3480, acc: 0.8694
loss: 0.3475, acc: 0.8695
loss: 0.3465, acc: 0.8699
loss: 0.3463, acc: 0.8700
loss: 0.3462, acc: 0.8699
loss: 0.3472, acc: 0.8693
> val_acc: 0.8108, val_f1: 0.8047
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3306, acc: 0.8500
loss: 0.3058, acc: 0.8844
loss: 0.2992, acc: 0.8812
loss: 0.3285, acc: 0.8766
loss: 0.3299, acc: 0.8738
loss: 0.3200, acc: 0.8792
loss: 0.3160, acc: 0.8768
loss: 0.3072, acc: 0.8820
loss: 0.3121, acc: 0.8812
loss: 0.3084, acc: 0.8844
loss: 0.3124, acc: 0.8841
loss: 0.3091, acc: 0.8849
loss: 0.3157, acc: 0.8832
loss: 0.3143, acc: 0.8817
loss: 0.3091, acc: 0.8850
loss: 0.3213, acc: 0.8789
loss: 0.3248, acc: 0.8783
loss: 0.3242, acc: 0.8788
loss: 0.3232, acc: 0.8780
loss: 0.3241, acc: 0.8778
loss: 0.3210, acc: 0.8798
loss: 0.3229, acc: 0.8795
loss: 0.3260, acc: 0.8791
loss: 0.3280, acc: 0.8773
loss: 0.3328, acc: 0.8760
loss: 0.3317, acc: 0.8769
loss: 0.3349, acc: 0.8755
loss: 0.3361, acc: 0.8752
loss: 0.3364, acc: 0.8746
loss: 0.3408, acc: 0.8725
loss: 0.3455, acc: 0.8712
loss: 0.3424, acc: 0.8723
loss: 0.3411, acc: 0.8725
loss: 0.3443, acc: 0.8710
loss: 0.3479, acc: 0.8695
loss: 0.3525, acc: 0.8672
loss: 0.3542, acc: 0.8660
loss: 0.3516, acc: 0.8671
loss: 0.3515, acc: 0.8673
loss: 0.3497, acc: 0.8684
loss: 0.3478, acc: 0.8686
loss: 0.3483, acc: 0.8683
loss: 0.3504, acc: 0.8674
loss: 0.3516, acc: 0.8669
loss: 0.3511, acc: 0.8665
loss: 0.3494, acc: 0.8674
loss: 0.3508, acc: 0.8670
loss: 0.3514, acc: 0.8663
loss: 0.3489, acc: 0.8673
loss: 0.3494, acc: 0.8672
loss: 0.3498, acc: 0.8674
loss: 0.3508, acc: 0.8668
loss: 0.3513, acc: 0.8663
loss: 0.3517, acc: 0.8660
loss: 0.3510, acc: 0.8660
loss: 0.3524, acc: 0.8655
loss: 0.3529, acc: 0.8649
loss: 0.3517, acc: 0.8658
loss: 0.3508, acc: 0.8660
loss: 0.3498, acc: 0.8665
loss: 0.3498, acc: 0.8668
loss: 0.3493, acc: 0.8670
loss: 0.3492, acc: 0.8672
loss: 0.3483, acc: 0.8670
loss: 0.3472, acc: 0.8671
loss: 0.3502, acc: 0.8660
loss: 0.3500, acc: 0.8662
loss: 0.3503, acc: 0.8661
loss: 0.3490, acc: 0.8666
loss: 0.3481, acc: 0.8671
> val_acc: 0.8191, val_f1: 0.8136
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8443, test_f1: 0.8403
>> test_acc: 0.8413, test_f1: 0.8357
>> test_acc: 0.8353, test_f1: 0.8280
>> test_acc: 0.8443, test_f1: 0.8403

>> avg_test_acc: 0.8403, avg_test_f1: 0.8347
>> max_test_acc: 0.8443, max_test_f1: 0.8403
