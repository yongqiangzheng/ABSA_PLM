cuda memory allocated: 441448448
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1413, acc: 0.4062
loss: 1.0899, acc: 0.4125
loss: 1.0549, acc: 0.4604
loss: 1.0403, acc: 0.4766
loss: 1.0196, acc: 0.5025
loss: 0.9630, acc: 0.5437
loss: 0.9420, acc: 0.5661
loss: 0.9180, acc: 0.5820
loss: 0.8923, acc: 0.5986
loss: 0.8699, acc: 0.6100
loss: 0.8549, acc: 0.6176
loss: 0.8427, acc: 0.6266
loss: 0.8243, acc: 0.6375
loss: 0.8169, acc: 0.6433
> val_acc: 0.7351, val_f1: 0.6583
>> saved: peft/bert_lora/lap14//acc_0.7351_f1_0.6583_230828-0413
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.7365, acc: 0.7500
loss: 0.6173, acc: 0.7589
loss: 0.6209, acc: 0.7422
loss: 0.5964, acc: 0.7592
loss: 0.5883, acc: 0.7642
loss: 0.5712, acc: 0.7778
loss: 0.5870, acc: 0.7754
loss: 0.5903, acc: 0.7770
loss: 0.5952, acc: 0.7708
loss: 0.5934, acc: 0.7719
loss: 0.5891, acc: 0.7734
loss: 0.5738, acc: 0.7802
loss: 0.5794, acc: 0.7777
loss: 0.5899, acc: 0.7719
loss: 0.5840, acc: 0.7739
> val_acc: 0.7649, val_f1: 0.7050
>> saved: peft/bert_lora/lap14//acc_0.7649_f1_0.705_230828-0414
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4601, acc: 0.8125
loss: 0.5273, acc: 0.7986
loss: 0.5004, acc: 0.8036
loss: 0.5011, acc: 0.8010
loss: 0.4820, acc: 0.8086
loss: 0.4639, acc: 0.8157
loss: 0.4644, acc: 0.8162
loss: 0.4626, acc: 0.8149
loss: 0.4658, acc: 0.8125
loss: 0.4735, acc: 0.8093
loss: 0.4760, acc: 0.8096
loss: 0.4764, acc: 0.8083
loss: 0.4775, acc: 0.8096
loss: 0.4747, acc: 0.8102
> val_acc: 0.7774, val_f1: 0.7178
>> saved: peft/bert_lora/lap14//acc_0.7774_f1_0.7178_230828-0414
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1648, acc: 0.9688
loss: 0.3281, acc: 0.8698
loss: 0.3265, acc: 0.8807
loss: 0.3556, acc: 0.8691
loss: 0.3657, acc: 0.8646
loss: 0.3498, acc: 0.8678
loss: 0.3466, acc: 0.8690
loss: 0.3497, acc: 0.8698
loss: 0.3548, acc: 0.8643
loss: 0.3506, acc: 0.8682
loss: 0.3581, acc: 0.8646
loss: 0.3601, acc: 0.8610
loss: 0.3670, acc: 0.8586
loss: 0.3707, acc: 0.8584
loss: 0.3687, acc: 0.8587
> val_acc: 0.7727, val_f1: 0.7220
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1958, acc: 0.9583
loss: 0.2239, acc: 0.9297
loss: 0.2468, acc: 0.9159
loss: 0.3021, acc: 0.8889
loss: 0.3192, acc: 0.8750
loss: 0.3180, acc: 0.8761
loss: 0.3250, acc: 0.8759
loss: 0.3261, acc: 0.8775
loss: 0.3213, acc: 0.8779
loss: 0.3315, acc: 0.8737
loss: 0.3372, acc: 0.8697
loss: 0.3345, acc: 0.8702
loss: 0.3355, acc: 0.8715
loss: 0.3308, acc: 0.8732
loss: 0.3308, acc: 0.8724
> val_acc: 0.7696, val_f1: 0.7264
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2503, acc: 0.9313
loss: 0.1916, acc: 0.9406
loss: 0.1927, acc: 0.9333
loss: 0.1982, acc: 0.9234
loss: 0.1968, acc: 0.9250
loss: 0.1965, acc: 0.9219
loss: 0.2079, acc: 0.9143
loss: 0.2198, acc: 0.9125
loss: 0.2326, acc: 0.9090
loss: 0.2379, acc: 0.9069
loss: 0.2372, acc: 0.9097
loss: 0.2385, acc: 0.9094
loss: 0.2381, acc: 0.9096
loss: 0.2449, acc: 0.9071
> val_acc: 0.7884, val_f1: 0.7498
>> saved: peft/bert_lora/lap14//acc_0.7884_f1_0.7498_230828-0415
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1798, acc: 0.9219
loss: 0.1811, acc: 0.9196
loss: 0.2032, acc: 0.9089
loss: 0.1984, acc: 0.9136
loss: 0.2074, acc: 0.9105
loss: 0.2071, acc: 0.9086
loss: 0.2144, acc: 0.9082
loss: 0.2260, acc: 0.9046
loss: 0.2319, acc: 0.9033
loss: 0.2292, acc: 0.9076
loss: 0.2400, acc: 0.9062
loss: 0.2362, acc: 0.9073
loss: 0.2352, acc: 0.9078
loss: 0.2317, acc: 0.9109
loss: 0.2280, acc: 0.9132
> val_acc: 0.7680, val_f1: 0.7181
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1102, acc: 0.9609
loss: 0.1189, acc: 0.9583
loss: 0.1298, acc: 0.9509
loss: 0.1933, acc: 0.9293
loss: 0.2095, acc: 0.9193
loss: 0.2019, acc: 0.9235
loss: 0.1893, acc: 0.9292
loss: 0.1839, acc: 0.9311
loss: 0.1853, acc: 0.9318
loss: 0.1937, acc: 0.9292
loss: 0.2004, acc: 0.9271
loss: 0.2020, acc: 0.9274
loss: 0.2008, acc: 0.9263
loss: 0.1978, acc: 0.9266
> val_acc: 0.7602, val_f1: 0.6982
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0957, acc: 0.9688
loss: 0.2512, acc: 0.9115
loss: 0.2453, acc: 0.9091
loss: 0.2615, acc: 0.9023
loss: 0.2608, acc: 0.9048
loss: 0.2554, acc: 0.9087
loss: 0.2493, acc: 0.9093
loss: 0.2441, acc: 0.9089
loss: 0.2259, acc: 0.9162
loss: 0.2192, acc: 0.9164
loss: 0.2105, acc: 0.9179
loss: 0.2042, acc: 0.9202
loss: 0.2123, acc: 0.9180
loss: 0.2068, acc: 0.9219
loss: 0.2088, acc: 0.9225
> val_acc: 0.7790, val_f1: 0.7462
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1331, acc: 0.9479
loss: 0.1193, acc: 0.9531
loss: 0.1319, acc: 0.9495
loss: 0.1366, acc: 0.9462
loss: 0.1399, acc: 0.9429
loss: 0.1338, acc: 0.9453
loss: 0.1384, acc: 0.9460
loss: 0.1522, acc: 0.9424
loss: 0.1475, acc: 0.9448
loss: 0.1488, acc: 0.9427
loss: 0.1497, acc: 0.9422
loss: 0.1512, acc: 0.9413
loss: 0.1562, acc: 0.9405
loss: 0.1574, acc: 0.9384
loss: 0.1577, acc: 0.9399
> val_acc: 0.7696, val_f1: 0.7242
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2152, acc: 0.9437
loss: 0.1560, acc: 0.9594
loss: 0.1397, acc: 0.9583
loss: 0.1289, acc: 0.9609
loss: 0.1348, acc: 0.9575
loss: 0.1450, acc: 0.9521
loss: 0.1448, acc: 0.9527
loss: 0.1451, acc: 0.9516
loss: 0.1462, acc: 0.9486
loss: 0.1496, acc: 0.9481
loss: 0.1449, acc: 0.9494
loss: 0.1522, acc: 0.9453
loss: 0.1510, acc: 0.9442
loss: 0.1510, acc: 0.9433
> val_acc: 0.7539, val_f1: 0.6930
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.7884, test_f1: 0.7498
cuda memory allocated: 484357632
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1768, acc: 0.3187
loss: 1.1174, acc: 0.3969
loss: 1.1157, acc: 0.3833
loss: 1.1108, acc: 0.3891
loss: 1.0952, acc: 0.3987
loss: 1.0955, acc: 0.4010
loss: 1.0891, acc: 0.4107
loss: 1.0669, acc: 0.4367
loss: 1.0421, acc: 0.4611
loss: 1.0218, acc: 0.4850
loss: 1.0057, acc: 0.4994
loss: 0.9814, acc: 0.5167
loss: 0.9702, acc: 0.5269
loss: 0.9557, acc: 0.5366
> val_acc: 0.7524, val_f1: 0.7059
>> saved: peft/bert_lora/lap14//acc_0.7524_f1_0.7059_230828-0417
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.8021, acc: 0.6094
loss: 0.6799, acc: 0.7098
loss: 0.6651, acc: 0.7240
loss: 0.6348, acc: 0.7371
loss: 0.6190, acc: 0.7401
loss: 0.6291, acc: 0.7407
loss: 0.6137, acc: 0.7490
loss: 0.6161, acc: 0.7508
loss: 0.6167, acc: 0.7463
loss: 0.6106, acc: 0.7540
loss: 0.6094, acc: 0.7554
loss: 0.6087, acc: 0.7566
loss: 0.6012, acc: 0.7601
loss: 0.5968, acc: 0.7612
loss: 0.5926, acc: 0.7622
> val_acc: 0.7618, val_f1: 0.7028
>> saved: peft/bert_lora/lap14//acc_0.7618_f1_0.7028_230828-0418
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4177, acc: 0.8672
loss: 0.4969, acc: 0.8403
loss: 0.4698, acc: 0.8415
loss: 0.4905, acc: 0.8273
loss: 0.4820, acc: 0.8320
loss: 0.4709, acc: 0.8319
loss: 0.4679, acc: 0.8290
loss: 0.4697, acc: 0.8261
loss: 0.4790, acc: 0.8189
loss: 0.4768, acc: 0.8151
loss: 0.4774, acc: 0.8160
loss: 0.4749, acc: 0.8183
loss: 0.4781, acc: 0.8169
loss: 0.4734, acc: 0.8184
> val_acc: 0.7524, val_f1: 0.6924
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3999, acc: 0.8438
loss: 0.3556, acc: 0.8594
loss: 0.3584, acc: 0.8580
loss: 0.3712, acc: 0.8496
loss: 0.3563, acc: 0.8512
loss: 0.3817, acc: 0.8401
loss: 0.3854, acc: 0.8417
loss: 0.3895, acc: 0.8411
loss: 0.3833, acc: 0.8468
loss: 0.3739, acc: 0.8505
loss: 0.3811, acc: 0.8499
loss: 0.3811, acc: 0.8493
loss: 0.3821, acc: 0.8478
loss: 0.3805, acc: 0.8499
loss: 0.3793, acc: 0.8512
> val_acc: 0.7743, val_f1: 0.7183
>> saved: peft/bert_lora/lap14//acc_0.7743_f1_0.7183_230828-0418
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2910, acc: 0.8958
loss: 0.3206, acc: 0.8867
loss: 0.3110, acc: 0.8750
loss: 0.2999, acc: 0.8767
loss: 0.3129, acc: 0.8736
loss: 0.3166, acc: 0.8694
loss: 0.3178, acc: 0.8684
loss: 0.3130, acc: 0.8725
loss: 0.3121, acc: 0.8735
loss: 0.3200, acc: 0.8685
loss: 0.3191, acc: 0.8697
loss: 0.3191, acc: 0.8718
loss: 0.3192, acc: 0.8715
loss: 0.3173, acc: 0.8722
loss: 0.3147, acc: 0.8737
> val_acc: 0.7618, val_f1: 0.7089
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2189, acc: 0.9313
loss: 0.2750, acc: 0.9125
loss: 0.2502, acc: 0.9187
loss: 0.2565, acc: 0.9156
loss: 0.2497, acc: 0.9187
loss: 0.2417, acc: 0.9156
loss: 0.2388, acc: 0.9116
loss: 0.2459, acc: 0.9109
loss: 0.2533, acc: 0.9083
loss: 0.2575, acc: 0.9044
loss: 0.2570, acc: 0.9034
loss: 0.2554, acc: 0.9031
loss: 0.2690, acc: 0.8986
loss: 0.2693, acc: 0.8991
> val_acc: 0.7492, val_f1: 0.7020
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2758, acc: 0.9062
loss: 0.2514, acc: 0.9196
loss: 0.2088, acc: 0.9349
loss: 0.2220, acc: 0.9320
loss: 0.2010, acc: 0.9361
loss: 0.2034, acc: 0.9329
loss: 0.2165, acc: 0.9248
loss: 0.2155, acc: 0.9240
loss: 0.2288, acc: 0.9204
loss: 0.2244, acc: 0.9222
loss: 0.2237, acc: 0.9219
loss: 0.2180, acc: 0.9227
loss: 0.2238, acc: 0.9209
loss: 0.2296, acc: 0.9179
loss: 0.2294, acc: 0.9175
> val_acc: 0.7696, val_f1: 0.7129
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2407, acc: 0.9219
loss: 0.1801, acc: 0.9375
loss: 0.1708, acc: 0.9330
loss: 0.1961, acc: 0.9243
loss: 0.1957, acc: 0.9258
loss: 0.2029, acc: 0.9224
loss: 0.2054, acc: 0.9228
loss: 0.2046, acc: 0.9223
loss: 0.2059, acc: 0.9233
loss: 0.2094, acc: 0.9203
loss: 0.2094, acc: 0.9201
loss: 0.2046, acc: 0.9221
loss: 0.2100, acc: 0.9214
loss: 0.2164, acc: 0.9203
> val_acc: 0.7476, val_f1: 0.7051
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2063, acc: 0.9062
loss: 0.1804, acc: 0.9323
loss: 0.1613, acc: 0.9460
loss: 0.1535, acc: 0.9492
loss: 0.1501, acc: 0.9464
loss: 0.1413, acc: 0.9483
loss: 0.1570, acc: 0.9466
loss: 0.1535, acc: 0.9497
loss: 0.1512, acc: 0.9497
loss: 0.1525, acc: 0.9484
loss: 0.1583, acc: 0.9461
loss: 0.1702, acc: 0.9431
loss: 0.1730, acc: 0.9411
loss: 0.1739, acc: 0.9413
loss: 0.1704, acc: 0.9419
> val_acc: 0.7586, val_f1: 0.7126
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.7743, test_f1: 0.7183
cuda memory allocated: 467580416
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0984, acc: 0.3625
loss: 1.0742, acc: 0.4000
loss: 1.0989, acc: 0.4062
loss: 1.1018, acc: 0.3844
loss: 1.0984, acc: 0.3775
loss: 1.0919, acc: 0.3823
loss: 1.0806, acc: 0.4009
loss: 1.0618, acc: 0.4273
loss: 1.0366, acc: 0.4500
loss: 1.0165, acc: 0.4694
loss: 1.0023, acc: 0.4835
loss: 0.9777, acc: 0.5005
loss: 0.9642, acc: 0.5144
loss: 0.9453, acc: 0.5299
> val_acc: 0.6944, val_f1: 0.5777
>> saved: peft/bert_lora/lap14//acc_0.6944_f1_0.5777_230828-0420
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.7024, acc: 0.7031
loss: 0.7276, acc: 0.7054
loss: 0.6697, acc: 0.7266
loss: 0.6330, acc: 0.7445
loss: 0.6653, acc: 0.7358
loss: 0.6661, acc: 0.7338
loss: 0.6469, acc: 0.7432
loss: 0.6318, acc: 0.7475
loss: 0.6296, acc: 0.7522
loss: 0.6372, acc: 0.7447
loss: 0.6359, acc: 0.7452
loss: 0.6218, acc: 0.7538
loss: 0.6253, acc: 0.7515
loss: 0.6289, acc: 0.7463
loss: 0.6231, acc: 0.7478
> val_acc: 0.7680, val_f1: 0.7208
>> saved: peft/bert_lora/lap14//acc_0.768_f1_0.7208_230828-0421
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5255, acc: 0.8047
loss: 0.5006, acc: 0.8229
loss: 0.4575, acc: 0.8326
loss: 0.4549, acc: 0.8240
loss: 0.4860, acc: 0.8021
loss: 0.4804, acc: 0.8039
loss: 0.4792, acc: 0.8079
loss: 0.4827, acc: 0.8085
loss: 0.4862, acc: 0.8075
loss: 0.4843, acc: 0.8074
loss: 0.4779, acc: 0.8096
loss: 0.4834, acc: 0.8072
loss: 0.4844, acc: 0.8066
loss: 0.4904, acc: 0.8039
> val_acc: 0.7492, val_f1: 0.6859
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2825, acc: 0.9062
loss: 0.3356, acc: 0.8646
loss: 0.3857, acc: 0.8523
loss: 0.3585, acc: 0.8652
loss: 0.3639, acc: 0.8661
loss: 0.3722, acc: 0.8618
loss: 0.3696, acc: 0.8649
loss: 0.3820, acc: 0.8611
loss: 0.3941, acc: 0.8514
loss: 0.3985, acc: 0.8505
loss: 0.4005, acc: 0.8474
loss: 0.4111, acc: 0.8432
loss: 0.4205, acc: 0.8417
loss: 0.4169, acc: 0.8438
loss: 0.4121, acc: 0.8429
> val_acc: 0.7555, val_f1: 0.6945
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2262, acc: 0.9062
loss: 0.2134, acc: 0.9141
loss: 0.2605, acc: 0.8990
loss: 0.2948, acc: 0.8837
loss: 0.3037, acc: 0.8777
loss: 0.3103, acc: 0.8750
loss: 0.3220, acc: 0.8693
loss: 0.3254, acc: 0.8701
loss: 0.3314, acc: 0.8685
loss: 0.3244, acc: 0.8724
loss: 0.3150, acc: 0.8768
loss: 0.3212, acc: 0.8739
loss: 0.3221, acc: 0.8730
loss: 0.3210, acc: 0.8732
loss: 0.3258, acc: 0.8707
> val_acc: 0.7696, val_f1: 0.7136
>> saved: peft/bert_lora/lap14//acc_0.7696_f1_0.7136_230828-0422
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2979, acc: 0.8750
loss: 0.2608, acc: 0.9031
loss: 0.2536, acc: 0.9042
loss: 0.2683, acc: 0.8969
loss: 0.2585, acc: 0.9050
loss: 0.2702, acc: 0.9021
loss: 0.2803, acc: 0.8973
loss: 0.2809, acc: 0.8969
loss: 0.2896, acc: 0.8917
loss: 0.2903, acc: 0.8931
loss: 0.2901, acc: 0.8932
loss: 0.2926, acc: 0.8901
loss: 0.2892, acc: 0.8923
loss: 0.2859, acc: 0.8942
> val_acc: 0.7618, val_f1: 0.7290
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2063, acc: 0.9375
loss: 0.2051, acc: 0.9018
loss: 0.2285, acc: 0.9089
loss: 0.2656, acc: 0.8952
loss: 0.2609, acc: 0.9048
loss: 0.2495, acc: 0.9086
loss: 0.2458, acc: 0.9121
loss: 0.2509, acc: 0.9105
loss: 0.2441, acc: 0.9115
loss: 0.2396, acc: 0.9136
loss: 0.2444, acc: 0.9123
loss: 0.2454, acc: 0.9134
loss: 0.2474, acc: 0.9118
loss: 0.2482, acc: 0.9104
loss: 0.2474, acc: 0.9106
> val_acc: 0.7508, val_f1: 0.6989
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1917, acc: 0.9297
loss: 0.1625, acc: 0.9375
loss: 0.1772, acc: 0.9286
loss: 0.1915, acc: 0.9243
loss: 0.2179, acc: 0.9167
loss: 0.2311, acc: 0.9127
loss: 0.2295, acc: 0.9108
loss: 0.2279, acc: 0.9103
loss: 0.2189, acc: 0.9141
loss: 0.2138, acc: 0.9165
loss: 0.2166, acc: 0.9167
loss: 0.2182, acc: 0.9174
loss: 0.2126, acc: 0.9189
loss: 0.2148, acc: 0.9171
> val_acc: 0.7367, val_f1: 0.6734
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1838, acc: 0.9062
loss: 0.1433, acc: 0.9427
loss: 0.1594, acc: 0.9489
loss: 0.1655, acc: 0.9434
loss: 0.1679, acc: 0.9405
loss: 0.1617, acc: 0.9411
loss: 0.1713, acc: 0.9355
loss: 0.1868, acc: 0.9314
loss: 0.1916, acc: 0.9291
loss: 0.1867, acc: 0.9321
loss: 0.1764, acc: 0.9357
loss: 0.1790, acc: 0.9336
loss: 0.1798, acc: 0.9329
loss: 0.1866, acc: 0.9313
loss: 0.1840, acc: 0.9309
> val_acc: 0.7618, val_f1: 0.7199
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1842, acc: 0.9167
loss: 0.1591, acc: 0.9492
loss: 0.1466, acc: 0.9519
loss: 0.1847, acc: 0.9392
loss: 0.1865, acc: 0.9334
loss: 0.1901, acc: 0.9330
loss: 0.1828, acc: 0.9356
loss: 0.1910, acc: 0.9342
loss: 0.1922, acc: 0.9302
loss: 0.1858, acc: 0.9323
loss: 0.1884, acc: 0.9304
loss: 0.1873, acc: 0.9321
loss: 0.1826, acc: 0.9345
loss: 0.1845, acc: 0.9338
loss: 0.1891, acc: 0.9321
> val_acc: 0.7586, val_f1: 0.7015
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.7696, test_f1: 0.7136
>> test_acc: 0.7884, test_f1: 0.7498
>> test_acc: 0.7743, test_f1: 0.7183
>> test_acc: 0.7696, test_f1: 0.7136

>> avg_test_acc: 0.7774, avg_test_f1: 0.7273
>> max_test_acc: 0.7884, max_test_f1: 0.7498
