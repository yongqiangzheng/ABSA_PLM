cuda memory allocated: 441448448
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0811, acc: 0.5938
loss: 0.9937, acc: 0.6125
loss: 0.9702, acc: 0.6125
loss: 0.9542, acc: 0.6078
loss: 0.9381, acc: 0.6125
loss: 0.9098, acc: 0.6260
loss: 0.8848, acc: 0.6384
loss: 0.8606, acc: 0.6477
loss: 0.8483, acc: 0.6556
loss: 0.8360, acc: 0.6606
loss: 0.8334, acc: 0.6619
loss: 0.8202, acc: 0.6656
loss: 0.7942, acc: 0.6769
loss: 0.7924, acc: 0.6768
loss: 0.7820, acc: 0.6808
loss: 0.7823, acc: 0.6797
loss: 0.7796, acc: 0.6805
loss: 0.7724, acc: 0.6823
loss: 0.7711, acc: 0.6839
loss: 0.7657, acc: 0.6856
loss: 0.7625, acc: 0.6854
loss: 0.7518, acc: 0.6906
> val_acc: 0.7911, val_f1: 0.6377
>> saved: peft/bert_lora/rest14//acc_0.7911_f1_0.6377_230828-0525
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5734, acc: 0.8125
loss: 0.5237, acc: 0.7991
loss: 0.5130, acc: 0.8151
loss: 0.5702, acc: 0.7739
loss: 0.5616, acc: 0.7741
loss: 0.5794, acc: 0.7743
loss: 0.5764, acc: 0.7725
loss: 0.5734, acc: 0.7770
loss: 0.5683, acc: 0.7783
loss: 0.5718, acc: 0.7753
loss: 0.5756, acc: 0.7710
loss: 0.5761, acc: 0.7708
loss: 0.5729, acc: 0.7712
loss: 0.5760, acc: 0.7710
loss: 0.5769, acc: 0.7700
loss: 0.5787, acc: 0.7691
loss: 0.5797, acc: 0.7683
loss: 0.5769, acc: 0.7687
loss: 0.5742, acc: 0.7683
loss: 0.5701, acc: 0.7703
loss: 0.5663, acc: 0.7693
loss: 0.5672, acc: 0.7690
loss: 0.5655, acc: 0.7687
> val_acc: 0.8241, val_f1: 0.7064
>> saved: peft/bert_lora/rest14//acc_0.8241_f1_0.7064_230828-0526
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4914, acc: 0.7891
loss: 0.4240, acc: 0.8194
loss: 0.3971, acc: 0.8371
loss: 0.4045, acc: 0.8405
loss: 0.4111, acc: 0.8385
loss: 0.4339, acc: 0.8157
loss: 0.4298, acc: 0.8171
loss: 0.4194, acc: 0.8189
loss: 0.4257, acc: 0.8175
loss: 0.4235, acc: 0.8221
loss: 0.4223, acc: 0.8235
loss: 0.4199, acc: 0.8247
loss: 0.4250, acc: 0.8252
loss: 0.4239, acc: 0.8243
loss: 0.4208, acc: 0.8252
loss: 0.4269, acc: 0.8236
loss: 0.4261, acc: 0.8244
loss: 0.4321, acc: 0.8220
loss: 0.4351, acc: 0.8245
loss: 0.4319, acc: 0.8264
loss: 0.4387, acc: 0.8215
loss: 0.4367, acc: 0.8222
> val_acc: 0.8063, val_f1: 0.7009
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3394, acc: 0.8750
loss: 0.3651, acc: 0.8542
loss: 0.3139, acc: 0.8778
loss: 0.3059, acc: 0.8809
loss: 0.3523, acc: 0.8676
loss: 0.3527, acc: 0.8702
loss: 0.3706, acc: 0.8669
loss: 0.3720, acc: 0.8637
loss: 0.3837, acc: 0.8544
loss: 0.3949, acc: 0.8492
loss: 0.3825, acc: 0.8542
loss: 0.3825, acc: 0.8532
loss: 0.3842, acc: 0.8519
loss: 0.3887, acc: 0.8485
loss: 0.3867, acc: 0.8486
loss: 0.3914, acc: 0.8450
loss: 0.3866, acc: 0.8457
loss: 0.3824, acc: 0.8481
loss: 0.3826, acc: 0.8489
loss: 0.3829, acc: 0.8473
loss: 0.3870, acc: 0.8456
loss: 0.3901, acc: 0.8440
loss: 0.3893, acc: 0.8435
> val_acc: 0.8482, val_f1: 0.7724
>> saved: peft/bert_lora/rest14//acc_0.8482_f1_0.7724_230828-0527
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2363, acc: 0.9271
loss: 0.2828, acc: 0.9023
loss: 0.2611, acc: 0.9014
loss: 0.2888, acc: 0.8906
loss: 0.2868, acc: 0.8886
loss: 0.2877, acc: 0.8895
loss: 0.2883, acc: 0.8911
loss: 0.2836, acc: 0.8898
loss: 0.2724, acc: 0.8939
loss: 0.2700, acc: 0.8952
loss: 0.2622, acc: 0.8980
loss: 0.2648, acc: 0.8971
loss: 0.2621, acc: 0.8973
loss: 0.2677, acc: 0.8980
loss: 0.2777, acc: 0.8943
loss: 0.2865, acc: 0.8890
loss: 0.2871, acc: 0.8886
loss: 0.2958, acc: 0.8853
loss: 0.3037, acc: 0.8837
loss: 0.3054, acc: 0.8823
loss: 0.3147, acc: 0.8771
loss: 0.3145, acc: 0.8764
loss: 0.3164, acc: 0.8767
> val_acc: 0.8464, val_f1: 0.7707
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2674, acc: 0.9125
loss: 0.2715, acc: 0.8906
loss: 0.2630, acc: 0.9083
loss: 0.2490, acc: 0.9141
loss: 0.2729, acc: 0.9025
loss: 0.2837, acc: 0.8958
loss: 0.2924, acc: 0.8902
loss: 0.2895, acc: 0.8930
loss: 0.2813, acc: 0.8958
loss: 0.2875, acc: 0.8938
loss: 0.2888, acc: 0.8926
loss: 0.2875, acc: 0.8927
loss: 0.2913, acc: 0.8885
loss: 0.2941, acc: 0.8862
loss: 0.2944, acc: 0.8862
loss: 0.3005, acc: 0.8840
loss: 0.2992, acc: 0.8838
loss: 0.2982, acc: 0.8847
loss: 0.2973, acc: 0.8849
loss: 0.2979, acc: 0.8847
loss: 0.2980, acc: 0.8857
loss: 0.2951, acc: 0.8861
> val_acc: 0.8295, val_f1: 0.7202
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2206, acc: 0.8906
loss: 0.1852, acc: 0.9152
loss: 0.1760, acc: 0.9297
loss: 0.2234, acc: 0.9210
loss: 0.2168, acc: 0.9247
loss: 0.2086, acc: 0.9282
loss: 0.2061, acc: 0.9287
loss: 0.1943, acc: 0.9333
loss: 0.2001, acc: 0.9315
loss: 0.2007, acc: 0.9275
loss: 0.2016, acc: 0.9243
loss: 0.2068, acc: 0.9232
loss: 0.2160, acc: 0.9199
loss: 0.2156, acc: 0.9202
loss: 0.2192, acc: 0.9206
loss: 0.2186, acc: 0.9209
loss: 0.2154, acc: 0.9211
loss: 0.2187, acc: 0.9199
loss: 0.2232, acc: 0.9181
loss: 0.2216, acc: 0.9185
loss: 0.2230, acc: 0.9176
loss: 0.2221, acc: 0.9185
loss: 0.2248, acc: 0.9169
> val_acc: 0.8259, val_f1: 0.7275
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2385, acc: 0.8984
loss: 0.2216, acc: 0.9062
loss: 0.2710, acc: 0.8929
loss: 0.2510, acc: 0.9046
loss: 0.2404, acc: 0.9062
loss: 0.2300, acc: 0.9073
loss: 0.2218, acc: 0.9108
loss: 0.2192, acc: 0.9111
loss: 0.2185, acc: 0.9134
loss: 0.2174, acc: 0.9139
loss: 0.2210, acc: 0.9132
loss: 0.2179, acc: 0.9153
loss: 0.2258, acc: 0.9111
loss: 0.2280, acc: 0.9099
loss: 0.2363, acc: 0.9079
loss: 0.2312, acc: 0.9106
loss: 0.2279, acc: 0.9118
loss: 0.2313, acc: 0.9119
loss: 0.2288, acc: 0.9122
loss: 0.2299, acc: 0.9116
loss: 0.2308, acc: 0.9117
loss: 0.2285, acc: 0.9114
> val_acc: 0.8357, val_f1: 0.7471
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1482, acc: 0.9688
loss: 0.2299, acc: 0.9167
loss: 0.1770, acc: 0.9403
loss: 0.1729, acc: 0.9375
loss: 0.1770, acc: 0.9345
loss: 0.1649, acc: 0.9375
loss: 0.1642, acc: 0.9365
loss: 0.1717, acc: 0.9349
loss: 0.1712, acc: 0.9345
loss: 0.1742, acc: 0.9341
loss: 0.1801, acc: 0.9326
loss: 0.1816, acc: 0.9325
loss: 0.1882, acc: 0.9278
loss: 0.1906, acc: 0.9280
loss: 0.1872, acc: 0.9291
loss: 0.1906, acc: 0.9268
loss: 0.1876, acc: 0.9279
loss: 0.1882, acc: 0.9277
loss: 0.1977, acc: 0.9251
loss: 0.1937, acc: 0.9258
loss: 0.1957, acc: 0.9251
loss: 0.1968, acc: 0.9251
loss: 0.1949, acc: 0.9260
> val_acc: 0.8464, val_f1: 0.7722
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8482, test_f1: 0.7724
cuda memory allocated: 484357632
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0990, acc: 0.4562
loss: 1.0304, acc: 0.5281
loss: 1.0257, acc: 0.5292
loss: 0.9957, acc: 0.5516
loss: 0.9578, acc: 0.5625
loss: 0.9397, acc: 0.5833
loss: 0.9282, acc: 0.5946
loss: 0.9043, acc: 0.6094
loss: 0.8742, acc: 0.6222
loss: 0.8567, acc: 0.6300
loss: 0.8416, acc: 0.6369
loss: 0.8285, acc: 0.6432
loss: 0.8257, acc: 0.6447
loss: 0.8112, acc: 0.6527
loss: 0.8068, acc: 0.6521
loss: 0.8007, acc: 0.6535
loss: 0.7907, acc: 0.6577
loss: 0.7865, acc: 0.6611
loss: 0.7801, acc: 0.6668
loss: 0.7749, acc: 0.6687
loss: 0.7643, acc: 0.6726
loss: 0.7548, acc: 0.6759
> val_acc: 0.7652, val_f1: 0.6057
>> saved: peft/bert_lora/rest14//acc_0.7652_f1_0.6057_230828-0530
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4849, acc: 0.8281
loss: 0.5663, acc: 0.7500
loss: 0.5724, acc: 0.7422
loss: 0.6004, acc: 0.7316
loss: 0.5932, acc: 0.7358
loss: 0.6017, acc: 0.7338
loss: 0.5972, acc: 0.7402
loss: 0.5900, acc: 0.7466
loss: 0.5931, acc: 0.7455
loss: 0.6069, acc: 0.7374
loss: 0.6000, acc: 0.7434
loss: 0.6115, acc: 0.7418
loss: 0.6120, acc: 0.7419
loss: 0.6038, acc: 0.7467
loss: 0.6058, acc: 0.7452
loss: 0.6051, acc: 0.7443
loss: 0.6073, acc: 0.7416
loss: 0.6059, acc: 0.7432
loss: 0.6126, acc: 0.7418
loss: 0.6101, acc: 0.7445
loss: 0.6090, acc: 0.7439
loss: 0.6102, acc: 0.7447
loss: 0.6110, acc: 0.7433
> val_acc: 0.8098, val_f1: 0.6839
>> saved: peft/bert_lora/rest14//acc_0.8098_f1_0.6839_230828-0531
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4700, acc: 0.8125
loss: 0.5184, acc: 0.7812
loss: 0.4813, acc: 0.7969
loss: 0.4629, acc: 0.8059
loss: 0.4612, acc: 0.8125
loss: 0.4595, acc: 0.8125
loss: 0.4655, acc: 0.8116
loss: 0.4625, acc: 0.8117
loss: 0.4746, acc: 0.8068
loss: 0.4783, acc: 0.8055
loss: 0.4722, acc: 0.8050
loss: 0.4751, acc: 0.8035
loss: 0.4741, acc: 0.8022
loss: 0.4688, acc: 0.8048
loss: 0.4652, acc: 0.8062
loss: 0.4667, acc: 0.8042
loss: 0.4734, acc: 0.8010
loss: 0.4674, acc: 0.8044
loss: 0.4704, acc: 0.8009
loss: 0.4678, acc: 0.8011
loss: 0.4719, acc: 0.8011
loss: 0.4764, acc: 0.7990
> val_acc: 0.8036, val_f1: 0.6608
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.5386, acc: 0.7812
loss: 0.4104, acc: 0.8125
loss: 0.4920, acc: 0.7812
loss: 0.4613, acc: 0.7969
loss: 0.4235, acc: 0.8095
loss: 0.4065, acc: 0.8269
loss: 0.4065, acc: 0.8296
loss: 0.4071, acc: 0.8325
loss: 0.3990, acc: 0.8323
loss: 0.4030, acc: 0.8336
loss: 0.3970, acc: 0.8370
loss: 0.3898, acc: 0.8382
loss: 0.3907, acc: 0.8391
loss: 0.3991, acc: 0.8362
loss: 0.4024, acc: 0.8341
loss: 0.3964, acc: 0.8355
loss: 0.3951, acc: 0.8353
loss: 0.3902, acc: 0.8372
loss: 0.3849, acc: 0.8396
loss: 0.3882, acc: 0.8402
loss: 0.3872, acc: 0.8397
loss: 0.3896, acc: 0.8376
loss: 0.3907, acc: 0.8376
> val_acc: 0.8330, val_f1: 0.7535
>> saved: peft/bert_lora/rest14//acc_0.833_f1_0.7535_230828-0532
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3503, acc: 0.8646
loss: 0.3418, acc: 0.8516
loss: 0.3345, acc: 0.8582
loss: 0.3467, acc: 0.8542
loss: 0.3321, acc: 0.8573
loss: 0.3242, acc: 0.8638
loss: 0.3209, acc: 0.8655
loss: 0.3287, acc: 0.8618
loss: 0.3239, acc: 0.8656
loss: 0.3258, acc: 0.8652
loss: 0.3334, acc: 0.8620
loss: 0.3303, acc: 0.8626
loss: 0.3334, acc: 0.8616
loss: 0.3267, acc: 0.8640
loss: 0.3266, acc: 0.8643
loss: 0.3332, acc: 0.8638
loss: 0.3331, acc: 0.8645
loss: 0.3355, acc: 0.8636
loss: 0.3341, acc: 0.8639
loss: 0.3333, acc: 0.8638
loss: 0.3313, acc: 0.8644
loss: 0.3344, acc: 0.8655
loss: 0.3329, acc: 0.8667
> val_acc: 0.8420, val_f1: 0.7595
>> saved: peft/bert_lora/rest14//acc_0.842_f1_0.7595_230828-0532
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2608, acc: 0.8938
loss: 0.2484, acc: 0.9125
loss: 0.2525, acc: 0.9000
loss: 0.2225, acc: 0.9141
loss: 0.2395, acc: 0.9050
loss: 0.2584, acc: 0.8948
loss: 0.2602, acc: 0.8964
loss: 0.2692, acc: 0.8906
loss: 0.2738, acc: 0.8889
loss: 0.2923, acc: 0.8869
loss: 0.3042, acc: 0.8818
loss: 0.2989, acc: 0.8844
loss: 0.3006, acc: 0.8861
loss: 0.3022, acc: 0.8848
loss: 0.3046, acc: 0.8829
loss: 0.3031, acc: 0.8832
loss: 0.3032, acc: 0.8842
loss: 0.3063, acc: 0.8840
loss: 0.3102, acc: 0.8812
loss: 0.3136, acc: 0.8797
loss: 0.3099, acc: 0.8812
loss: 0.3113, acc: 0.8807
> val_acc: 0.8357, val_f1: 0.7373
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2325, acc: 0.9062
loss: 0.2277, acc: 0.9018
loss: 0.2113, acc: 0.9036
loss: 0.2259, acc: 0.9118
loss: 0.2460, acc: 0.8991
loss: 0.2571, acc: 0.8970
loss: 0.2678, acc: 0.8945
loss: 0.2661, acc: 0.8953
loss: 0.2607, acc: 0.8996
loss: 0.2593, acc: 0.9003
loss: 0.2576, acc: 0.8984
loss: 0.2577, acc: 0.8964
loss: 0.2575, acc: 0.8967
loss: 0.2561, acc: 0.8960
loss: 0.2586, acc: 0.8976
loss: 0.2603, acc: 0.8977
loss: 0.2593, acc: 0.8986
loss: 0.2607, acc: 0.8991
loss: 0.2644, acc: 0.8974
loss: 0.2632, acc: 0.8985
loss: 0.2647, acc: 0.8983
loss: 0.2689, acc: 0.8969
loss: 0.2757, acc: 0.8948
> val_acc: 0.8500, val_f1: 0.7717
>> saved: peft/bert_lora/rest14//acc_0.85_f1_0.7717_230828-0533
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1543, acc: 0.9609
loss: 0.1626, acc: 0.9479
loss: 0.1802, acc: 0.9375
loss: 0.1937, acc: 0.9342
loss: 0.1949, acc: 0.9336
loss: 0.1883, acc: 0.9343
loss: 0.2003, acc: 0.9256
loss: 0.1997, acc: 0.9263
loss: 0.2153, acc: 0.9233
loss: 0.2144, acc: 0.9209
loss: 0.2183, acc: 0.9190
loss: 0.2163, acc: 0.9195
loss: 0.2182, acc: 0.9170
loss: 0.2142, acc: 0.9180
loss: 0.2195, acc: 0.9164
loss: 0.2157, acc: 0.9173
loss: 0.2212, acc: 0.9144
loss: 0.2194, acc: 0.9154
loss: 0.2215, acc: 0.9149
loss: 0.2236, acc: 0.9138
loss: 0.2278, acc: 0.9120
loss: 0.2314, acc: 0.9100
> val_acc: 0.8357, val_f1: 0.7592
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0476, acc: 1.0000
loss: 0.1368, acc: 0.9427
loss: 0.1539, acc: 0.9347
loss: 0.1820, acc: 0.9199
loss: 0.1854, acc: 0.9241
loss: 0.1962, acc: 0.9255
loss: 0.1934, acc: 0.9264
loss: 0.1986, acc: 0.9253
loss: 0.2087, acc: 0.9215
loss: 0.2093, acc: 0.9192
loss: 0.2096, acc: 0.9197
loss: 0.2137, acc: 0.9180
loss: 0.2178, acc: 0.9160
loss: 0.2138, acc: 0.9181
loss: 0.2209, acc: 0.9146
loss: 0.2280, acc: 0.9124
loss: 0.2295, acc: 0.9097
loss: 0.2377, acc: 0.9052
loss: 0.2362, acc: 0.9056
loss: 0.2371, acc: 0.9062
loss: 0.2374, acc: 0.9062
loss: 0.2415, acc: 0.9042
loss: 0.2401, acc: 0.9054
> val_acc: 0.8304, val_f1: 0.7359
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2934, acc: 0.8646
loss: 0.2222, acc: 0.9062
loss: 0.2162, acc: 0.9087
loss: 0.1960, acc: 0.9201
loss: 0.1956, acc: 0.9171
loss: 0.2047, acc: 0.9141
loss: 0.1920, acc: 0.9195
loss: 0.1867, acc: 0.9235
loss: 0.1889, acc: 0.9230
loss: 0.1971, acc: 0.9206
loss: 0.2003, acc: 0.9198
loss: 0.1990, acc: 0.9197
loss: 0.2017, acc: 0.9191
loss: 0.2044, acc: 0.9187
loss: 0.2041, acc: 0.9178
loss: 0.2022, acc: 0.9199
loss: 0.2001, acc: 0.9209
loss: 0.2011, acc: 0.9215
loss: 0.2027, acc: 0.9217
loss: 0.2067, acc: 0.9193
loss: 0.2101, acc: 0.9184
loss: 0.2133, acc: 0.9170
loss: 0.2153, acc: 0.9163
> val_acc: 0.8295, val_f1: 0.7444
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.1654, acc: 0.9500
loss: 0.1721, acc: 0.9469
loss: 0.1710, acc: 0.9458
loss: 0.1765, acc: 0.9391
loss: 0.1768, acc: 0.9350
loss: 0.1705, acc: 0.9396
loss: 0.1745, acc: 0.9366
loss: 0.1749, acc: 0.9352
loss: 0.1845, acc: 0.9285
loss: 0.1928, acc: 0.9250
loss: 0.1917, acc: 0.9239
loss: 0.1968, acc: 0.9208
loss: 0.1977, acc: 0.9212
loss: 0.2039, acc: 0.9210
loss: 0.2012, acc: 0.9213
loss: 0.1996, acc: 0.9223
loss: 0.1981, acc: 0.9232
loss: 0.1957, acc: 0.9247
loss: 0.1995, acc: 0.9237
loss: 0.1974, acc: 0.9244
loss: 0.1974, acc: 0.9250
loss: 0.2003, acc: 0.9236
> val_acc: 0.8411, val_f1: 0.7577
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1477, acc: 0.9531
loss: 0.1327, acc: 0.9688
loss: 0.1191, acc: 0.9635
loss: 0.1373, acc: 0.9559
loss: 0.1585, acc: 0.9460
loss: 0.1567, acc: 0.9479
loss: 0.1642, acc: 0.9404
loss: 0.1550, acc: 0.9443
loss: 0.1588, acc: 0.9427
loss: 0.1639, acc: 0.9395
loss: 0.1587, acc: 0.9417
loss: 0.1561, acc: 0.9430
loss: 0.1574, acc: 0.9435
loss: 0.1591, acc: 0.9422
loss: 0.1627, acc: 0.9405
loss: 0.1620, acc: 0.9399
loss: 0.1622, acc: 0.9409
loss: 0.1642, acc: 0.9404
loss: 0.1614, acc: 0.9412
loss: 0.1649, acc: 0.9398
loss: 0.1663, acc: 0.9396
loss: 0.1669, acc: 0.9404
loss: 0.1683, acc: 0.9403
> val_acc: 0.8402, val_f1: 0.7635
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8500, test_f1: 0.7717
cuda memory allocated: 467580416
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0036, acc: 0.5938
loss: 1.0076, acc: 0.5563
loss: 0.9897, acc: 0.5750
loss: 0.9754, acc: 0.5750
loss: 0.9406, acc: 0.5925
loss: 0.9280, acc: 0.5979
loss: 0.9130, acc: 0.6062
loss: 0.8903, acc: 0.6156
loss: 0.8611, acc: 0.6292
loss: 0.8455, acc: 0.6356
loss: 0.8277, acc: 0.6415
loss: 0.8047, acc: 0.6516
loss: 0.8011, acc: 0.6572
loss: 0.7985, acc: 0.6576
loss: 0.7878, acc: 0.6637
loss: 0.7767, acc: 0.6703
loss: 0.7644, acc: 0.6765
loss: 0.7609, acc: 0.6778
loss: 0.7530, acc: 0.6813
loss: 0.7471, acc: 0.6850
loss: 0.7421, acc: 0.6875
loss: 0.7361, acc: 0.6895
> val_acc: 0.7768, val_f1: 0.5824
>> saved: peft/bert_lora/rest14//acc_0.7768_f1_0.5824_230828-0536
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4973, acc: 0.7812
loss: 0.5293, acc: 0.7679
loss: 0.5183, acc: 0.7891
loss: 0.5070, acc: 0.7923
loss: 0.5072, acc: 0.7940
loss: 0.5054, acc: 0.8009
loss: 0.5042, acc: 0.8027
loss: 0.5202, acc: 0.8007
loss: 0.5296, acc: 0.7984
loss: 0.5272, acc: 0.7965
loss: 0.5309, acc: 0.7939
loss: 0.5299, acc: 0.7928
loss: 0.5335, acc: 0.7868
loss: 0.5269, acc: 0.7892
loss: 0.5305, acc: 0.7878
loss: 0.5323, acc: 0.7873
loss: 0.5305, acc: 0.7873
loss: 0.5277, acc: 0.7881
loss: 0.5228, acc: 0.7901
loss: 0.5253, acc: 0.7867
loss: 0.5239, acc: 0.7877
loss: 0.5237, acc: 0.7865
loss: 0.5256, acc: 0.7849
> val_acc: 0.8295, val_f1: 0.7430
>> saved: peft/bert_lora/rest14//acc_0.8295_f1_0.743_230828-0537
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3660, acc: 0.8750
loss: 0.3620, acc: 0.8576
loss: 0.3491, acc: 0.8594
loss: 0.3658, acc: 0.8487
loss: 0.3768, acc: 0.8477
loss: 0.3943, acc: 0.8405
loss: 0.4052, acc: 0.8364
loss: 0.4015, acc: 0.8381
loss: 0.4107, acc: 0.8352
loss: 0.4111, acc: 0.8374
loss: 0.4190, acc: 0.8339
loss: 0.4245, acc: 0.8289
loss: 0.4285, acc: 0.8262
loss: 0.4271, acc: 0.8274
loss: 0.4240, acc: 0.8290
loss: 0.4179, acc: 0.8315
loss: 0.4207, acc: 0.8292
loss: 0.4207, acc: 0.8287
loss: 0.4223, acc: 0.8288
loss: 0.4227, acc: 0.8283
loss: 0.4186, acc: 0.8299
loss: 0.4193, acc: 0.8303
> val_acc: 0.8152, val_f1: 0.6920
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3184, acc: 0.8438
loss: 0.3405, acc: 0.8750
loss: 0.2950, acc: 0.8892
loss: 0.2693, acc: 0.9004
loss: 0.2766, acc: 0.8899
loss: 0.2960, acc: 0.8846
loss: 0.2999, acc: 0.8861
loss: 0.3119, acc: 0.8863
loss: 0.3217, acc: 0.8811
loss: 0.3217, acc: 0.8818
loss: 0.3212, acc: 0.8799
loss: 0.3222, acc: 0.8767
loss: 0.3304, acc: 0.8765
loss: 0.3365, acc: 0.8750
loss: 0.3369, acc: 0.8754
loss: 0.3378, acc: 0.8758
loss: 0.3356, acc: 0.8769
loss: 0.3372, acc: 0.8765
loss: 0.3367, acc: 0.8760
loss: 0.3373, acc: 0.8757
loss: 0.3393, acc: 0.8747
loss: 0.3398, acc: 0.8741
loss: 0.3417, acc: 0.8736
> val_acc: 0.8321, val_f1: 0.7401
>> saved: peft/bert_lora/rest14//acc_0.8321_f1_0.7401_230828-0538
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2278, acc: 0.8750
loss: 0.2167, acc: 0.9062
loss: 0.2282, acc: 0.9087
loss: 0.2161, acc: 0.9115
loss: 0.2273, acc: 0.9049
loss: 0.2508, acc: 0.9018
loss: 0.2437, acc: 0.9062
loss: 0.2421, acc: 0.9046
loss: 0.2427, acc: 0.9041
loss: 0.2453, acc: 0.9030
loss: 0.2513, acc: 0.9027
loss: 0.2570, acc: 0.9009
loss: 0.2611, acc: 0.8998
loss: 0.2666, acc: 0.8980
loss: 0.2713, acc: 0.8964
loss: 0.2754, acc: 0.8946
loss: 0.2758, acc: 0.8931
loss: 0.2789, acc: 0.8931
loss: 0.2820, acc: 0.8918
loss: 0.2853, acc: 0.8897
loss: 0.2817, acc: 0.8911
loss: 0.2876, acc: 0.8895
loss: 0.2877, acc: 0.8894
> val_acc: 0.8286, val_f1: 0.7463
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3362, acc: 0.8688
loss: 0.2784, acc: 0.8875
loss: 0.2706, acc: 0.8958
loss: 0.2454, acc: 0.9109
loss: 0.2224, acc: 0.9163
loss: 0.2264, acc: 0.9125
loss: 0.2364, acc: 0.9089
loss: 0.2383, acc: 0.9047
loss: 0.2412, acc: 0.9021
loss: 0.2378, acc: 0.9038
loss: 0.2377, acc: 0.9040
loss: 0.2472, acc: 0.8995
loss: 0.2466, acc: 0.9014
loss: 0.2469, acc: 0.9018
loss: 0.2478, acc: 0.9012
loss: 0.2588, acc: 0.8980
loss: 0.2620, acc: 0.8971
loss: 0.2680, acc: 0.8955
loss: 0.2638, acc: 0.8974
loss: 0.2656, acc: 0.8972
loss: 0.2690, acc: 0.8961
loss: 0.2698, acc: 0.8960
> val_acc: 0.8330, val_f1: 0.7585
>> saved: peft/bert_lora/rest14//acc_0.833_f1_0.7585_230828-0539
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1813, acc: 0.9375
loss: 0.2337, acc: 0.9196
loss: 0.1837, acc: 0.9349
loss: 0.1931, acc: 0.9301
loss: 0.2068, acc: 0.9219
loss: 0.2197, acc: 0.9190
loss: 0.2170, acc: 0.9170
loss: 0.2233, acc: 0.9147
loss: 0.2282, acc: 0.9137
loss: 0.2265, acc: 0.9149
loss: 0.2323, acc: 0.9129
loss: 0.2382, acc: 0.9123
loss: 0.2367, acc: 0.9133
loss: 0.2370, acc: 0.9118
loss: 0.2345, acc: 0.9123
loss: 0.2348, acc: 0.9115
loss: 0.2371, acc: 0.9112
loss: 0.2434, acc: 0.9084
loss: 0.2422, acc: 0.9093
loss: 0.2409, acc: 0.9108
loss: 0.2403, acc: 0.9105
loss: 0.2406, acc: 0.9100
loss: 0.2432, acc: 0.9090
> val_acc: 0.8429, val_f1: 0.7610
>> saved: peft/bert_lora/rest14//acc_0.8429_f1_0.761_230828-0540
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1378, acc: 0.9453
loss: 0.1471, acc: 0.9479
loss: 0.1481, acc: 0.9487
loss: 0.1845, acc: 0.9391
loss: 0.1762, acc: 0.9427
loss: 0.1743, acc: 0.9397
loss: 0.1845, acc: 0.9347
loss: 0.1801, acc: 0.9375
loss: 0.1763, acc: 0.9389
loss: 0.1821, acc: 0.9369
loss: 0.1807, acc: 0.9369
loss: 0.1856, acc: 0.9338
loss: 0.1921, acc: 0.9312
loss: 0.1869, acc: 0.9339
loss: 0.1855, acc: 0.9350
loss: 0.1856, acc: 0.9351
loss: 0.1915, acc: 0.9345
loss: 0.1943, acc: 0.9326
loss: 0.1979, acc: 0.9309
loss: 0.1989, acc: 0.9302
loss: 0.1990, acc: 0.9288
loss: 0.2025, acc: 0.9272
> val_acc: 0.8313, val_f1: 0.7541
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0954, acc: 1.0000
loss: 0.1707, acc: 0.9323
loss: 0.1548, acc: 0.9347
loss: 0.1253, acc: 0.9531
loss: 0.1371, acc: 0.9479
loss: 0.1441, acc: 0.9471
loss: 0.1425, acc: 0.9506
loss: 0.1592, acc: 0.9410
loss: 0.1544, acc: 0.9421
loss: 0.1520, acc: 0.9436
loss: 0.1561, acc: 0.9424
loss: 0.1551, acc: 0.9436
loss: 0.1590, acc: 0.9416
loss: 0.1580, acc: 0.9408
loss: 0.1555, acc: 0.9419
loss: 0.1586, acc: 0.9400
loss: 0.1575, acc: 0.9406
loss: 0.1588, acc: 0.9397
loss: 0.1597, acc: 0.9385
loss: 0.1654, acc: 0.9368
loss: 0.1697, acc: 0.9353
loss: 0.1714, acc: 0.9351
loss: 0.1712, acc: 0.9347
> val_acc: 0.8152, val_f1: 0.7301
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1479, acc: 0.9479
loss: 0.1541, acc: 0.9570
loss: 0.1634, acc: 0.9495
loss: 0.1607, acc: 0.9514
loss: 0.1611, acc: 0.9484
loss: 0.1500, acc: 0.9509
loss: 0.1506, acc: 0.9517
loss: 0.1515, acc: 0.9523
loss: 0.1491, acc: 0.9528
loss: 0.1489, acc: 0.9518
loss: 0.1556, acc: 0.9499
loss: 0.1531, acc: 0.9494
loss: 0.1551, acc: 0.9489
loss: 0.1510, acc: 0.9499
loss: 0.1484, acc: 0.9512
loss: 0.1500, acc: 0.9503
loss: 0.1604, acc: 0.9458
loss: 0.1612, acc: 0.9446
loss: 0.1592, acc: 0.9439
loss: 0.1580, acc: 0.9436
loss: 0.1616, acc: 0.9421
loss: 0.1622, acc: 0.9424
loss: 0.1639, acc: 0.9424
> val_acc: 0.8366, val_f1: 0.7528
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.1364, acc: 0.9500
loss: 0.1412, acc: 0.9437
loss: 0.1519, acc: 0.9417
loss: 0.1394, acc: 0.9484
loss: 0.1533, acc: 0.9437
loss: 0.1545, acc: 0.9437
loss: 0.1640, acc: 0.9411
loss: 0.1597, acc: 0.9414
loss: 0.1603, acc: 0.9396
loss: 0.1676, acc: 0.9387
loss: 0.1746, acc: 0.9381
loss: 0.1780, acc: 0.9365
loss: 0.1729, acc: 0.9380
loss: 0.1675, acc: 0.9402
loss: 0.1670, acc: 0.9413
loss: 0.1708, acc: 0.9406
loss: 0.1733, acc: 0.9401
loss: 0.1714, acc: 0.9413
loss: 0.1742, acc: 0.9401
loss: 0.1753, acc: 0.9406
loss: 0.1721, acc: 0.9420
loss: 0.1705, acc: 0.9420
> val_acc: 0.8357, val_f1: 0.7609
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1649, acc: 0.9531
loss: 0.1333, acc: 0.9420
loss: 0.1382, acc: 0.9401
loss: 0.1249, acc: 0.9485
loss: 0.1263, acc: 0.9432
loss: 0.1297, acc: 0.9421
loss: 0.1639, acc: 0.9307
loss: 0.1636, acc: 0.9307
loss: 0.1701, acc: 0.9308
loss: 0.1722, acc: 0.9295
loss: 0.1698, acc: 0.9333
loss: 0.1747, acc: 0.9315
loss: 0.1810, acc: 0.9289
loss: 0.1916, acc: 0.9272
loss: 0.1960, acc: 0.9253
loss: 0.1935, acc: 0.9274
loss: 0.1966, acc: 0.9264
loss: 0.1958, acc: 0.9264
loss: 0.1955, acc: 0.9276
loss: 0.1933, acc: 0.9285
loss: 0.1924, acc: 0.9286
loss: 0.1891, acc: 0.9302
loss: 0.1880, acc: 0.9305
> val_acc: 0.8223, val_f1: 0.7340
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8429, test_f1: 0.7610
>> test_acc: 0.8482, test_f1: 0.7724
>> test_acc: 0.8500, test_f1: 0.7717
>> test_acc: 0.8429, test_f1: 0.7610

>> avg_test_acc: 0.8470, avg_test_f1: 0.7684
>> max_test_acc: 0.8500, max_test_f1: 0.7724
