Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 439079424
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1254, acc: 0.3750
loss: 1.1031, acc: 0.4094
loss: 1.0837, acc: 0.4333
loss: 1.0601, acc: 0.4547
loss: 1.0377, acc: 0.4775
loss: 1.0129, acc: 0.4990
loss: 0.9810, acc: 0.5295
loss: 0.9425, acc: 0.5570
loss: 0.9065, acc: 0.5785
loss: 0.8845, acc: 0.5919
loss: 0.8711, acc: 0.6023
loss: 0.8526, acc: 0.6161
loss: 0.8410, acc: 0.6245
loss: 0.8259, acc: 0.6339
> val_acc: 0.7116, val_f1: 0.6220
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7116_f1_0.622_230828-0341.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.3706, acc: 0.8906
loss: 0.4587, acc: 0.8304
loss: 0.5223, acc: 0.7917
loss: 0.5355, acc: 0.7831
loss: 0.5240, acc: 0.7898
loss: 0.5277, acc: 0.7905
loss: 0.5202, acc: 0.7959
loss: 0.5125, acc: 0.7981
loss: 0.5215, acc: 0.7939
loss: 0.5139, acc: 0.7999
loss: 0.5222, acc: 0.7981
loss: 0.5229, acc: 0.7993
loss: 0.5195, acc: 0.8014
loss: 0.5187, acc: 0.8013
loss: 0.5163, acc: 0.8021
> val_acc: 0.7884, val_f1: 0.7501
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7884_f1_0.7501_230828-0342.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4198, acc: 0.8516
loss: 0.3434, acc: 0.8819
loss: 0.3413, acc: 0.8817
loss: 0.3609, acc: 0.8750
loss: 0.3390, acc: 0.8815
loss: 0.3367, acc: 0.8836
loss: 0.3260, acc: 0.8888
loss: 0.3264, acc: 0.8878
loss: 0.3242, acc: 0.8871
loss: 0.3236, acc: 0.8871
loss: 0.3315, acc: 0.8837
loss: 0.3366, acc: 0.8803
loss: 0.3433, acc: 0.8770
loss: 0.3424, acc: 0.8786
> val_acc: 0.7398, val_f1: 0.6619
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1303, acc: 0.9688
loss: 0.2181, acc: 0.9375
loss: 0.2523, acc: 0.9261
loss: 0.2317, acc: 0.9316
loss: 0.2326, acc: 0.9256
loss: 0.2242, acc: 0.9267
loss: 0.2280, acc: 0.9234
loss: 0.2200, acc: 0.9288
loss: 0.2208, acc: 0.9268
loss: 0.2174, acc: 0.9293
loss: 0.2171, acc: 0.9283
loss: 0.2190, acc: 0.9258
loss: 0.2176, acc: 0.9242
loss: 0.2197, acc: 0.9252
loss: 0.2144, acc: 0.9256
> val_acc: 0.7774, val_f1: 0.7273
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0753, acc: 0.9792
loss: 0.1017, acc: 0.9688
loss: 0.1272, acc: 0.9591
loss: 0.1268, acc: 0.9583
loss: 0.1450, acc: 0.9552
loss: 0.1338, acc: 0.9598
loss: 0.1222, acc: 0.9631
loss: 0.1250, acc: 0.9605
loss: 0.1212, acc: 0.9615
loss: 0.1167, acc: 0.9635
loss: 0.1231, acc: 0.9587
loss: 0.1233, acc: 0.9574
loss: 0.1282, acc: 0.9559
loss: 0.1284, acc: 0.9559
loss: 0.1293, acc: 0.9558
> val_acc: 0.7524, val_f1: 0.7219
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0776, acc: 0.9750
loss: 0.0866, acc: 0.9719
loss: 0.0767, acc: 0.9771
loss: 0.0773, acc: 0.9781
loss: 0.0783, acc: 0.9750
loss: 0.0807, acc: 0.9760
loss: 0.0883, acc: 0.9750
loss: 0.0894, acc: 0.9727
loss: 0.0856, acc: 0.9736
loss: 0.0907, acc: 0.9719
loss: 0.0887, acc: 0.9727
loss: 0.0883, acc: 0.9734
loss: 0.0925, acc: 0.9716
loss: 0.0961, acc: 0.9696
> val_acc: 0.7602, val_f1: 0.6949
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1432, acc: 0.9219
loss: 0.0925, acc: 0.9598
loss: 0.0794, acc: 0.9714
loss: 0.0744, acc: 0.9724
loss: 0.0659, acc: 0.9759
loss: 0.0666, acc: 0.9745
loss: 0.0645, acc: 0.9756
loss: 0.0636, acc: 0.9738
loss: 0.0615, acc: 0.9754
loss: 0.0573, acc: 0.9774
loss: 0.0561, acc: 0.9772
loss: 0.0572, acc: 0.9786
loss: 0.0631, acc: 0.9773
loss: 0.0638, acc: 0.9762
loss: 0.0673, acc: 0.9757
> val_acc: 0.7602, val_f1: 0.7021
>> early stop.
>> test_acc: 0.7884, test_f1: 0.7501
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2253822464
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0840, acc: 0.3438
loss: 1.0481, acc: 0.4125
loss: 1.0369, acc: 0.4417
loss: 1.0316, acc: 0.4609
loss: 1.0114, acc: 0.4875
loss: 0.9933, acc: 0.5104
loss: 0.9500, acc: 0.5429
loss: 0.9222, acc: 0.5609
loss: 0.9032, acc: 0.5764
loss: 0.8753, acc: 0.5925
loss: 0.8581, acc: 0.6057
loss: 0.8447, acc: 0.6161
loss: 0.8298, acc: 0.6260
loss: 0.8109, acc: 0.6375
> val_acc: 0.7445, val_f1: 0.6678
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7445_f1_0.6678_230828-0345.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4271, acc: 0.8438
loss: 0.4733, acc: 0.8259
loss: 0.4890, acc: 0.8281
loss: 0.4916, acc: 0.8217
loss: 0.4893, acc: 0.8210
loss: 0.4990, acc: 0.8148
loss: 0.4914, acc: 0.8223
loss: 0.4792, acc: 0.8277
loss: 0.4843, acc: 0.8192
loss: 0.4892, acc: 0.8165
loss: 0.4907, acc: 0.8155
loss: 0.4951, acc: 0.8125
loss: 0.4956, acc: 0.8115
loss: 0.4930, acc: 0.8130
loss: 0.4850, acc: 0.8151
> val_acc: 0.7555, val_f1: 0.6975
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7555_f1_0.6975_230828-0345.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2232, acc: 0.9375
loss: 0.3256, acc: 0.8854
loss: 0.3359, acc: 0.8772
loss: 0.3386, acc: 0.8766
loss: 0.3327, acc: 0.8802
loss: 0.3269, acc: 0.8825
loss: 0.3239, acc: 0.8842
loss: 0.3074, acc: 0.8886
loss: 0.3141, acc: 0.8878
loss: 0.3194, acc: 0.8846
loss: 0.3251, acc: 0.8796
loss: 0.3197, acc: 0.8803
loss: 0.3198, acc: 0.8799
loss: 0.3145, acc: 0.8827
> val_acc: 0.7806, val_f1: 0.7265
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7806_f1_0.7265_230828-0346.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2317, acc: 0.9062
loss: 0.2406, acc: 0.9167
loss: 0.2077, acc: 0.9290
loss: 0.2030, acc: 0.9258
loss: 0.2013, acc: 0.9241
loss: 0.2016, acc: 0.9243
loss: 0.1892, acc: 0.9304
loss: 0.1863, acc: 0.9340
loss: 0.1770, acc: 0.9390
loss: 0.1880, acc: 0.9348
loss: 0.1916, acc: 0.9332
loss: 0.2016, acc: 0.9314
loss: 0.2063, acc: 0.9298
loss: 0.2023, acc: 0.9318
loss: 0.2034, acc: 0.9309
> val_acc: 0.7821, val_f1: 0.7422
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7821_f1_0.7422_230828-0346.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0710, acc: 0.9896
loss: 0.0628, acc: 0.9922
loss: 0.0799, acc: 0.9784
loss: 0.0921, acc: 0.9705
loss: 0.0940, acc: 0.9660
loss: 0.1066, acc: 0.9643
loss: 0.1016, acc: 0.9669
loss: 0.1065, acc: 0.9638
loss: 0.1041, acc: 0.9658
loss: 0.1139, acc: 0.9622
loss: 0.1125, acc: 0.9640
loss: 0.1093, acc: 0.9655
loss: 0.1096, acc: 0.9648
loss: 0.1102, acc: 0.9642
loss: 0.1090, acc: 0.9643
> val_acc: 0.7476, val_f1: 0.6960
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0434, acc: 0.9875
loss: 0.0790, acc: 0.9812
loss: 0.0897, acc: 0.9729
loss: 0.0857, acc: 0.9734
loss: 0.0766, acc: 0.9762
loss: 0.0828, acc: 0.9740
loss: 0.0942, acc: 0.9696
loss: 0.0903, acc: 0.9711
loss: 0.0959, acc: 0.9681
loss: 0.0953, acc: 0.9669
loss: 0.0927, acc: 0.9682
loss: 0.0965, acc: 0.9661
loss: 0.0936, acc: 0.9673
loss: 0.0938, acc: 0.9674
> val_acc: 0.7743, val_f1: 0.7258
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0480, acc: 0.9844
loss: 0.0742, acc: 0.9777
loss: 0.0620, acc: 0.9792
loss: 0.0478, acc: 0.9853
loss: 0.0414, acc: 0.9872
loss: 0.0446, acc: 0.9873
loss: 0.0581, acc: 0.9834
loss: 0.0622, acc: 0.9814
loss: 0.0583, acc: 0.9829
loss: 0.0582, acc: 0.9827
loss: 0.0566, acc: 0.9826
loss: 0.0566, acc: 0.9836
loss: 0.0546, acc: 0.9834
loss: 0.0625, acc: 0.9804
loss: 0.0663, acc: 0.9792
> val_acc: 0.7524, val_f1: 0.6957
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0345, acc: 0.9922
loss: 0.0258, acc: 0.9965
loss: 0.0220, acc: 0.9955
loss: 0.0216, acc: 0.9951
loss: 0.0322, acc: 0.9896
loss: 0.0320, acc: 0.9881
loss: 0.0435, acc: 0.9835
loss: 0.0462, acc: 0.9840
loss: 0.0508, acc: 0.9830
loss: 0.0512, acc: 0.9834
loss: 0.0521, acc: 0.9832
loss: 0.0522, acc: 0.9831
loss: 0.0546, acc: 0.9829
loss: 0.0551, acc: 0.9823
> val_acc: 0.7618, val_f1: 0.6970
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0320, acc: 0.9688
loss: 0.0280, acc: 0.9896
loss: 0.0274, acc: 0.9886
loss: 0.0221, acc: 0.9922
loss: 0.0240, acc: 0.9911
loss: 0.0278, acc: 0.9880
loss: 0.0321, acc: 0.9879
loss: 0.0357, acc: 0.9878
loss: 0.0402, acc: 0.9855
loss: 0.0404, acc: 0.9851
loss: 0.0432, acc: 0.9835
loss: 0.0447, acc: 0.9827
loss: 0.0448, acc: 0.9831
loss: 0.0458, acc: 0.9830
loss: 0.0434, acc: 0.9842
> val_acc: 0.7727, val_f1: 0.7214
>> early stop.
>> test_acc: 0.7821, test_f1: 0.7422
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2261162496
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1495, acc: 0.4750
loss: 1.1288, acc: 0.4562
loss: 1.0979, acc: 0.4625
loss: 1.0677, acc: 0.4781
loss: 1.0485, acc: 0.4925
loss: 1.0128, acc: 0.5219
loss: 0.9816, acc: 0.5464
loss: 0.9629, acc: 0.5602
loss: 0.9338, acc: 0.5799
loss: 0.9102, acc: 0.5975
loss: 0.8997, acc: 0.6028
loss: 0.8943, acc: 0.6052
loss: 0.8771, acc: 0.6154
loss: 0.8616, acc: 0.6246
> val_acc: 0.6959, val_f1: 0.5616
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.6959_f1_0.5616_230828-0349.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5341, acc: 0.7969
loss: 0.6055, acc: 0.7723
loss: 0.6021, acc: 0.7656
loss: 0.5952, acc: 0.7794
loss: 0.5910, acc: 0.7827
loss: 0.5580, acc: 0.7940
loss: 0.5635, acc: 0.7900
loss: 0.5631, acc: 0.7880
loss: 0.5554, acc: 0.7872
loss: 0.5501, acc: 0.7872
loss: 0.5436, acc: 0.7915
loss: 0.5417, acc: 0.7917
loss: 0.5441, acc: 0.7903
loss: 0.5452, acc: 0.7896
loss: 0.5416, acc: 0.7912
> val_acc: 0.7508, val_f1: 0.6841
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7508_f1_0.6841_230828-0349.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4260, acc: 0.8359
loss: 0.3742, acc: 0.8750
loss: 0.3625, acc: 0.8728
loss: 0.3572, acc: 0.8799
loss: 0.3700, acc: 0.8724
loss: 0.3840, acc: 0.8631
loss: 0.3786, acc: 0.8649
loss: 0.3665, acc: 0.8734
loss: 0.3631, acc: 0.8736
loss: 0.3694, acc: 0.8693
loss: 0.3753, acc: 0.8657
loss: 0.3744, acc: 0.8671
loss: 0.3775, acc: 0.8647
loss: 0.3751, acc: 0.8637
> val_acc: 0.7351, val_f1: 0.6754
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1559, acc: 1.0000
loss: 0.2111, acc: 0.9479
loss: 0.1861, acc: 0.9489
loss: 0.1962, acc: 0.9395
loss: 0.2018, acc: 0.9390
loss: 0.2202, acc: 0.9315
loss: 0.2320, acc: 0.9234
loss: 0.2288, acc: 0.9201
loss: 0.2306, acc: 0.9169
loss: 0.2285, acc: 0.9158
loss: 0.2330, acc: 0.9148
loss: 0.2381, acc: 0.9124
loss: 0.2377, acc: 0.9124
loss: 0.2388, acc: 0.9105
loss: 0.2387, acc: 0.9111
> val_acc: 0.7602, val_f1: 0.7167
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7602_f1_0.7167_230828-0350.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1088, acc: 0.9688
loss: 0.1375, acc: 0.9453
loss: 0.1472, acc: 0.9543
loss: 0.1565, acc: 0.9497
loss: 0.1641, acc: 0.9429
loss: 0.1622, acc: 0.9442
loss: 0.1641, acc: 0.9460
loss: 0.1628, acc: 0.9457
loss: 0.1544, acc: 0.9491
loss: 0.1548, acc: 0.9479
loss: 0.1511, acc: 0.9475
loss: 0.1499, acc: 0.9483
loss: 0.1488, acc: 0.9484
loss: 0.1501, acc: 0.9476
loss: 0.1536, acc: 0.9450
> val_acc: 0.7539, val_f1: 0.7031
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.1296, acc: 0.9563
loss: 0.1300, acc: 0.9563
loss: 0.1077, acc: 0.9646
loss: 0.0938, acc: 0.9719
loss: 0.0849, acc: 0.9750
loss: 0.0938, acc: 0.9750
loss: 0.1020, acc: 0.9723
loss: 0.0998, acc: 0.9727
loss: 0.1008, acc: 0.9729
loss: 0.1015, acc: 0.9725
loss: 0.1044, acc: 0.9705
loss: 0.1052, acc: 0.9688
loss: 0.1053, acc: 0.9697
loss: 0.1080, acc: 0.9692
> val_acc: 0.7618, val_f1: 0.7098
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7618_f1_0.7098_230828-0351.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0314, acc: 0.9844
loss: 0.0461, acc: 0.9911
loss: 0.0648, acc: 0.9844
loss: 0.0597, acc: 0.9835
loss: 0.0676, acc: 0.9830
loss: 0.0722, acc: 0.9815
loss: 0.0717, acc: 0.9824
loss: 0.0726, acc: 0.9823
loss: 0.0678, acc: 0.9836
loss: 0.0663, acc: 0.9827
loss: 0.0660, acc: 0.9820
loss: 0.0660, acc: 0.9808
loss: 0.0663, acc: 0.9803
loss: 0.0691, acc: 0.9781
loss: 0.0680, acc: 0.9787
> val_acc: 0.7696, val_f1: 0.7285
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7696_f1_0.7285_230828-0352.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0492, acc: 0.9688
loss: 0.0382, acc: 0.9757
loss: 0.0489, acc: 0.9777
loss: 0.0483, acc: 0.9803
loss: 0.0503, acc: 0.9805
loss: 0.0453, acc: 0.9828
loss: 0.0462, acc: 0.9816
loss: 0.0424, acc: 0.9840
loss: 0.0435, acc: 0.9844
loss: 0.0544, acc: 0.9821
loss: 0.0567, acc: 0.9821
loss: 0.0538, acc: 0.9831
loss: 0.0580, acc: 0.9824
loss: 0.0592, acc: 0.9819
> val_acc: 0.7884, val_f1: 0.7492
>> saved: /media/b115/Backup/NLP/bert/lap14/acc_0.7884_f1_0.7492_230828-0352.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0381, acc: 1.0000
loss: 0.0461, acc: 0.9896
loss: 0.0660, acc: 0.9716
loss: 0.0775, acc: 0.9688
loss: 0.0829, acc: 0.9702
loss: 0.0770, acc: 0.9724
loss: 0.0714, acc: 0.9748
loss: 0.0650, acc: 0.9766
loss: 0.0618, acc: 0.9787
loss: 0.0579, acc: 0.9796
loss: 0.0585, acc: 0.9792
loss: 0.0583, acc: 0.9794
loss: 0.0543, acc: 0.9810
loss: 0.0546, acc: 0.9801
loss: 0.0586, acc: 0.9793
> val_acc: 0.7790, val_f1: 0.7283
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0258, acc: 0.9896
loss: 0.0254, acc: 0.9922
loss: 0.0184, acc: 0.9952
loss: 0.0181, acc: 0.9965
loss: 0.0342, acc: 0.9905
loss: 0.0396, acc: 0.9888
loss: 0.0411, acc: 0.9886
loss: 0.0402, acc: 0.9885
loss: 0.0456, acc: 0.9862
loss: 0.0448, acc: 0.9857
loss: 0.0444, acc: 0.9858
loss: 0.0449, acc: 0.9865
loss: 0.0473, acc: 0.9856
loss: 0.0458, acc: 0.9858
loss: 0.0470, acc: 0.9845
> val_acc: 0.7680, val_f1: 0.7119
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0275, acc: 0.9938
loss: 0.0416, acc: 0.9875
loss: 0.0354, acc: 0.9896
loss: 0.0318, acc: 0.9906
loss: 0.0384, acc: 0.9888
loss: 0.0489, acc: 0.9865
loss: 0.0548, acc: 0.9848
loss: 0.0505, acc: 0.9867
loss: 0.0469, acc: 0.9875
loss: 0.0469, acc: 0.9869
loss: 0.0451, acc: 0.9869
loss: 0.0434, acc: 0.9870
loss: 0.0429, acc: 0.9865
loss: 0.0406, acc: 0.9875
> val_acc: 0.7524, val_f1: 0.7076
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.0036, acc: 1.0000
loss: 0.0095, acc: 1.0000
loss: 0.0108, acc: 1.0000
loss: 0.0109, acc: 0.9982
loss: 0.0159, acc: 0.9957
loss: 0.0234, acc: 0.9919
loss: 0.0273, acc: 0.9893
loss: 0.0260, acc: 0.9899
loss: 0.0251, acc: 0.9903
loss: 0.0276, acc: 0.9894
loss: 0.0365, acc: 0.9862
loss: 0.0362, acc: 0.9868
loss: 0.0361, acc: 0.9864
loss: 0.0361, acc: 0.9865
loss: 0.0344, acc: 0.9870
> val_acc: 0.7727, val_f1: 0.7264
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.0346, acc: 0.9844
loss: 0.0308, acc: 0.9896
loss: 0.0323, acc: 0.9866
loss: 0.0317, acc: 0.9868
loss: 0.0296, acc: 0.9883
loss: 0.0278, acc: 0.9892
loss: 0.0295, acc: 0.9871
loss: 0.0284, acc: 0.9872
loss: 0.0295, acc: 0.9865
loss: 0.0282, acc: 0.9872
loss: 0.0281, acc: 0.9873
loss: 0.0292, acc: 0.9868
loss: 0.0302, acc: 0.9868
loss: 0.0292, acc: 0.9869
> val_acc: 0.7524, val_f1: 0.6839
>> early stop.
>> test_acc: 0.7884, test_f1: 0.7492
>> test_acc: 0.7884, test_f1: 0.7501
>> test_acc: 0.7821, test_f1: 0.7422
>> test_acc: 0.7884, test_f1: 0.7492

>> avg_test_acc: 0.7863, avg_test_f1: 0.7472
>> max_test_acc: 0.7884, max_test_f1: 0.7501
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 499894784
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0861, acc: 0.4062
loss: 1.0726, acc: 0.4313
loss: 1.0558, acc: 0.4542
loss: 1.0394, acc: 0.4859
loss: 0.9898, acc: 0.5262
loss: 0.9329, acc: 0.5656
loss: 0.8857, acc: 0.5955
loss: 0.8688, acc: 0.6094
loss: 0.8641, acc: 0.6062
loss: 0.8406, acc: 0.6200
loss: 0.8212, acc: 0.6301
loss: 0.8046, acc: 0.6401
loss: 0.7937, acc: 0.6510
loss: 0.7840, acc: 0.6562
> val_acc: 0.7727, val_f1: 0.7118
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.7727_f1_0.7118_230828-0355.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6185, acc: 0.7969
loss: 0.5885, acc: 0.7857
loss: 0.5622, acc: 0.7943
loss: 0.5708, acc: 0.7776
loss: 0.5613, acc: 0.7884
loss: 0.5375, acc: 0.8009
loss: 0.5275, acc: 0.8086
loss: 0.5310, acc: 0.8083
loss: 0.5233, acc: 0.8103
loss: 0.5028, acc: 0.8185
loss: 0.5010, acc: 0.8209
loss: 0.4979, acc: 0.8213
loss: 0.4915, acc: 0.8216
loss: 0.4819, acc: 0.8246
loss: 0.4774, acc: 0.8255
> val_acc: 0.7868, val_f1: 0.7394
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.7868_f1_0.7394_230828-0356.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3164, acc: 0.9141
loss: 0.3231, acc: 0.9132
loss: 0.3673, acc: 0.8728
loss: 0.3901, acc: 0.8602
loss: 0.3923, acc: 0.8620
loss: 0.3804, acc: 0.8621
loss: 0.3572, acc: 0.8713
loss: 0.3456, acc: 0.8750
loss: 0.3290, acc: 0.8821
loss: 0.3257, acc: 0.8833
loss: 0.3328, acc: 0.8802
loss: 0.3303, acc: 0.8808
loss: 0.3235, acc: 0.8828
loss: 0.3199, acc: 0.8832
> val_acc: 0.8103, val_f1: 0.7707
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8103_f1_0.7707_230828-0356.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3605, acc: 0.8750
loss: 0.2984, acc: 0.9062
loss: 0.2681, acc: 0.9034
loss: 0.2798, acc: 0.8945
loss: 0.2707, acc: 0.8929
loss: 0.2647, acc: 0.8966
loss: 0.2532, acc: 0.9032
loss: 0.2499, acc: 0.9036
loss: 0.2372, acc: 0.9093
loss: 0.2247, acc: 0.9158
loss: 0.2292, acc: 0.9167
loss: 0.2348, acc: 0.9157
loss: 0.2363, acc: 0.9134
loss: 0.2324, acc: 0.9162
loss: 0.2391, acc: 0.9129
> val_acc: 0.7978, val_f1: 0.7543
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1792, acc: 0.9479
loss: 0.1595, acc: 0.9453
loss: 0.1490, acc: 0.9567
loss: 0.1580, acc: 0.9444
loss: 0.1497, acc: 0.9484
loss: 0.1608, acc: 0.9442
loss: 0.1699, acc: 0.9413
loss: 0.1687, acc: 0.9424
loss: 0.1627, acc: 0.9448
loss: 0.1538, acc: 0.9460
loss: 0.1541, acc: 0.9481
loss: 0.1573, acc: 0.9472
loss: 0.1534, acc: 0.9484
loss: 0.1549, acc: 0.9485
loss: 0.1569, acc: 0.9472
> val_acc: 0.8135, val_f1: 0.7752
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8135_f1_0.7752_230828-0357.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0601, acc: 0.9875
loss: 0.1102, acc: 0.9656
loss: 0.1326, acc: 0.9604
loss: 0.1312, acc: 0.9609
loss: 0.1245, acc: 0.9587
loss: 0.1237, acc: 0.9594
loss: 0.1256, acc: 0.9580
loss: 0.1197, acc: 0.9594
loss: 0.1216, acc: 0.9604
loss: 0.1198, acc: 0.9606
loss: 0.1196, acc: 0.9602
loss: 0.1165, acc: 0.9620
loss: 0.1186, acc: 0.9615
loss: 0.1214, acc: 0.9612
> val_acc: 0.8025, val_f1: 0.7600
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1126, acc: 0.9688
loss: 0.0695, acc: 0.9732
loss: 0.0783, acc: 0.9740
loss: 0.0772, acc: 0.9724
loss: 0.0732, acc: 0.9730
loss: 0.0808, acc: 0.9722
loss: 0.0819, acc: 0.9707
loss: 0.0744, acc: 0.9747
loss: 0.0771, acc: 0.9740
loss: 0.0807, acc: 0.9721
loss: 0.0814, acc: 0.9712
loss: 0.0846, acc: 0.9709
loss: 0.0824, acc: 0.9708
loss: 0.0823, acc: 0.9711
loss: 0.0828, acc: 0.9701
> val_acc: 0.8056, val_f1: 0.7625
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0248, acc: 0.9922
loss: 0.0528, acc: 0.9826
loss: 0.0557, acc: 0.9821
loss: 0.0613, acc: 0.9803
loss: 0.0544, acc: 0.9818
loss: 0.0686, acc: 0.9763
loss: 0.0765, acc: 0.9733
loss: 0.0775, acc: 0.9736
loss: 0.0748, acc: 0.9744
loss: 0.0749, acc: 0.9751
loss: 0.0760, acc: 0.9745
loss: 0.0713, acc: 0.9767
loss: 0.0775, acc: 0.9741
loss: 0.0741, acc: 0.9751
> val_acc: 0.8150, val_f1: 0.7804
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.815_f1_0.7804_230828-0358.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2069, acc: 0.9375
loss: 0.0815, acc: 0.9688
loss: 0.0540, acc: 0.9830
loss: 0.0600, acc: 0.9805
loss: 0.0502, acc: 0.9836
loss: 0.0503, acc: 0.9832
loss: 0.0481, acc: 0.9829
loss: 0.0475, acc: 0.9826
loss: 0.0475, acc: 0.9832
loss: 0.0468, acc: 0.9844
loss: 0.0550, acc: 0.9822
loss: 0.0596, acc: 0.9799
loss: 0.0657, acc: 0.9775
loss: 0.0673, acc: 0.9763
loss: 0.0683, acc: 0.9762
> val_acc: 0.8119, val_f1: 0.7780
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0459, acc: 0.9792
loss: 0.1570, acc: 0.9297
loss: 0.2141, acc: 0.9135
loss: 0.2188, acc: 0.9184
loss: 0.2087, acc: 0.9239
loss: 0.1838, acc: 0.9342
loss: 0.1692, acc: 0.9403
loss: 0.1617, acc: 0.9433
loss: 0.1593, acc: 0.9433
loss: 0.1563, acc: 0.9440
loss: 0.1490, acc: 0.9463
loss: 0.1428, acc: 0.9483
loss: 0.1385, acc: 0.9499
loss: 0.1351, acc: 0.9513
loss: 0.1345, acc: 0.9510
> val_acc: 0.8025, val_f1: 0.7669
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0351, acc: 0.9875
loss: 0.0277, acc: 0.9906
loss: 0.0259, acc: 0.9917
loss: 0.0278, acc: 0.9891
loss: 0.0414, acc: 0.9862
loss: 0.0457, acc: 0.9854
loss: 0.0482, acc: 0.9848
loss: 0.0498, acc: 0.9844
loss: 0.0478, acc: 0.9847
loss: 0.0477, acc: 0.9856
loss: 0.0477, acc: 0.9847
loss: 0.0528, acc: 0.9833
loss: 0.0511, acc: 0.9837
loss: 0.0496, acc: 0.9839
> val_acc: 0.8119, val_f1: 0.7742
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.0753, acc: 0.9688
loss: 0.0762, acc: 0.9777
loss: 0.0577, acc: 0.9818
loss: 0.0615, acc: 0.9798
loss: 0.0530, acc: 0.9830
loss: 0.0502, acc: 0.9826
loss: 0.0450, acc: 0.9844
loss: 0.0462, acc: 0.9840
loss: 0.0437, acc: 0.9844
loss: 0.0475, acc: 0.9827
loss: 0.0472, acc: 0.9820
loss: 0.0469, acc: 0.9830
loss: 0.0507, acc: 0.9819
loss: 0.0530, acc: 0.9818
loss: 0.0518, acc: 0.9818
> val_acc: 0.8072, val_f1: 0.7652
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.0247, acc: 0.9922
loss: 0.0227, acc: 0.9931
loss: 0.0318, acc: 0.9933
loss: 0.0286, acc: 0.9934
loss: 0.0311, acc: 0.9922
loss: 0.0283, acc: 0.9914
loss: 0.0312, acc: 0.9917
loss: 0.0297, acc: 0.9912
loss: 0.0291, acc: 0.9908
loss: 0.0291, acc: 0.9904
loss: 0.0286, acc: 0.9902
loss: 0.0276, acc: 0.9905
loss: 0.0299, acc: 0.9897
loss: 0.0341, acc: 0.9882
> val_acc: 0.7962, val_f1: 0.7536
>> early stop.
>> test_acc: 0.8150, test_f1: 0.7804
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2558812672
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0768, acc: 0.4250
loss: 1.0654, acc: 0.4188
loss: 1.0471, acc: 0.4583
loss: 1.0104, acc: 0.5047
loss: 0.9674, acc: 0.5400
loss: 0.9127, acc: 0.5760
loss: 0.8866, acc: 0.5902
loss: 0.8596, acc: 0.6070
loss: 0.8364, acc: 0.6250
loss: 0.8053, acc: 0.6425
loss: 0.7855, acc: 0.6528
loss: 0.7734, acc: 0.6620
loss: 0.7551, acc: 0.6726
loss: 0.7520, acc: 0.6750
> val_acc: 0.7398, val_f1: 0.7001
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.7398_f1_0.7001_230828-0401.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5304, acc: 0.7344
loss: 0.4306, acc: 0.8259
loss: 0.4794, acc: 0.8151
loss: 0.4703, acc: 0.8143
loss: 0.4445, acc: 0.8224
loss: 0.4302, acc: 0.8333
loss: 0.4362, acc: 0.8330
loss: 0.4471, acc: 0.8269
loss: 0.4429, acc: 0.8281
loss: 0.4473, acc: 0.8265
loss: 0.4472, acc: 0.8275
loss: 0.4463, acc: 0.8295
loss: 0.4349, acc: 0.8332
loss: 0.4282, acc: 0.8368
loss: 0.4328, acc: 0.8346
> val_acc: 0.8041, val_f1: 0.7708
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8041_f1_0.7708_230828-0402.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2896, acc: 0.8750
loss: 0.2635, acc: 0.9028
loss: 0.3016, acc: 0.8973
loss: 0.2958, acc: 0.8914
loss: 0.2966, acc: 0.8906
loss: 0.2938, acc: 0.8890
loss: 0.2917, acc: 0.8925
loss: 0.2857, acc: 0.8974
loss: 0.2824, acc: 0.9006
loss: 0.2766, acc: 0.9018
loss: 0.2724, acc: 0.9039
loss: 0.2712, acc: 0.9047
loss: 0.2757, acc: 0.9043
loss: 0.2796, acc: 0.9031
> val_acc: 0.8056, val_f1: 0.7650
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8056_f1_0.765_230828-0402.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1923, acc: 0.9688
loss: 0.2239, acc: 0.9271
loss: 0.2103, acc: 0.9261
loss: 0.2020, acc: 0.9316
loss: 0.1815, acc: 0.9390
loss: 0.1846, acc: 0.9375
loss: 0.1890, acc: 0.9355
loss: 0.1803, acc: 0.9384
loss: 0.1796, acc: 0.9390
loss: 0.1789, acc: 0.9402
loss: 0.1783, acc: 0.9400
loss: 0.1817, acc: 0.9392
loss: 0.1877, acc: 0.9355
loss: 0.1886, acc: 0.9361
loss: 0.1945, acc: 0.9331
> val_acc: 0.7994, val_f1: 0.7632
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1029, acc: 0.9792
loss: 0.1076, acc: 0.9609
loss: 0.1016, acc: 0.9663
loss: 0.1003, acc: 0.9635
loss: 0.1143, acc: 0.9592
loss: 0.1094, acc: 0.9632
loss: 0.1343, acc: 0.9545
loss: 0.1410, acc: 0.9498
loss: 0.1427, acc: 0.9491
loss: 0.1488, acc: 0.9486
loss: 0.1497, acc: 0.9487
loss: 0.1504, acc: 0.9499
loss: 0.1511, acc: 0.9494
loss: 0.1535, acc: 0.9499
loss: 0.1493, acc: 0.9519
> val_acc: 0.7947, val_f1: 0.7612
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0800, acc: 0.9563
loss: 0.0806, acc: 0.9656
loss: 0.0999, acc: 0.9688
loss: 0.1083, acc: 0.9672
loss: 0.0951, acc: 0.9700
loss: 0.1003, acc: 0.9667
loss: 0.0947, acc: 0.9688
loss: 0.0915, acc: 0.9680
loss: 0.0986, acc: 0.9660
loss: 0.0977, acc: 0.9663
loss: 0.1018, acc: 0.9665
loss: 0.1017, acc: 0.9672
loss: 0.1065, acc: 0.9654
loss: 0.1120, acc: 0.9621
> val_acc: 0.7994, val_f1: 0.7580
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1669, acc: 0.9375
loss: 0.1135, acc: 0.9732
loss: 0.1067, acc: 0.9740
loss: 0.1007, acc: 0.9743
loss: 0.0917, acc: 0.9744
loss: 0.1010, acc: 0.9688
loss: 0.1067, acc: 0.9668
loss: 0.1054, acc: 0.9662
loss: 0.1035, acc: 0.9673
loss: 0.0989, acc: 0.9681
loss: 0.1036, acc: 0.9645
loss: 0.1061, acc: 0.9644
loss: 0.1032, acc: 0.9652
loss: 0.1046, acc: 0.9650
loss: 0.1009, acc: 0.9666
> val_acc: 0.8025, val_f1: 0.7650
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0276, acc: 0.9922
loss: 0.0441, acc: 0.9826
loss: 0.0693, acc: 0.9732
loss: 0.0742, acc: 0.9688
loss: 0.0724, acc: 0.9701
loss: 0.0645, acc: 0.9741
loss: 0.0667, acc: 0.9724
loss: 0.0659, acc: 0.9736
loss: 0.0670, acc: 0.9709
loss: 0.0661, acc: 0.9719
loss: 0.0789, acc: 0.9676
loss: 0.0810, acc: 0.9682
loss: 0.0818, acc: 0.9688
loss: 0.0816, acc: 0.9697
> val_acc: 0.8103, val_f1: 0.7770
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8103_f1_0.777_230828-0404.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0296, acc: 1.0000
loss: 0.0895, acc: 0.9792
loss: 0.0683, acc: 0.9773
loss: 0.0669, acc: 0.9746
loss: 0.0665, acc: 0.9792
loss: 0.0726, acc: 0.9772
loss: 0.0700, acc: 0.9788
loss: 0.0676, acc: 0.9792
loss: 0.0698, acc: 0.9794
loss: 0.0661, acc: 0.9796
loss: 0.0723, acc: 0.9786
loss: 0.0760, acc: 0.9777
loss: 0.0809, acc: 0.9764
loss: 0.0773, acc: 0.9782
loss: 0.0750, acc: 0.9780
> val_acc: 0.8135, val_f1: 0.7866
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8135_f1_0.7866_230828-0405.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0826, acc: 0.9688
loss: 0.0503, acc: 0.9844
loss: 0.0655, acc: 0.9808
loss: 0.0566, acc: 0.9826
loss: 0.0572, acc: 0.9810
loss: 0.0533, acc: 0.9821
loss: 0.0582, acc: 0.9801
loss: 0.0586, acc: 0.9803
loss: 0.0670, acc: 0.9767
loss: 0.0626, acc: 0.9785
loss: 0.0640, acc: 0.9782
loss: 0.0644, acc: 0.9779
loss: 0.0660, acc: 0.9772
loss: 0.0757, acc: 0.9743
loss: 0.0776, acc: 0.9738
> val_acc: 0.8182, val_f1: 0.7850
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8182_f1_0.785_230828-0405.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0436, acc: 0.9938
loss: 0.0297, acc: 0.9969
loss: 0.0321, acc: 0.9958
loss: 0.0293, acc: 0.9969
loss: 0.0291, acc: 0.9950
loss: 0.0321, acc: 0.9917
loss: 0.0297, acc: 0.9929
loss: 0.0301, acc: 0.9922
loss: 0.0287, acc: 0.9924
loss: 0.0292, acc: 0.9925
loss: 0.0301, acc: 0.9909
loss: 0.0301, acc: 0.9906
loss: 0.0362, acc: 0.9885
loss: 0.0392, acc: 0.9879
> val_acc: 0.7947, val_f1: 0.7458
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.0237, acc: 0.9844
loss: 0.0154, acc: 0.9955
loss: 0.0199, acc: 0.9948
loss: 0.0231, acc: 0.9926
loss: 0.0381, acc: 0.9872
loss: 0.0407, acc: 0.9861
loss: 0.0401, acc: 0.9863
loss: 0.0399, acc: 0.9856
loss: 0.0365, acc: 0.9874
loss: 0.0438, acc: 0.9847
loss: 0.0425, acc: 0.9850
loss: 0.0399, acc: 0.9863
loss: 0.0420, acc: 0.9859
loss: 0.0415, acc: 0.9860
loss: 0.0422, acc: 0.9861
> val_acc: 0.8182, val_f1: 0.7861
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.0289, acc: 0.9922
loss: 0.0251, acc: 0.9931
loss: 0.0215, acc: 0.9933
loss: 0.0234, acc: 0.9901
loss: 0.0198, acc: 0.9922
loss: 0.0210, acc: 0.9914
loss: 0.0241, acc: 0.9908
loss: 0.0268, acc: 0.9896
loss: 0.0289, acc: 0.9886
loss: 0.0272, acc: 0.9892
loss: 0.0343, acc: 0.9873
loss: 0.0354, acc: 0.9873
loss: 0.0345, acc: 0.9878
loss: 0.0376, acc: 0.9860
> val_acc: 0.7947, val_f1: 0.7491
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.0074, acc: 1.0000
loss: 0.0454, acc: 0.9792
loss: 0.0361, acc: 0.9830
loss: 0.0332, acc: 0.9844
loss: 0.0307, acc: 0.9851
loss: 0.0290, acc: 0.9868
loss: 0.0281, acc: 0.9879
loss: 0.0278, acc: 0.9878
loss: 0.0297, acc: 0.9870
loss: 0.0339, acc: 0.9857
loss: 0.0371, acc: 0.9847
loss: 0.0394, acc: 0.9838
loss: 0.0430, acc: 0.9831
loss: 0.0459, acc: 0.9806
loss: 0.0461, acc: 0.9806
> val_acc: 0.7931, val_f1: 0.7463
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 0.0921, acc: 0.9792
loss: 0.0440, acc: 0.9922
loss: 0.0355, acc: 0.9928
loss: 0.0360, acc: 0.9913
loss: 0.0320, acc: 0.9918
loss: 0.0300, acc: 0.9911
loss: 0.0293, acc: 0.9896
loss: 0.0289, acc: 0.9893
loss: 0.0272, acc: 0.9898
loss: 0.0289, acc: 0.9902
loss: 0.0283, acc: 0.9906
loss: 0.0269, acc: 0.9908
loss: 0.0255, acc: 0.9911
loss: 0.0267, acc: 0.9908
loss: 0.0279, acc: 0.9905
> val_acc: 0.7962, val_f1: 0.7484
>> early stop.
>> test_acc: 0.8182, test_f1: 0.7850
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2569167360
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0686, acc: 0.4750
loss: 1.0851, acc: 0.4219
loss: 1.0849, acc: 0.4292
loss: 1.0763, acc: 0.4422
loss: 1.0567, acc: 0.4637
loss: 1.0243, acc: 0.4969
loss: 0.9866, acc: 0.5259
loss: 0.9356, acc: 0.5617
loss: 0.9163, acc: 0.5799
loss: 0.8940, acc: 0.5944
loss: 0.8627, acc: 0.6108
loss: 0.8469, acc: 0.6203
loss: 0.8274, acc: 0.6337
loss: 0.8142, acc: 0.6415
> val_acc: 0.7665, val_f1: 0.6942
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.7665_f1_0.6942_230828-0408.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5780, acc: 0.7812
loss: 0.5029, acc: 0.8259
loss: 0.5072, acc: 0.8047
loss: 0.5230, acc: 0.8033
loss: 0.5274, acc: 0.7955
loss: 0.5298, acc: 0.7975
loss: 0.5187, acc: 0.8008
loss: 0.5080, acc: 0.8057
loss: 0.4974, acc: 0.8110
loss: 0.5022, acc: 0.8078
loss: 0.5154, acc: 0.8017
loss: 0.5142, acc: 0.7999
loss: 0.5135, acc: 0.8004
loss: 0.5101, acc: 0.8027
loss: 0.5040, acc: 0.8064
> val_acc: 0.8088, val_f1: 0.7684
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8088_f1_0.7684_230828-0409.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3465, acc: 0.8750
loss: 0.3442, acc: 0.8681
loss: 0.3209, acc: 0.8750
loss: 0.3201, acc: 0.8783
loss: 0.3087, acc: 0.8789
loss: 0.3054, acc: 0.8836
loss: 0.3185, acc: 0.8768
loss: 0.3199, acc: 0.8726
loss: 0.3305, acc: 0.8686
loss: 0.3246, acc: 0.8718
loss: 0.3207, acc: 0.8738
loss: 0.3254, acc: 0.8729
loss: 0.3250, acc: 0.8745
loss: 0.3307, acc: 0.8727
> val_acc: 0.7962, val_f1: 0.7577
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4338, acc: 0.8750
loss: 0.2185, acc: 0.9479
loss: 0.1826, acc: 0.9545
loss: 0.2113, acc: 0.9316
loss: 0.2366, acc: 0.9167
loss: 0.2442, acc: 0.9135
loss: 0.2404, acc: 0.9133
loss: 0.2470, acc: 0.9106
loss: 0.2373, acc: 0.9146
loss: 0.2331, acc: 0.9164
loss: 0.2220, acc: 0.9203
loss: 0.2278, acc: 0.9180
loss: 0.2432, acc: 0.9139
loss: 0.2451, acc: 0.9138
loss: 0.2420, acc: 0.9151
> val_acc: 0.8103, val_f1: 0.7691
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8103_f1_0.7691_230828-0410.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1191, acc: 0.9688
loss: 0.1352, acc: 0.9570
loss: 0.1347, acc: 0.9567
loss: 0.1409, acc: 0.9514
loss: 0.1555, acc: 0.9402
loss: 0.1544, acc: 0.9408
loss: 0.1612, acc: 0.9384
loss: 0.1582, acc: 0.9400
loss: 0.1611, acc: 0.9397
loss: 0.1605, acc: 0.9408
loss: 0.1629, acc: 0.9393
loss: 0.1697, acc: 0.9380
loss: 0.1674, acc: 0.9395
loss: 0.1650, acc: 0.9407
loss: 0.1656, acc: 0.9407
> val_acc: 0.8135, val_f1: 0.7665
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8135_f1_0.7665_230828-0410.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.1618, acc: 0.9437
loss: 0.1461, acc: 0.9469
loss: 0.1553, acc: 0.9417
loss: 0.1651, acc: 0.9391
loss: 0.1503, acc: 0.9475
loss: 0.1507, acc: 0.9500
loss: 0.1524, acc: 0.9482
loss: 0.1489, acc: 0.9492
loss: 0.1412, acc: 0.9521
loss: 0.1396, acc: 0.9537
loss: 0.1304, acc: 0.9574
loss: 0.1291, acc: 0.9589
loss: 0.1313, acc: 0.9577
loss: 0.1309, acc: 0.9576
> val_acc: 0.8197, val_f1: 0.7839
>> saved: /media/b115/Backup/NLP/roberta/lap14/acc_0.8197_f1_0.7839_230828-0411.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1729, acc: 0.9375
loss: 0.1166, acc: 0.9643
loss: 0.1007, acc: 0.9714
loss: 0.0872, acc: 0.9761
loss: 0.0772, acc: 0.9773
loss: 0.0721, acc: 0.9769
loss: 0.0794, acc: 0.9756
loss: 0.0799, acc: 0.9747
loss: 0.0885, acc: 0.9740
loss: 0.0935, acc: 0.9707
loss: 0.0968, acc: 0.9663
loss: 0.0985, acc: 0.9660
loss: 0.1037, acc: 0.9637
loss: 0.1053, acc: 0.9636
loss: 0.1027, acc: 0.9648
> val_acc: 0.8025, val_f1: 0.7660
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0645, acc: 0.9844
loss: 0.0530, acc: 0.9861
loss: 0.0479, acc: 0.9844
loss: 0.0399, acc: 0.9885
loss: 0.0416, acc: 0.9896
loss: 0.0599, acc: 0.9806
loss: 0.0604, acc: 0.9779
loss: 0.0659, acc: 0.9784
loss: 0.0675, acc: 0.9773
loss: 0.0647, acc: 0.9783
loss: 0.0632, acc: 0.9792
loss: 0.0661, acc: 0.9778
loss: 0.0667, acc: 0.9771
loss: 0.0775, acc: 0.9733
> val_acc: 0.8135, val_f1: 0.7820
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1119, acc: 1.0000
loss: 0.0890, acc: 0.9635
loss: 0.0824, acc: 0.9744
loss: 0.0710, acc: 0.9785
loss: 0.0620, acc: 0.9807
loss: 0.0635, acc: 0.9784
loss: 0.0646, acc: 0.9768
loss: 0.0601, acc: 0.9783
loss: 0.0585, acc: 0.9771
loss: 0.0626, acc: 0.9776
loss: 0.0630, acc: 0.9779
loss: 0.0659, acc: 0.9782
loss: 0.0678, acc: 0.9785
loss: 0.0688, acc: 0.9782
loss: 0.0741, acc: 0.9767
> val_acc: 0.8103, val_f1: 0.7792
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0770, acc: 0.9792
loss: 0.0448, acc: 0.9844
loss: 0.0511, acc: 0.9832
loss: 0.0635, acc: 0.9826
loss: 0.0550, acc: 0.9851
loss: 0.0541, acc: 0.9855
loss: 0.0511, acc: 0.9858
loss: 0.0561, acc: 0.9836
loss: 0.0606, acc: 0.9826
loss: 0.0609, acc: 0.9818
loss: 0.0641, acc: 0.9800
loss: 0.0685, acc: 0.9790
loss: 0.0673, acc: 0.9797
loss: 0.0682, acc: 0.9784
loss: 0.0666, acc: 0.9794
> val_acc: 0.8041, val_f1: 0.7644
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0191, acc: 0.9938
loss: 0.0196, acc: 0.9938
loss: 0.0271, acc: 0.9896
loss: 0.0237, acc: 0.9906
loss: 0.0224, acc: 0.9912
loss: 0.0405, acc: 0.9865
loss: 0.0530, acc: 0.9830
loss: 0.0529, acc: 0.9828
loss: 0.0578, acc: 0.9819
loss: 0.0566, acc: 0.9825
loss: 0.0582, acc: 0.9818
loss: 0.0602, acc: 0.9812
loss: 0.0606, acc: 0.9812
loss: 0.0612, acc: 0.9812
> val_acc: 0.7994, val_f1: 0.7597
>> early stop.
>> test_acc: 0.8197, test_f1: 0.7839
>> test_acc: 0.8150, test_f1: 0.7804
>> test_acc: 0.8182, test_f1: 0.7850
>> test_acc: 0.8197, test_f1: 0.7839

>> avg_test_acc: 0.8177, avg_test_f1: 0.7831
>> max_test_acc: 0.8197, max_test_f1: 0.7850
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 441448448
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1413, acc: 0.4062
loss: 1.0899, acc: 0.4125
loss: 1.0549, acc: 0.4604
loss: 1.0403, acc: 0.4766
loss: 1.0196, acc: 0.5025
loss: 0.9630, acc: 0.5437
loss: 0.9420, acc: 0.5661
loss: 0.9180, acc: 0.5820
loss: 0.8923, acc: 0.5986
loss: 0.8699, acc: 0.6100
loss: 0.8549, acc: 0.6176
loss: 0.8427, acc: 0.6266
loss: 0.8243, acc: 0.6375
loss: 0.8169, acc: 0.6433
> val_acc: 0.7351, val_f1: 0.6583
>> saved: peft/bert_lora/lap14//acc_0.7351_f1_0.6583_230828-0413
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.7365, acc: 0.7500
loss: 0.6173, acc: 0.7589
loss: 0.6209, acc: 0.7422
loss: 0.5964, acc: 0.7592
loss: 0.5883, acc: 0.7642
loss: 0.5712, acc: 0.7778
loss: 0.5870, acc: 0.7754
loss: 0.5903, acc: 0.7770
loss: 0.5952, acc: 0.7708
loss: 0.5934, acc: 0.7719
loss: 0.5891, acc: 0.7734
loss: 0.5738, acc: 0.7802
loss: 0.5794, acc: 0.7777
loss: 0.5899, acc: 0.7719
loss: 0.5840, acc: 0.7739
> val_acc: 0.7649, val_f1: 0.7050
>> saved: peft/bert_lora/lap14//acc_0.7649_f1_0.705_230828-0414
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4601, acc: 0.8125
loss: 0.5273, acc: 0.7986
loss: 0.5004, acc: 0.8036
loss: 0.5011, acc: 0.8010
loss: 0.4820, acc: 0.8086
loss: 0.4639, acc: 0.8157
loss: 0.4644, acc: 0.8162
loss: 0.4626, acc: 0.8149
loss: 0.4658, acc: 0.8125
loss: 0.4735, acc: 0.8093
loss: 0.4760, acc: 0.8096
loss: 0.4764, acc: 0.8083
loss: 0.4775, acc: 0.8096
loss: 0.4747, acc: 0.8102
> val_acc: 0.7774, val_f1: 0.7178
>> saved: peft/bert_lora/lap14//acc_0.7774_f1_0.7178_230828-0414
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1648, acc: 0.9688
loss: 0.3281, acc: 0.8698
loss: 0.3265, acc: 0.8807
loss: 0.3556, acc: 0.8691
loss: 0.3657, acc: 0.8646
loss: 0.3498, acc: 0.8678
loss: 0.3466, acc: 0.8690
loss: 0.3497, acc: 0.8698
loss: 0.3548, acc: 0.8643
loss: 0.3506, acc: 0.8682
loss: 0.3581, acc: 0.8646
loss: 0.3601, acc: 0.8610
loss: 0.3670, acc: 0.8586
loss: 0.3707, acc: 0.8584
loss: 0.3687, acc: 0.8587
> val_acc: 0.7727, val_f1: 0.7220
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1958, acc: 0.9583
loss: 0.2239, acc: 0.9297
loss: 0.2468, acc: 0.9159
loss: 0.3021, acc: 0.8889
loss: 0.3192, acc: 0.8750
loss: 0.3180, acc: 0.8761
loss: 0.3250, acc: 0.8759
loss: 0.3261, acc: 0.8775
loss: 0.3213, acc: 0.8779
loss: 0.3315, acc: 0.8737
loss: 0.3372, acc: 0.8697
loss: 0.3345, acc: 0.8702
loss: 0.3355, acc: 0.8715
loss: 0.3308, acc: 0.8732
loss: 0.3308, acc: 0.8724
> val_acc: 0.7696, val_f1: 0.7264
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2503, acc: 0.9313
loss: 0.1916, acc: 0.9406
loss: 0.1927, acc: 0.9333
loss: 0.1982, acc: 0.9234
loss: 0.1968, acc: 0.9250
loss: 0.1965, acc: 0.9219
loss: 0.2079, acc: 0.9143
loss: 0.2198, acc: 0.9125
loss: 0.2326, acc: 0.9090
loss: 0.2379, acc: 0.9069
loss: 0.2372, acc: 0.9097
loss: 0.2385, acc: 0.9094
loss: 0.2381, acc: 0.9096
loss: 0.2449, acc: 0.9071
> val_acc: 0.7884, val_f1: 0.7498
>> saved: peft/bert_lora/lap14//acc_0.7884_f1_0.7498_230828-0415
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1798, acc: 0.9219
loss: 0.1811, acc: 0.9196
loss: 0.2032, acc: 0.9089
loss: 0.1984, acc: 0.9136
loss: 0.2074, acc: 0.9105
loss: 0.2071, acc: 0.9086
loss: 0.2144, acc: 0.9082
loss: 0.2260, acc: 0.9046
loss: 0.2319, acc: 0.9033
loss: 0.2292, acc: 0.9076
loss: 0.2400, acc: 0.9062
loss: 0.2362, acc: 0.9073
loss: 0.2352, acc: 0.9078
loss: 0.2317, acc: 0.9109
loss: 0.2280, acc: 0.9132
> val_acc: 0.7680, val_f1: 0.7181
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1102, acc: 0.9609
loss: 0.1189, acc: 0.9583
loss: 0.1298, acc: 0.9509
loss: 0.1933, acc: 0.9293
loss: 0.2095, acc: 0.9193
loss: 0.2019, acc: 0.9235
loss: 0.1893, acc: 0.9292
loss: 0.1839, acc: 0.9311
loss: 0.1853, acc: 0.9318
loss: 0.1937, acc: 0.9292
loss: 0.2004, acc: 0.9271
loss: 0.2020, acc: 0.9274
loss: 0.2008, acc: 0.9263
loss: 0.1978, acc: 0.9266
> val_acc: 0.7602, val_f1: 0.6982
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0957, acc: 0.9688
loss: 0.2512, acc: 0.9115
loss: 0.2453, acc: 0.9091
loss: 0.2615, acc: 0.9023
loss: 0.2608, acc: 0.9048
loss: 0.2554, acc: 0.9087
loss: 0.2493, acc: 0.9093
loss: 0.2441, acc: 0.9089
loss: 0.2259, acc: 0.9162
loss: 0.2192, acc: 0.9164
loss: 0.2105, acc: 0.9179
loss: 0.2042, acc: 0.9202
loss: 0.2123, acc: 0.9180
loss: 0.2068, acc: 0.9219
loss: 0.2088, acc: 0.9225
> val_acc: 0.7790, val_f1: 0.7462
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1331, acc: 0.9479
loss: 0.1193, acc: 0.9531
loss: 0.1319, acc: 0.9495
loss: 0.1366, acc: 0.9462
loss: 0.1399, acc: 0.9429
loss: 0.1338, acc: 0.9453
loss: 0.1384, acc: 0.9460
loss: 0.1522, acc: 0.9424
loss: 0.1475, acc: 0.9448
loss: 0.1488, acc: 0.9427
loss: 0.1497, acc: 0.9422
loss: 0.1512, acc: 0.9413
loss: 0.1562, acc: 0.9405
loss: 0.1574, acc: 0.9384
loss: 0.1577, acc: 0.9399
> val_acc: 0.7696, val_f1: 0.7242
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2152, acc: 0.9437
loss: 0.1560, acc: 0.9594
loss: 0.1397, acc: 0.9583
loss: 0.1289, acc: 0.9609
loss: 0.1348, acc: 0.9575
loss: 0.1450, acc: 0.9521
loss: 0.1448, acc: 0.9527
loss: 0.1451, acc: 0.9516
loss: 0.1462, acc: 0.9486
loss: 0.1496, acc: 0.9481
loss: 0.1449, acc: 0.9494
loss: 0.1522, acc: 0.9453
loss: 0.1510, acc: 0.9442
loss: 0.1510, acc: 0.9433
> val_acc: 0.7539, val_f1: 0.6930
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.7884, test_f1: 0.7498
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 484357632
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1768, acc: 0.3187
loss: 1.1174, acc: 0.3969
loss: 1.1157, acc: 0.3833
loss: 1.1108, acc: 0.3891
loss: 1.0952, acc: 0.3987
loss: 1.0955, acc: 0.4010
loss: 1.0891, acc: 0.4107
loss: 1.0669, acc: 0.4367
loss: 1.0421, acc: 0.4611
loss: 1.0218, acc: 0.4850
loss: 1.0057, acc: 0.4994
loss: 0.9814, acc: 0.5167
loss: 0.9702, acc: 0.5269
loss: 0.9557, acc: 0.5366
> val_acc: 0.7524, val_f1: 0.7059
>> saved: peft/bert_lora/lap14//acc_0.7524_f1_0.7059_230828-0417
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.8021, acc: 0.6094
loss: 0.6799, acc: 0.7098
loss: 0.6651, acc: 0.7240
loss: 0.6348, acc: 0.7371
loss: 0.6190, acc: 0.7401
loss: 0.6291, acc: 0.7407
loss: 0.6137, acc: 0.7490
loss: 0.6161, acc: 0.7508
loss: 0.6167, acc: 0.7463
loss: 0.6106, acc: 0.7540
loss: 0.6094, acc: 0.7554
loss: 0.6087, acc: 0.7566
loss: 0.6012, acc: 0.7601
loss: 0.5968, acc: 0.7612
loss: 0.5926, acc: 0.7622
> val_acc: 0.7618, val_f1: 0.7028
>> saved: peft/bert_lora/lap14//acc_0.7618_f1_0.7028_230828-0418
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4177, acc: 0.8672
loss: 0.4969, acc: 0.8403
loss: 0.4698, acc: 0.8415
loss: 0.4905, acc: 0.8273
loss: 0.4820, acc: 0.8320
loss: 0.4709, acc: 0.8319
loss: 0.4679, acc: 0.8290
loss: 0.4697, acc: 0.8261
loss: 0.4790, acc: 0.8189
loss: 0.4768, acc: 0.8151
loss: 0.4774, acc: 0.8160
loss: 0.4749, acc: 0.8183
loss: 0.4781, acc: 0.8169
loss: 0.4734, acc: 0.8184
> val_acc: 0.7524, val_f1: 0.6924
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3999, acc: 0.8438
loss: 0.3556, acc: 0.8594
loss: 0.3584, acc: 0.8580
loss: 0.3712, acc: 0.8496
loss: 0.3563, acc: 0.8512
loss: 0.3817, acc: 0.8401
loss: 0.3854, acc: 0.8417
loss: 0.3895, acc: 0.8411
loss: 0.3833, acc: 0.8468
loss: 0.3739, acc: 0.8505
loss: 0.3811, acc: 0.8499
loss: 0.3811, acc: 0.8493
loss: 0.3821, acc: 0.8478
loss: 0.3805, acc: 0.8499
loss: 0.3793, acc: 0.8512
> val_acc: 0.7743, val_f1: 0.7183
>> saved: peft/bert_lora/lap14//acc_0.7743_f1_0.7183_230828-0418
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2910, acc: 0.8958
loss: 0.3206, acc: 0.8867
loss: 0.3110, acc: 0.8750
loss: 0.2999, acc: 0.8767
loss: 0.3129, acc: 0.8736
loss: 0.3166, acc: 0.8694
loss: 0.3178, acc: 0.8684
loss: 0.3130, acc: 0.8725
loss: 0.3121, acc: 0.8735
loss: 0.3200, acc: 0.8685
loss: 0.3191, acc: 0.8697
loss: 0.3191, acc: 0.8718
loss: 0.3192, acc: 0.8715
loss: 0.3173, acc: 0.8722
loss: 0.3147, acc: 0.8737
> val_acc: 0.7618, val_f1: 0.7089
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2189, acc: 0.9313
loss: 0.2750, acc: 0.9125
loss: 0.2502, acc: 0.9187
loss: 0.2565, acc: 0.9156
loss: 0.2497, acc: 0.9187
loss: 0.2417, acc: 0.9156
loss: 0.2388, acc: 0.9116
loss: 0.2459, acc: 0.9109
loss: 0.2533, acc: 0.9083
loss: 0.2575, acc: 0.9044
loss: 0.2570, acc: 0.9034
loss: 0.2554, acc: 0.9031
loss: 0.2690, acc: 0.8986
loss: 0.2693, acc: 0.8991
> val_acc: 0.7492, val_f1: 0.7020
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2758, acc: 0.9062
loss: 0.2514, acc: 0.9196
loss: 0.2088, acc: 0.9349
loss: 0.2220, acc: 0.9320
loss: 0.2010, acc: 0.9361
loss: 0.2034, acc: 0.9329
loss: 0.2165, acc: 0.9248
loss: 0.2155, acc: 0.9240
loss: 0.2288, acc: 0.9204
loss: 0.2244, acc: 0.9222
loss: 0.2237, acc: 0.9219
loss: 0.2180, acc: 0.9227
loss: 0.2238, acc: 0.9209
loss: 0.2296, acc: 0.9179
loss: 0.2294, acc: 0.9175
> val_acc: 0.7696, val_f1: 0.7129
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2407, acc: 0.9219
loss: 0.1801, acc: 0.9375
loss: 0.1708, acc: 0.9330
loss: 0.1961, acc: 0.9243
loss: 0.1957, acc: 0.9258
loss: 0.2029, acc: 0.9224
loss: 0.2054, acc: 0.9228
loss: 0.2046, acc: 0.9223
loss: 0.2059, acc: 0.9233
loss: 0.2094, acc: 0.9203
loss: 0.2094, acc: 0.9201
loss: 0.2046, acc: 0.9221
loss: 0.2100, acc: 0.9214
loss: 0.2164, acc: 0.9203
> val_acc: 0.7476, val_f1: 0.7051
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2063, acc: 0.9062
loss: 0.1804, acc: 0.9323
loss: 0.1613, acc: 0.9460
loss: 0.1535, acc: 0.9492
loss: 0.1501, acc: 0.9464
loss: 0.1413, acc: 0.9483
loss: 0.1570, acc: 0.9466
loss: 0.1535, acc: 0.9497
loss: 0.1512, acc: 0.9497
loss: 0.1525, acc: 0.9484
loss: 0.1583, acc: 0.9461
loss: 0.1702, acc: 0.9431
loss: 0.1730, acc: 0.9411
loss: 0.1739, acc: 0.9413
loss: 0.1704, acc: 0.9419
> val_acc: 0.7586, val_f1: 0.7126
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.7743, test_f1: 0.7183
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 467580416
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0984, acc: 0.3625
loss: 1.0742, acc: 0.4000
loss: 1.0989, acc: 0.4062
loss: 1.1018, acc: 0.3844
loss: 1.0984, acc: 0.3775
loss: 1.0919, acc: 0.3823
loss: 1.0806, acc: 0.4009
loss: 1.0618, acc: 0.4273
loss: 1.0366, acc: 0.4500
loss: 1.0165, acc: 0.4694
loss: 1.0023, acc: 0.4835
loss: 0.9777, acc: 0.5005
loss: 0.9642, acc: 0.5144
loss: 0.9453, acc: 0.5299
> val_acc: 0.6944, val_f1: 0.5777
>> saved: peft/bert_lora/lap14//acc_0.6944_f1_0.5777_230828-0420
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.7024, acc: 0.7031
loss: 0.7276, acc: 0.7054
loss: 0.6697, acc: 0.7266
loss: 0.6330, acc: 0.7445
loss: 0.6653, acc: 0.7358
loss: 0.6661, acc: 0.7338
loss: 0.6469, acc: 0.7432
loss: 0.6318, acc: 0.7475
loss: 0.6296, acc: 0.7522
loss: 0.6372, acc: 0.7447
loss: 0.6359, acc: 0.7452
loss: 0.6218, acc: 0.7538
loss: 0.6253, acc: 0.7515
loss: 0.6289, acc: 0.7463
loss: 0.6231, acc: 0.7478
> val_acc: 0.7680, val_f1: 0.7208
>> saved: peft/bert_lora/lap14//acc_0.768_f1_0.7208_230828-0421
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5255, acc: 0.8047
loss: 0.5006, acc: 0.8229
loss: 0.4575, acc: 0.8326
loss: 0.4549, acc: 0.8240
loss: 0.4860, acc: 0.8021
loss: 0.4804, acc: 0.8039
loss: 0.4792, acc: 0.8079
loss: 0.4827, acc: 0.8085
loss: 0.4862, acc: 0.8075
loss: 0.4843, acc: 0.8074
loss: 0.4779, acc: 0.8096
loss: 0.4834, acc: 0.8072
loss: 0.4844, acc: 0.8066
loss: 0.4904, acc: 0.8039
> val_acc: 0.7492, val_f1: 0.6859
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2825, acc: 0.9062
loss: 0.3356, acc: 0.8646
loss: 0.3857, acc: 0.8523
loss: 0.3585, acc: 0.8652
loss: 0.3639, acc: 0.8661
loss: 0.3722, acc: 0.8618
loss: 0.3696, acc: 0.8649
loss: 0.3820, acc: 0.8611
loss: 0.3941, acc: 0.8514
loss: 0.3985, acc: 0.8505
loss: 0.4005, acc: 0.8474
loss: 0.4111, acc: 0.8432
loss: 0.4205, acc: 0.8417
loss: 0.4169, acc: 0.8438
loss: 0.4121, acc: 0.8429
> val_acc: 0.7555, val_f1: 0.6945
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2262, acc: 0.9062
loss: 0.2134, acc: 0.9141
loss: 0.2605, acc: 0.8990
loss: 0.2948, acc: 0.8837
loss: 0.3037, acc: 0.8777
loss: 0.3103, acc: 0.8750
loss: 0.3220, acc: 0.8693
loss: 0.3254, acc: 0.8701
loss: 0.3314, acc: 0.8685
loss: 0.3244, acc: 0.8724
loss: 0.3150, acc: 0.8768
loss: 0.3212, acc: 0.8739
loss: 0.3221, acc: 0.8730
loss: 0.3210, acc: 0.8732
loss: 0.3258, acc: 0.8707
> val_acc: 0.7696, val_f1: 0.7136
>> saved: peft/bert_lora/lap14//acc_0.7696_f1_0.7136_230828-0422
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2979, acc: 0.8750
loss: 0.2608, acc: 0.9031
loss: 0.2536, acc: 0.9042
loss: 0.2683, acc: 0.8969
loss: 0.2585, acc: 0.9050
loss: 0.2702, acc: 0.9021
loss: 0.2803, acc: 0.8973
loss: 0.2809, acc: 0.8969
loss: 0.2896, acc: 0.8917
loss: 0.2903, acc: 0.8931
loss: 0.2901, acc: 0.8932
loss: 0.2926, acc: 0.8901
loss: 0.2892, acc: 0.8923
loss: 0.2859, acc: 0.8942
> val_acc: 0.7618, val_f1: 0.7290
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2063, acc: 0.9375
loss: 0.2051, acc: 0.9018
loss: 0.2285, acc: 0.9089
loss: 0.2656, acc: 0.8952
loss: 0.2609, acc: 0.9048
loss: 0.2495, acc: 0.9086
loss: 0.2458, acc: 0.9121
loss: 0.2509, acc: 0.9105
loss: 0.2441, acc: 0.9115
loss: 0.2396, acc: 0.9136
loss: 0.2444, acc: 0.9123
loss: 0.2454, acc: 0.9134
loss: 0.2474, acc: 0.9118
loss: 0.2482, acc: 0.9104
loss: 0.2474, acc: 0.9106
> val_acc: 0.7508, val_f1: 0.6989
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1917, acc: 0.9297
loss: 0.1625, acc: 0.9375
loss: 0.1772, acc: 0.9286
loss: 0.1915, acc: 0.9243
loss: 0.2179, acc: 0.9167
loss: 0.2311, acc: 0.9127
loss: 0.2295, acc: 0.9108
loss: 0.2279, acc: 0.9103
loss: 0.2189, acc: 0.9141
loss: 0.2138, acc: 0.9165
loss: 0.2166, acc: 0.9167
loss: 0.2182, acc: 0.9174
loss: 0.2126, acc: 0.9189
loss: 0.2148, acc: 0.9171
> val_acc: 0.7367, val_f1: 0.6734
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1838, acc: 0.9062
loss: 0.1433, acc: 0.9427
loss: 0.1594, acc: 0.9489
loss: 0.1655, acc: 0.9434
loss: 0.1679, acc: 0.9405
loss: 0.1617, acc: 0.9411
loss: 0.1713, acc: 0.9355
loss: 0.1868, acc: 0.9314
loss: 0.1916, acc: 0.9291
loss: 0.1867, acc: 0.9321
loss: 0.1764, acc: 0.9357
loss: 0.1790, acc: 0.9336
loss: 0.1798, acc: 0.9329
loss: 0.1866, acc: 0.9313
loss: 0.1840, acc: 0.9309
> val_acc: 0.7618, val_f1: 0.7199
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1842, acc: 0.9167
loss: 0.1591, acc: 0.9492
loss: 0.1466, acc: 0.9519
loss: 0.1847, acc: 0.9392
loss: 0.1865, acc: 0.9334
loss: 0.1901, acc: 0.9330
loss: 0.1828, acc: 0.9356
loss: 0.1910, acc: 0.9342
loss: 0.1922, acc: 0.9302
loss: 0.1858, acc: 0.9323
loss: 0.1884, acc: 0.9304
loss: 0.1873, acc: 0.9321
loss: 0.1826, acc: 0.9345
loss: 0.1845, acc: 0.9338
loss: 0.1891, acc: 0.9321
> val_acc: 0.7586, val_f1: 0.7015
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.7696, test_f1: 0.7136
>> test_acc: 0.7884, test_f1: 0.7498
>> test_acc: 0.7743, test_f1: 0.7183
>> test_acc: 0.7696, test_f1: 0.7136

>> avg_test_acc: 0.7774, avg_test_f1: 0.7273
>> max_test_acc: 0.7884, max_test_f1: 0.7498
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 504626176
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1172, acc: 0.3625
loss: 1.0991, acc: 0.3937
loss: 1.0658, acc: 0.4396
loss: 1.0232, acc: 0.4859
loss: 0.9754, acc: 0.5275
loss: 0.9376, acc: 0.5615
loss: 0.9058, acc: 0.5821
loss: 0.8749, acc: 0.6000
loss: 0.8515, acc: 0.6201
loss: 0.8193, acc: 0.6381
loss: 0.7891, acc: 0.6557
loss: 0.7793, acc: 0.6599
loss: 0.7555, acc: 0.6726
loss: 0.7509, acc: 0.6768
> val_acc: 0.7680, val_f1: 0.7069
>> saved: peft/roberta_lora/lap14//acc_0.768_f1_0.7069_230828-0424
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5111, acc: 0.8281
loss: 0.5385, acc: 0.7946
loss: 0.5310, acc: 0.7760
loss: 0.5246, acc: 0.7886
loss: 0.5296, acc: 0.7812
loss: 0.5241, acc: 0.7836
loss: 0.5382, acc: 0.7754
loss: 0.5371, acc: 0.7753
loss: 0.5287, acc: 0.7812
loss: 0.5240, acc: 0.7832
loss: 0.5198, acc: 0.7873
loss: 0.5076, acc: 0.7928
loss: 0.5008, acc: 0.7979
loss: 0.5001, acc: 0.7980
loss: 0.4985, acc: 0.8003
> val_acc: 0.7853, val_f1: 0.7417
>> saved: peft/roberta_lora/lap14//acc_0.7853_f1_0.7417_230828-0424
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4808, acc: 0.8281
loss: 0.4892, acc: 0.8090
loss: 0.4432, acc: 0.8259
loss: 0.4302, acc: 0.8339
loss: 0.4241, acc: 0.8346
loss: 0.4300, acc: 0.8362
loss: 0.4408, acc: 0.8327
loss: 0.4434, acc: 0.8285
loss: 0.4480, acc: 0.8303
loss: 0.4370, acc: 0.8329
loss: 0.4474, acc: 0.8299
loss: 0.4638, acc: 0.8210
loss: 0.4643, acc: 0.8193
loss: 0.4694, acc: 0.8175
> val_acc: 0.7868, val_f1: 0.7276
>> saved: peft/roberta_lora/lap14//acc_0.7868_f1_0.7276_230828-0424
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2012, acc: 0.9688
loss: 0.2572, acc: 0.9219
loss: 0.3702, acc: 0.8750
loss: 0.3487, acc: 0.8789
loss: 0.3224, acc: 0.8824
loss: 0.3112, acc: 0.8846
loss: 0.3237, acc: 0.8841
loss: 0.3377, acc: 0.8776
loss: 0.3344, acc: 0.8780
loss: 0.3299, acc: 0.8798
loss: 0.3348, acc: 0.8762
loss: 0.3409, acc: 0.8717
loss: 0.3537, acc: 0.8642
loss: 0.3544, acc: 0.8622
loss: 0.3517, acc: 0.8640
> val_acc: 0.8103, val_f1: 0.7717
>> saved: peft/roberta_lora/lap14//acc_0.8103_f1_0.7717_230828-0425
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2257, acc: 0.9167
loss: 0.2623, acc: 0.8984
loss: 0.2742, acc: 0.8870
loss: 0.2999, acc: 0.8785
loss: 0.3093, acc: 0.8764
loss: 0.3170, acc: 0.8739
loss: 0.3204, acc: 0.8731
loss: 0.3143, acc: 0.8766
loss: 0.3210, acc: 0.8750
loss: 0.3216, acc: 0.8724
loss: 0.3286, acc: 0.8726
loss: 0.3453, acc: 0.8675
loss: 0.3543, acc: 0.8656
loss: 0.3575, acc: 0.8617
loss: 0.3525, acc: 0.8647
> val_acc: 0.7994, val_f1: 0.7573
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2060, acc: 0.9437
loss: 0.2634, acc: 0.9094
loss: 0.2639, acc: 0.9042
loss: 0.2654, acc: 0.9000
loss: 0.2590, acc: 0.9038
loss: 0.2870, acc: 0.8990
loss: 0.3176, acc: 0.8830
loss: 0.3280, acc: 0.8805
loss: 0.3295, acc: 0.8785
loss: 0.3290, acc: 0.8788
loss: 0.3266, acc: 0.8790
loss: 0.3199, acc: 0.8823
loss: 0.3183, acc: 0.8827
loss: 0.3149, acc: 0.8835
> val_acc: 0.8041, val_f1: 0.7595
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2907, acc: 0.9219
loss: 0.2377, acc: 0.9196
loss: 0.2211, acc: 0.9219
loss: 0.2519, acc: 0.9099
loss: 0.2592, acc: 0.9048
loss: 0.2495, acc: 0.9086
loss: 0.2468, acc: 0.9082
loss: 0.2528, acc: 0.9071
loss: 0.2507, acc: 0.9070
loss: 0.2479, acc: 0.9043
loss: 0.2422, acc: 0.9056
loss: 0.2489, acc: 0.9046
loss: 0.2508, acc: 0.9027
loss: 0.2577, acc: 0.9002
loss: 0.2562, acc: 0.9023
> val_acc: 0.8103, val_f1: 0.7742
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1510, acc: 0.9453
loss: 0.1342, acc: 0.9444
loss: 0.1535, acc: 0.9375
loss: 0.1672, acc: 0.9342
loss: 0.1779, acc: 0.9323
loss: 0.1820, acc: 0.9310
loss: 0.2022, acc: 0.9274
loss: 0.2055, acc: 0.9271
loss: 0.2064, acc: 0.9268
loss: 0.2133, acc: 0.9222
loss: 0.2124, acc: 0.9230
loss: 0.2126, acc: 0.9221
loss: 0.2122, acc: 0.9204
loss: 0.2107, acc: 0.9198
> val_acc: 0.8182, val_f1: 0.7865
>> saved: peft/roberta_lora/lap14//acc_0.8182_f1_0.7865_230828-0426
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1780, acc: 0.9062
loss: 0.2438, acc: 0.9010
loss: 0.2342, acc: 0.9062
loss: 0.2084, acc: 0.9141
loss: 0.2110, acc: 0.9152
loss: 0.2041, acc: 0.9207
loss: 0.2054, acc: 0.9173
loss: 0.2027, acc: 0.9201
loss: 0.2200, acc: 0.9184
loss: 0.2246, acc: 0.9164
loss: 0.2275, acc: 0.9142
loss: 0.2229, acc: 0.9163
loss: 0.2246, acc: 0.9170
loss: 0.2240, acc: 0.9186
loss: 0.2259, acc: 0.9173
> val_acc: 0.8025, val_f1: 0.7581
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1801, acc: 0.9167
loss: 0.1534, acc: 0.9375
loss: 0.1611, acc: 0.9351
loss: 0.1477, acc: 0.9410
loss: 0.1541, acc: 0.9389
loss: 0.1928, acc: 0.9286
loss: 0.1931, acc: 0.9309
loss: 0.1952, acc: 0.9309
loss: 0.1920, acc: 0.9310
loss: 0.1913, acc: 0.9310
loss: 0.1849, acc: 0.9340
loss: 0.1886, acc: 0.9321
loss: 0.1822, acc: 0.9350
loss: 0.1914, acc: 0.9315
loss: 0.1872, acc: 0.9326
> val_acc: 0.7900, val_f1: 0.7448
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2036, acc: 0.9125
loss: 0.1728, acc: 0.9281
loss: 0.1595, acc: 0.9354
loss: 0.1632, acc: 0.9422
loss: 0.1511, acc: 0.9450
loss: 0.1434, acc: 0.9490
loss: 0.1395, acc: 0.9509
loss: 0.1403, acc: 0.9500
loss: 0.1433, acc: 0.9479
loss: 0.1731, acc: 0.9381
loss: 0.1834, acc: 0.9335
loss: 0.1811, acc: 0.9344
loss: 0.1835, acc: 0.9337
loss: 0.1867, acc: 0.9335
> val_acc: 0.8041, val_f1: 0.7609
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.2363, acc: 0.9219
loss: 0.2015, acc: 0.9330
loss: 0.1868, acc: 0.9375
loss: 0.1988, acc: 0.9338
loss: 0.1757, acc: 0.9389
loss: 0.1990, acc: 0.9317
loss: 0.1995, acc: 0.9326
loss: 0.1932, acc: 0.9333
loss: 0.1952, acc: 0.9323
loss: 0.1959, acc: 0.9315
loss: 0.1898, acc: 0.9327
loss: 0.1842, acc: 0.9353
loss: 0.1834, acc: 0.9360
loss: 0.1796, acc: 0.9375
loss: 0.1765, acc: 0.9379
> val_acc: 0.8166, val_f1: 0.7812
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1410, acc: 0.9531
loss: 0.1354, acc: 0.9583
loss: 0.1464, acc: 0.9487
loss: 0.1453, acc: 0.9424
loss: 0.1314, acc: 0.9466
loss: 0.1352, acc: 0.9472
loss: 0.1543, acc: 0.9393
loss: 0.1736, acc: 0.9335
loss: 0.1835, acc: 0.9290
loss: 0.1791, acc: 0.9279
loss: 0.1883, acc: 0.9248
loss: 0.2033, acc: 0.9184
loss: 0.2099, acc: 0.9180
loss: 0.2095, acc: 0.9189
> val_acc: 0.7774, val_f1: 0.7146
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8182, test_f1: 0.7865
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 560526848
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1660, acc: 0.3375
loss: 1.1064, acc: 0.3719
loss: 1.0587, acc: 0.4292
loss: 1.0029, acc: 0.4875
loss: 0.9443, acc: 0.5413
loss: 0.9038, acc: 0.5687
loss: 0.8720, acc: 0.5938
loss: 0.8472, acc: 0.6156
loss: 0.8367, acc: 0.6194
loss: 0.8176, acc: 0.6331
loss: 0.7943, acc: 0.6443
loss: 0.7787, acc: 0.6542
loss: 0.7710, acc: 0.6601
loss: 0.7615, acc: 0.6656
> val_acc: 0.7727, val_f1: 0.7196
>> saved: peft/roberta_lora/lap14//acc_0.7727_f1_0.7196_230828-0428
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5423, acc: 0.7969
loss: 0.5575, acc: 0.7902
loss: 0.6175, acc: 0.7786
loss: 0.6201, acc: 0.7629
loss: 0.6023, acc: 0.7713
loss: 0.6017, acc: 0.7697
loss: 0.5897, acc: 0.7773
loss: 0.5854, acc: 0.7796
loss: 0.5933, acc: 0.7723
loss: 0.5880, acc: 0.7739
loss: 0.5748, acc: 0.7806
loss: 0.5667, acc: 0.7823
loss: 0.5613, acc: 0.7843
loss: 0.5532, acc: 0.7873
loss: 0.5538, acc: 0.7856
> val_acc: 0.7947, val_f1: 0.7502
>> saved: peft/roberta_lora/lap14//acc_0.7947_f1_0.7502_230828-0428
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5383, acc: 0.7656
loss: 0.4566, acc: 0.8090
loss: 0.5154, acc: 0.7879
loss: 0.5150, acc: 0.7944
loss: 0.5070, acc: 0.7969
loss: 0.4835, acc: 0.8114
loss: 0.4840, acc: 0.8097
loss: 0.4962, acc: 0.8021
loss: 0.4945, acc: 0.8054
loss: 0.4927, acc: 0.8087
loss: 0.4905, acc: 0.8079
loss: 0.4874, acc: 0.8077
loss: 0.4764, acc: 0.8140
loss: 0.4757, acc: 0.8148
> val_acc: 0.7947, val_f1: 0.7560
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2671, acc: 0.9375
loss: 0.3845, acc: 0.8490
loss: 0.4104, acc: 0.8466
loss: 0.3814, acc: 0.8516
loss: 0.3793, acc: 0.8542
loss: 0.3892, acc: 0.8570
loss: 0.3849, acc: 0.8569
loss: 0.3789, acc: 0.8576
loss: 0.3824, acc: 0.8537
loss: 0.3739, acc: 0.8594
loss: 0.3806, acc: 0.8560
loss: 0.3788, acc: 0.8566
loss: 0.3825, acc: 0.8540
loss: 0.3947, acc: 0.8475
loss: 0.3924, acc: 0.8490
> val_acc: 0.8088, val_f1: 0.7674
>> saved: peft/roberta_lora/lap14//acc_0.8088_f1_0.7674_230828-0429
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4050, acc: 0.8229
loss: 0.3545, acc: 0.8594
loss: 0.3486, acc: 0.8582
loss: 0.3491, acc: 0.8594
loss: 0.3486, acc: 0.8587
loss: 0.3492, acc: 0.8571
loss: 0.3519, acc: 0.8608
loss: 0.3446, acc: 0.8610
loss: 0.3588, acc: 0.8561
loss: 0.3529, acc: 0.8620
loss: 0.3468, acc: 0.8644
loss: 0.3469, acc: 0.8669
loss: 0.3485, acc: 0.8671
loss: 0.3525, acc: 0.8649
loss: 0.3469, acc: 0.8664
> val_acc: 0.8276, val_f1: 0.7997
>> saved: peft/roberta_lora/lap14//acc_0.8276_f1_0.7997_230828-0429
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2142, acc: 0.9313
loss: 0.2140, acc: 0.9250
loss: 0.2669, acc: 0.9062
loss: 0.3096, acc: 0.8859
loss: 0.2950, acc: 0.8938
loss: 0.2891, acc: 0.8917
loss: 0.2967, acc: 0.8875
loss: 0.3098, acc: 0.8812
loss: 0.3020, acc: 0.8847
loss: 0.3018, acc: 0.8812
loss: 0.3034, acc: 0.8818
loss: 0.3048, acc: 0.8797
loss: 0.3074, acc: 0.8793
loss: 0.3082, acc: 0.8790
> val_acc: 0.7931, val_f1: 0.7488
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2494, acc: 0.9219
loss: 0.2646, acc: 0.9018
loss: 0.2280, acc: 0.9089
loss: 0.2243, acc: 0.9099
loss: 0.2276, acc: 0.9134
loss: 0.2489, acc: 0.9086
loss: 0.2452, acc: 0.9082
loss: 0.2607, acc: 0.9037
loss: 0.2574, acc: 0.9025
loss: 0.2645, acc: 0.9009
loss: 0.2685, acc: 0.9002
loss: 0.2703, acc: 0.8997
loss: 0.2686, acc: 0.8997
loss: 0.2745, acc: 0.8974
loss: 0.2728, acc: 0.8963
> val_acc: 0.7962, val_f1: 0.7522
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2378, acc: 0.8906
loss: 0.2026, acc: 0.9132
loss: 0.2135, acc: 0.9062
loss: 0.2206, acc: 0.9079
loss: 0.2408, acc: 0.9036
loss: 0.2382, acc: 0.9041
loss: 0.2561, acc: 0.8998
loss: 0.2584, acc: 0.8990
loss: 0.2627, acc: 0.8963
loss: 0.2636, acc: 0.8986
loss: 0.2644, acc: 0.8999
loss: 0.2571, acc: 0.9025
loss: 0.2558, acc: 0.8999
loss: 0.2666, acc: 0.8954
> val_acc: 0.8088, val_f1: 0.7720
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1864, acc: 0.8750
loss: 0.3289, acc: 0.8750
loss: 0.3223, acc: 0.8864
loss: 0.3078, acc: 0.8809
loss: 0.2963, acc: 0.8884
loss: 0.2747, acc: 0.8942
loss: 0.2788, acc: 0.8931
loss: 0.2646, acc: 0.8967
loss: 0.2546, acc: 0.8986
loss: 0.2520, acc: 0.8967
loss: 0.2593, acc: 0.8946
loss: 0.2583, acc: 0.8956
loss: 0.2563, acc: 0.8975
loss: 0.2505, acc: 0.9006
loss: 0.2503, acc: 0.9005
> val_acc: 0.8229, val_f1: 0.7842
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2197, acc: 0.8958
loss: 0.1891, acc: 0.9297
loss: 0.1779, acc: 0.9399
loss: 0.2131, acc: 0.9253
loss: 0.2173, acc: 0.9253
loss: 0.2085, acc: 0.9275
loss: 0.2070, acc: 0.9261
loss: 0.2031, acc: 0.9276
loss: 0.2085, acc: 0.9251
loss: 0.2082, acc: 0.9258
loss: 0.2153, acc: 0.9263
loss: 0.2262, acc: 0.9219
loss: 0.2440, acc: 0.9132
loss: 0.2444, acc: 0.9131
loss: 0.2424, acc: 0.9154
> val_acc: 0.8072, val_f1: 0.7683
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8276, test_f1: 0.7997
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 543618560
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: lap14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1435, acc: 0.4625
loss: 1.1215, acc: 0.4188
loss: 1.0767, acc: 0.4458
loss: 1.0132, acc: 0.4984
loss: 0.9661, acc: 0.5400
loss: 0.9123, acc: 0.5729
loss: 0.8662, acc: 0.6018
loss: 0.8225, acc: 0.6266
loss: 0.7964, acc: 0.6417
loss: 0.7787, acc: 0.6519
loss: 0.7695, acc: 0.6591
loss: 0.7566, acc: 0.6677
loss: 0.7545, acc: 0.6716
loss: 0.7476, acc: 0.6763
> val_acc: 0.7539, val_f1: 0.6770
>> saved: peft/roberta_lora/lap14//acc_0.7539_f1_0.677_230828-0431
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5151, acc: 0.7969
loss: 0.5071, acc: 0.7991
loss: 0.5232, acc: 0.7812
loss: 0.5209, acc: 0.7831
loss: 0.5590, acc: 0.7798
loss: 0.5572, acc: 0.7824
loss: 0.5424, acc: 0.7861
loss: 0.5294, acc: 0.7948
loss: 0.5385, acc: 0.7879
loss: 0.5387, acc: 0.7886
loss: 0.5316, acc: 0.7903
loss: 0.5396, acc: 0.7867
loss: 0.5339, acc: 0.7898
loss: 0.5329, acc: 0.7892
loss: 0.5411, acc: 0.7873
> val_acc: 0.7665, val_f1: 0.7140
>> saved: peft/roberta_lora/lap14//acc_0.7665_f1_0.714_230828-0432
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4799, acc: 0.8047
loss: 0.4624, acc: 0.8229
loss: 0.4574, acc: 0.8192
loss: 0.4716, acc: 0.8158
loss: 0.4526, acc: 0.8268
loss: 0.4602, acc: 0.8276
loss: 0.4514, acc: 0.8290
loss: 0.4456, acc: 0.8301
loss: 0.4556, acc: 0.8217
loss: 0.4604, acc: 0.8202
loss: 0.4715, acc: 0.8160
loss: 0.4726, acc: 0.8167
loss: 0.4732, acc: 0.8130
loss: 0.4699, acc: 0.8134
> val_acc: 0.7821, val_f1: 0.7390
>> saved: peft/roberta_lora/lap14//acc_0.7821_f1_0.739_230828-0432
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1944, acc: 0.9688
loss: 0.3620, acc: 0.8594
loss: 0.3470, acc: 0.8693
loss: 0.3606, acc: 0.8652
loss: 0.3648, acc: 0.8616
loss: 0.3873, acc: 0.8558
loss: 0.4077, acc: 0.8438
loss: 0.4107, acc: 0.8438
loss: 0.4113, acc: 0.8445
loss: 0.4026, acc: 0.8478
loss: 0.3922, acc: 0.8536
loss: 0.3886, acc: 0.8521
loss: 0.3988, acc: 0.8458
loss: 0.4023, acc: 0.8428
loss: 0.4119, acc: 0.8371
> val_acc: 0.7853, val_f1: 0.7272
>> saved: peft/roberta_lora/lap14//acc_0.7853_f1_0.7272_230828-0432
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4123, acc: 0.8542
loss: 0.3706, acc: 0.8750
loss: 0.3749, acc: 0.8726
loss: 0.3581, acc: 0.8733
loss: 0.3800, acc: 0.8628
loss: 0.3746, acc: 0.8605
loss: 0.3864, acc: 0.8542
loss: 0.3892, acc: 0.8528
loss: 0.3956, acc: 0.8510
loss: 0.4091, acc: 0.8431
loss: 0.4018, acc: 0.8467
loss: 0.3935, acc: 0.8491
loss: 0.3940, acc: 0.8467
loss: 0.3955, acc: 0.8447
loss: 0.3829, acc: 0.8492
> val_acc: 0.8213, val_f1: 0.7920
>> saved: peft/roberta_lora/lap14//acc_0.8213_f1_0.792_230828-0433
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3857, acc: 0.8500
loss: 0.3353, acc: 0.8688
loss: 0.3347, acc: 0.8646
loss: 0.3632, acc: 0.8531
loss: 0.3378, acc: 0.8700
loss: 0.3235, acc: 0.8771
loss: 0.3031, acc: 0.8830
loss: 0.3120, acc: 0.8805
loss: 0.3127, acc: 0.8778
loss: 0.3045, acc: 0.8788
loss: 0.3157, acc: 0.8744
loss: 0.3185, acc: 0.8745
loss: 0.3218, acc: 0.8721
loss: 0.3227, acc: 0.8728
> val_acc: 0.8088, val_f1: 0.7709
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2264, acc: 0.9062
loss: 0.1846, acc: 0.9375
loss: 0.2478, acc: 0.9010
loss: 0.2443, acc: 0.9044
loss: 0.2592, acc: 0.8977
loss: 0.2623, acc: 0.8935
loss: 0.2574, acc: 0.8975
loss: 0.2707, acc: 0.8894
loss: 0.2967, acc: 0.8728
loss: 0.3224, acc: 0.8670
loss: 0.3304, acc: 0.8678
loss: 0.3402, acc: 0.8640
loss: 0.3377, acc: 0.8659
loss: 0.3359, acc: 0.8671
loss: 0.3392, acc: 0.8659
> val_acc: 0.8135, val_f1: 0.7752
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3350, acc: 0.8750
loss: 0.2754, acc: 0.8889
loss: 0.2595, acc: 0.8973
loss: 0.2591, acc: 0.8997
loss: 0.2521, acc: 0.9023
loss: 0.2504, acc: 0.9030
loss: 0.2446, acc: 0.9044
loss: 0.2531, acc: 0.9038
loss: 0.2528, acc: 0.9034
loss: 0.2688, acc: 0.8992
loss: 0.2645, acc: 0.9016
loss: 0.2626, acc: 0.9031
loss: 0.2647, acc: 0.9014
loss: 0.2672, acc: 0.9013
> val_acc: 0.8072, val_f1: 0.7781
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1589, acc: 0.9375
loss: 0.1897, acc: 0.9323
loss: 0.1801, acc: 0.9403
loss: 0.2011, acc: 0.9336
loss: 0.2338, acc: 0.9226
loss: 0.2459, acc: 0.9183
loss: 0.2555, acc: 0.9103
loss: 0.2490, acc: 0.9141
loss: 0.2397, acc: 0.9154
loss: 0.2436, acc: 0.9137
loss: 0.2414, acc: 0.9148
loss: 0.2435, acc: 0.9146
loss: 0.2490, acc: 0.9109
loss: 0.2422, acc: 0.9129
loss: 0.2433, acc: 0.9124
> val_acc: 0.8166, val_f1: 0.7882
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3111, acc: 0.8854
loss: 0.2340, acc: 0.9023
loss: 0.2193, acc: 0.9014
loss: 0.2065, acc: 0.9132
loss: 0.2361, acc: 0.9022
loss: 0.2360, acc: 0.9018
loss: 0.2423, acc: 0.8996
loss: 0.2324, acc: 0.9046
loss: 0.2276, acc: 0.9092
loss: 0.2157, acc: 0.9147
loss: 0.2088, acc: 0.9192
loss: 0.2071, acc: 0.9203
loss: 0.2051, acc: 0.9206
loss: 0.2037, acc: 0.9219
loss: 0.2060, acc: 0.9214
> val_acc: 0.8103, val_f1: 0.7667
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8213, test_f1: 0.7920
>> test_acc: 0.8182, test_f1: 0.7865
>> test_acc: 0.8276, test_f1: 0.7997
>> test_acc: 0.8213, test_f1: 0.7920

>> avg_test_acc: 0.8224, avg_test_f1: 0.7927
>> max_test_acc: 0.8276, max_test_f1: 0.7997
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 439079424
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1017, acc: 0.4125
loss: 1.0315, acc: 0.5000
loss: 1.0102, acc: 0.5125
loss: 0.9846, acc: 0.5375
loss: 0.9671, acc: 0.5537
loss: 0.9432, acc: 0.5646
loss: 0.9211, acc: 0.5705
loss: 0.8943, acc: 0.5773
loss: 0.8726, acc: 0.5944
loss: 0.8632, acc: 0.6050
loss: 0.8466, acc: 0.6165
loss: 0.8262, acc: 0.6245
loss: 0.8167, acc: 0.6293
loss: 0.8070, acc: 0.6362
loss: 0.7986, acc: 0.6412
loss: 0.7868, acc: 0.6500
loss: 0.7784, acc: 0.6570
loss: 0.7742, acc: 0.6587
loss: 0.7661, acc: 0.6655
loss: 0.7658, acc: 0.6672
loss: 0.7598, acc: 0.6711
loss: 0.7535, acc: 0.6730
> val_acc: 0.7973, val_f1: 0.6309
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.7973_f1_0.6309_230828-0435.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6127, acc: 0.7969
loss: 0.5150, acc: 0.8304
loss: 0.5428, acc: 0.8073
loss: 0.5496, acc: 0.7886
loss: 0.5456, acc: 0.7926
loss: 0.5387, acc: 0.7940
loss: 0.5357, acc: 0.7959
loss: 0.5143, acc: 0.8041
loss: 0.5109, acc: 0.8065
loss: 0.5033, acc: 0.8072
loss: 0.5044, acc: 0.8059
loss: 0.5049, acc: 0.8037
loss: 0.5015, acc: 0.8080
loss: 0.5012, acc: 0.8064
loss: 0.4982, acc: 0.8090
loss: 0.4934, acc: 0.8117
loss: 0.4937, acc: 0.8110
loss: 0.4951, acc: 0.8089
loss: 0.4948, acc: 0.8074
loss: 0.4925, acc: 0.8083
loss: 0.4899, acc: 0.8085
loss: 0.4889, acc: 0.8090
loss: 0.4842, acc: 0.8105
> val_acc: 0.8232, val_f1: 0.6984
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8232_f1_0.6984_230828-0436.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2923, acc: 0.8906
loss: 0.2598, acc: 0.9132
loss: 0.2846, acc: 0.8996
loss: 0.2974, acc: 0.8931
loss: 0.2820, acc: 0.8971
loss: 0.2850, acc: 0.8998
loss: 0.2896, acc: 0.8971
loss: 0.2837, acc: 0.8990
loss: 0.2854, acc: 0.8963
loss: 0.2903, acc: 0.8948
loss: 0.2912, acc: 0.8912
loss: 0.2923, acc: 0.8909
loss: 0.2961, acc: 0.8882
loss: 0.2920, acc: 0.8895
loss: 0.2918, acc: 0.8898
loss: 0.2908, acc: 0.8924
loss: 0.2967, acc: 0.8910
loss: 0.2995, acc: 0.8894
loss: 0.3012, acc: 0.8886
loss: 0.2991, acc: 0.8889
loss: 0.2982, acc: 0.8894
loss: 0.3008, acc: 0.8890
> val_acc: 0.8473, val_f1: 0.7688
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8473_f1_0.7688_230828-0437.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.0970, acc: 1.0000
loss: 0.1345, acc: 0.9688
loss: 0.1544, acc: 0.9517
loss: 0.1645, acc: 0.9473
loss: 0.1672, acc: 0.9449
loss: 0.1664, acc: 0.9447
loss: 0.1726, acc: 0.9405
loss: 0.1779, acc: 0.9375
loss: 0.1732, acc: 0.9405
loss: 0.1763, acc: 0.9402
loss: 0.1729, acc: 0.9424
loss: 0.1688, acc: 0.9431
loss: 0.1729, acc: 0.9411
loss: 0.1782, acc: 0.9384
loss: 0.1827, acc: 0.9357
loss: 0.1817, acc: 0.9350
loss: 0.1808, acc: 0.9363
loss: 0.1790, acc: 0.9368
loss: 0.1793, acc: 0.9368
loss: 0.1798, acc: 0.9368
loss: 0.1793, acc: 0.9369
loss: 0.1786, acc: 0.9378
loss: 0.1777, acc: 0.9378
> val_acc: 0.8393, val_f1: 0.7622
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0427, acc: 1.0000
loss: 0.0722, acc: 0.9844
loss: 0.0791, acc: 0.9784
loss: 0.0799, acc: 0.9740
loss: 0.0786, acc: 0.9755
loss: 0.0746, acc: 0.9766
loss: 0.0770, acc: 0.9754
loss: 0.0869, acc: 0.9720
loss: 0.0836, acc: 0.9731
loss: 0.0853, acc: 0.9720
loss: 0.0903, acc: 0.9711
loss: 0.0886, acc: 0.9714
loss: 0.0874, acc: 0.9712
loss: 0.0919, acc: 0.9692
loss: 0.0898, acc: 0.9700
loss: 0.0960, acc: 0.9671
loss: 0.0934, acc: 0.9676
loss: 0.0979, acc: 0.9663
loss: 0.0975, acc: 0.9661
loss: 0.0986, acc: 0.9659
loss: 0.1000, acc: 0.9651
loss: 0.1031, acc: 0.9644
loss: 0.1026, acc: 0.9642
> val_acc: 0.8438, val_f1: 0.7519
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0571, acc: 0.9875
loss: 0.0639, acc: 0.9812
loss: 0.0648, acc: 0.9750
loss: 0.0549, acc: 0.9797
loss: 0.0487, acc: 0.9825
loss: 0.0529, acc: 0.9802
loss: 0.0642, acc: 0.9768
loss: 0.0711, acc: 0.9742
loss: 0.0760, acc: 0.9722
loss: 0.0727, acc: 0.9750
loss: 0.0698, acc: 0.9761
loss: 0.0699, acc: 0.9766
loss: 0.0726, acc: 0.9760
loss: 0.0736, acc: 0.9750
loss: 0.0760, acc: 0.9738
loss: 0.0768, acc: 0.9727
loss: 0.0771, acc: 0.9721
loss: 0.0765, acc: 0.9726
loss: 0.0751, acc: 0.9730
loss: 0.0745, acc: 0.9734
loss: 0.0755, acc: 0.9729
loss: 0.0763, acc: 0.9724
> val_acc: 0.8473, val_f1: 0.7702
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0446, acc: 1.0000
loss: 0.0674, acc: 0.9866
loss: 0.0570, acc: 0.9870
loss: 0.0644, acc: 0.9853
loss: 0.0594, acc: 0.9844
loss: 0.0548, acc: 0.9861
loss: 0.0515, acc: 0.9863
loss: 0.0458, acc: 0.9882
loss: 0.0501, acc: 0.9859
loss: 0.0483, acc: 0.9860
loss: 0.0470, acc: 0.9862
loss: 0.0441, acc: 0.9874
loss: 0.0468, acc: 0.9859
loss: 0.0474, acc: 0.9855
loss: 0.0518, acc: 0.9835
loss: 0.0520, acc: 0.9830
loss: 0.0515, acc: 0.9832
loss: 0.0531, acc: 0.9828
loss: 0.0553, acc: 0.9823
loss: 0.0570, acc: 0.9816
loss: 0.0577, acc: 0.9813
loss: 0.0595, acc: 0.9810
loss: 0.0591, acc: 0.9813
> val_acc: 0.8295, val_f1: 0.7335
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0331, acc: 0.9922
loss: 0.0262, acc: 0.9965
loss: 0.0245, acc: 0.9955
loss: 0.0238, acc: 0.9951
loss: 0.0301, acc: 0.9922
loss: 0.0341, acc: 0.9903
loss: 0.0340, acc: 0.9908
loss: 0.0308, acc: 0.9920
loss: 0.0361, acc: 0.9908
loss: 0.0390, acc: 0.9904
loss: 0.0420, acc: 0.9896
loss: 0.0429, acc: 0.9889
loss: 0.0426, acc: 0.9888
loss: 0.0422, acc: 0.9887
loss: 0.0427, acc: 0.9886
loss: 0.0414, acc: 0.9889
loss: 0.0419, acc: 0.9888
loss: 0.0399, acc: 0.9895
loss: 0.0410, acc: 0.9890
loss: 0.0413, acc: 0.9883
loss: 0.0405, acc: 0.9886
loss: 0.0412, acc: 0.9885
> val_acc: 0.8446, val_f1: 0.7650
>> early stop.
>> test_acc: 0.8473, test_f1: 0.7688
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2253822464
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0715, acc: 0.5000
loss: 1.0487, acc: 0.5281
loss: 1.0166, acc: 0.5375
loss: 0.9956, acc: 0.5437
loss: 0.9676, acc: 0.5600
loss: 0.9345, acc: 0.5844
loss: 0.9040, acc: 0.6027
loss: 0.8786, acc: 0.6141
loss: 0.8509, acc: 0.6292
loss: 0.8405, acc: 0.6300
loss: 0.8151, acc: 0.6449
loss: 0.8046, acc: 0.6516
loss: 0.7955, acc: 0.6562
loss: 0.7870, acc: 0.6629
loss: 0.7803, acc: 0.6675
loss: 0.7684, acc: 0.6750
loss: 0.7518, acc: 0.6838
loss: 0.7390, acc: 0.6903
loss: 0.7293, acc: 0.6937
loss: 0.7179, acc: 0.7006
loss: 0.7167, acc: 0.7009
loss: 0.7089, acc: 0.7048
> val_acc: 0.8205, val_f1: 0.7070
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8205_f1_0.707_230828-0441.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4527, acc: 0.7969
loss: 0.4715, acc: 0.8259
loss: 0.4562, acc: 0.8281
loss: 0.4410, acc: 0.8401
loss: 0.4478, acc: 0.8324
loss: 0.4548, acc: 0.8275
loss: 0.4467, acc: 0.8330
loss: 0.4496, acc: 0.8277
loss: 0.4423, acc: 0.8296
loss: 0.4352, acc: 0.8338
loss: 0.4315, acc: 0.8353
loss: 0.4273, acc: 0.8372
loss: 0.4336, acc: 0.8357
loss: 0.4310, acc: 0.8372
loss: 0.4271, acc: 0.8394
loss: 0.4292, acc: 0.8377
loss: 0.4242, acc: 0.8407
loss: 0.4192, acc: 0.8420
loss: 0.4151, acc: 0.8448
loss: 0.4178, acc: 0.8425
loss: 0.4186, acc: 0.8416
loss: 0.4181, acc: 0.8426
loss: 0.4164, acc: 0.8435
> val_acc: 0.8330, val_f1: 0.7445
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.833_f1_0.7445_230828-0442.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.1926, acc: 0.9375
loss: 0.2693, acc: 0.9097
loss: 0.2786, acc: 0.9062
loss: 0.2702, acc: 0.9013
loss: 0.2577, acc: 0.9036
loss: 0.2543, acc: 0.9030
loss: 0.2577, acc: 0.9017
loss: 0.2732, acc: 0.8966
loss: 0.2734, acc: 0.8963
loss: 0.2722, acc: 0.8967
loss: 0.2691, acc: 0.8993
loss: 0.2639, acc: 0.9025
loss: 0.2553, acc: 0.9067
loss: 0.2556, acc: 0.9072
loss: 0.2524, acc: 0.9084
loss: 0.2530, acc: 0.9074
loss: 0.2552, acc: 0.9062
loss: 0.2507, acc: 0.9087
loss: 0.2528, acc: 0.9079
loss: 0.2513, acc: 0.9078
loss: 0.2507, acc: 0.9084
loss: 0.2520, acc: 0.9071
> val_acc: 0.8473, val_f1: 0.7737
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8473_f1_0.7737_230828-0443.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2164, acc: 0.9688
loss: 0.1590, acc: 0.9635
loss: 0.1453, acc: 0.9602
loss: 0.1515, acc: 0.9492
loss: 0.1642, acc: 0.9435
loss: 0.1607, acc: 0.9459
loss: 0.1569, acc: 0.9456
loss: 0.1527, acc: 0.9497
loss: 0.1467, acc: 0.9520
loss: 0.1466, acc: 0.9524
loss: 0.1436, acc: 0.9516
loss: 0.1454, acc: 0.9520
loss: 0.1463, acc: 0.9513
loss: 0.1554, acc: 0.9479
loss: 0.1560, acc: 0.9476
loss: 0.1520, acc: 0.9490
loss: 0.1561, acc: 0.9479
loss: 0.1600, acc: 0.9462
loss: 0.1609, acc: 0.9461
loss: 0.1605, acc: 0.9469
loss: 0.1591, acc: 0.9477
loss: 0.1585, acc: 0.9475
loss: 0.1607, acc: 0.9459
> val_acc: 0.8562, val_f1: 0.7926
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8562_f1_0.7926_230828-0443.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0819, acc: 0.9792
loss: 0.0892, acc: 0.9688
loss: 0.0689, acc: 0.9784
loss: 0.0597, acc: 0.9826
loss: 0.0782, acc: 0.9769
loss: 0.0871, acc: 0.9721
loss: 0.0889, acc: 0.9716
loss: 0.0887, acc: 0.9712
loss: 0.0885, acc: 0.9709
loss: 0.0874, acc: 0.9714
loss: 0.0856, acc: 0.9723
loss: 0.0910, acc: 0.9704
loss: 0.0932, acc: 0.9702
loss: 0.0921, acc: 0.9697
loss: 0.0929, acc: 0.9696
loss: 0.0972, acc: 0.9683
loss: 0.0942, acc: 0.9695
loss: 0.0939, acc: 0.9698
loss: 0.0987, acc: 0.9684
loss: 0.1000, acc: 0.9684
loss: 0.1021, acc: 0.9678
loss: 0.1028, acc: 0.9676
loss: 0.1029, acc: 0.9670
> val_acc: 0.8482, val_f1: 0.7625
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0445, acc: 0.9938
loss: 0.0448, acc: 0.9938
loss: 0.0369, acc: 0.9938
loss: 0.0407, acc: 0.9906
loss: 0.0404, acc: 0.9900
loss: 0.0390, acc: 0.9885
loss: 0.0429, acc: 0.9875
loss: 0.0463, acc: 0.9859
loss: 0.0445, acc: 0.9861
loss: 0.0465, acc: 0.9856
loss: 0.0482, acc: 0.9852
loss: 0.0477, acc: 0.9849
loss: 0.0517, acc: 0.9832
loss: 0.0513, acc: 0.9830
loss: 0.0526, acc: 0.9833
loss: 0.0516, acc: 0.9840
loss: 0.0523, acc: 0.9835
loss: 0.0522, acc: 0.9833
loss: 0.0542, acc: 0.9832
loss: 0.0559, acc: 0.9825
loss: 0.0558, acc: 0.9827
loss: 0.0550, acc: 0.9832
> val_acc: 0.8438, val_f1: 0.7612
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0416, acc: 0.9844
loss: 0.0672, acc: 0.9866
loss: 0.0509, acc: 0.9896
loss: 0.0430, acc: 0.9908
loss: 0.0399, acc: 0.9901
loss: 0.0378, acc: 0.9896
loss: 0.0407, acc: 0.9873
loss: 0.0414, acc: 0.9865
loss: 0.0402, acc: 0.9859
loss: 0.0552, acc: 0.9834
loss: 0.0577, acc: 0.9814
loss: 0.0552, acc: 0.9825
loss: 0.0601, acc: 0.9819
loss: 0.0590, acc: 0.9823
loss: 0.0561, acc: 0.9835
loss: 0.0548, acc: 0.9838
loss: 0.0541, acc: 0.9836
loss: 0.0543, acc: 0.9831
loss: 0.0543, acc: 0.9834
loss: 0.0554, acc: 0.9826
loss: 0.0581, acc: 0.9819
loss: 0.0588, acc: 0.9810
loss: 0.0575, acc: 0.9816
> val_acc: 0.8455, val_f1: 0.7634
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0480, acc: 0.9766
loss: 0.0529, acc: 0.9826
loss: 0.0407, acc: 0.9866
loss: 0.0564, acc: 0.9836
loss: 0.0516, acc: 0.9844
loss: 0.0467, acc: 0.9860
loss: 0.0474, acc: 0.9871
loss: 0.0466, acc: 0.9856
loss: 0.0450, acc: 0.9858
loss: 0.0427, acc: 0.9866
loss: 0.0424, acc: 0.9867
loss: 0.0428, acc: 0.9868
loss: 0.0464, acc: 0.9854
loss: 0.0466, acc: 0.9855
loss: 0.0478, acc: 0.9848
loss: 0.0471, acc: 0.9850
loss: 0.0488, acc: 0.9847
loss: 0.0478, acc: 0.9853
loss: 0.0496, acc: 0.9850
loss: 0.0484, acc: 0.9855
loss: 0.0495, acc: 0.9850
loss: 0.0494, acc: 0.9848
> val_acc: 0.8562, val_f1: 0.7868
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0093, acc: 1.0000
loss: 0.0454, acc: 0.9896
loss: 0.0403, acc: 0.9915
loss: 0.0325, acc: 0.9941
loss: 0.0295, acc: 0.9926
loss: 0.0412, acc: 0.9880
loss: 0.0366, acc: 0.9899
loss: 0.0423, acc: 0.9870
loss: 0.0390, acc: 0.9886
loss: 0.0370, acc: 0.9891
loss: 0.0351, acc: 0.9896
loss: 0.0332, acc: 0.9905
loss: 0.0329, acc: 0.9908
loss: 0.0306, acc: 0.9915
loss: 0.0346, acc: 0.9912
loss: 0.0343, acc: 0.9901
loss: 0.0337, acc: 0.9904
loss: 0.0340, acc: 0.9898
loss: 0.0360, acc: 0.9894
loss: 0.0356, acc: 0.9893
loss: 0.0375, acc: 0.9879
loss: 0.0386, acc: 0.9876
loss: 0.0387, acc: 0.9873
> val_acc: 0.8384, val_f1: 0.7438
>> early stop.
>> test_acc: 0.8562, test_f1: 0.7926
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2261162496
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 0.9995, acc: 0.5750
loss: 0.9916, acc: 0.5844
loss: 1.0205, acc: 0.5563
loss: 1.0088, acc: 0.5547
loss: 0.9837, acc: 0.5663
loss: 0.9608, acc: 0.5719
loss: 0.9290, acc: 0.5920
loss: 0.8968, acc: 0.6070
loss: 0.8871, acc: 0.6125
loss: 0.8698, acc: 0.6231
loss: 0.8468, acc: 0.6335
loss: 0.8362, acc: 0.6370
loss: 0.8254, acc: 0.6428
loss: 0.8151, acc: 0.6460
loss: 0.8026, acc: 0.6517
loss: 0.7910, acc: 0.6570
loss: 0.7757, acc: 0.6636
loss: 0.7650, acc: 0.6694
loss: 0.7568, acc: 0.6743
loss: 0.7482, acc: 0.6791
loss: 0.7434, acc: 0.6804
loss: 0.7399, acc: 0.6827
> val_acc: 0.7759, val_f1: 0.5841
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.7759_f1_0.5841_230828-0448.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5580, acc: 0.7500
loss: 0.5669, acc: 0.7723
loss: 0.5130, acc: 0.8073
loss: 0.5065, acc: 0.8088
loss: 0.5039, acc: 0.8082
loss: 0.5082, acc: 0.7951
loss: 0.4988, acc: 0.8008
loss: 0.4947, acc: 0.8015
loss: 0.4800, acc: 0.8065
loss: 0.4778, acc: 0.8085
loss: 0.4772, acc: 0.8089
loss: 0.4703, acc: 0.8136
loss: 0.4618, acc: 0.8175
loss: 0.4616, acc: 0.8181
loss: 0.4594, acc: 0.8173
loss: 0.4599, acc: 0.8174
loss: 0.4556, acc: 0.8182
loss: 0.4521, acc: 0.8182
loss: 0.4572, acc: 0.8176
loss: 0.4522, acc: 0.8206
loss: 0.4490, acc: 0.8226
loss: 0.4431, acc: 0.8262
loss: 0.4411, acc: 0.8278
> val_acc: 0.8304, val_f1: 0.7233
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8304_f1_0.7233_230828-0448.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.1611, acc: 0.9531
loss: 0.2393, acc: 0.9271
loss: 0.2408, acc: 0.9263
loss: 0.2626, acc: 0.9145
loss: 0.2644, acc: 0.9167
loss: 0.2651, acc: 0.9138
loss: 0.2772, acc: 0.9044
loss: 0.2691, acc: 0.9079
loss: 0.2713, acc: 0.9048
loss: 0.2665, acc: 0.9056
loss: 0.2581, acc: 0.9097
loss: 0.2598, acc: 0.9078
loss: 0.2569, acc: 0.9082
loss: 0.2587, acc: 0.9067
loss: 0.2571, acc: 0.9067
loss: 0.2591, acc: 0.9070
loss: 0.2579, acc: 0.9085
loss: 0.2601, acc: 0.9070
loss: 0.2594, acc: 0.9072
loss: 0.2591, acc: 0.9059
loss: 0.2575, acc: 0.9062
loss: 0.2575, acc: 0.9065
> val_acc: 0.8429, val_f1: 0.7729
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8429_f1_0.7729_230828-0449.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2683, acc: 0.9062
loss: 0.1579, acc: 0.9427
loss: 0.1372, acc: 0.9545
loss: 0.1211, acc: 0.9609
loss: 0.1200, acc: 0.9598
loss: 0.1314, acc: 0.9531
loss: 0.1280, acc: 0.9567
loss: 0.1232, acc: 0.9575
loss: 0.1231, acc: 0.9581
loss: 0.1179, acc: 0.9592
loss: 0.1205, acc: 0.9583
loss: 0.1233, acc: 0.9570
loss: 0.1227, acc: 0.9565
loss: 0.1244, acc: 0.9564
loss: 0.1278, acc: 0.9542
loss: 0.1283, acc: 0.9531
loss: 0.1263, acc: 0.9545
loss: 0.1289, acc: 0.9524
loss: 0.1323, acc: 0.9512
loss: 0.1325, acc: 0.9508
loss: 0.1322, acc: 0.9511
loss: 0.1366, acc: 0.9502
loss: 0.1368, acc: 0.9499
> val_acc: 0.8455, val_f1: 0.7613
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8455_f1_0.7613_230828-0450.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0281, acc: 1.0000
loss: 0.0441, acc: 0.9922
loss: 0.0597, acc: 0.9856
loss: 0.0796, acc: 0.9792
loss: 0.0762, acc: 0.9796
loss: 0.0693, acc: 0.9810
loss: 0.0761, acc: 0.9801
loss: 0.0789, acc: 0.9794
loss: 0.0779, acc: 0.9789
loss: 0.0779, acc: 0.9779
loss: 0.0777, acc: 0.9776
loss: 0.0828, acc: 0.9731
loss: 0.0875, acc: 0.9717
loss: 0.0890, acc: 0.9710
loss: 0.0893, acc: 0.9705
loss: 0.0882, acc: 0.9712
loss: 0.0852, acc: 0.9721
loss: 0.0869, acc: 0.9712
loss: 0.0887, acc: 0.9704
loss: 0.0878, acc: 0.9707
loss: 0.0891, acc: 0.9706
loss: 0.0908, acc: 0.9699
loss: 0.0928, acc: 0.9687
> val_acc: 0.8411, val_f1: 0.7454
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0256, acc: 1.0000
loss: 0.0635, acc: 0.9875
loss: 0.0613, acc: 0.9854
loss: 0.0604, acc: 0.9844
loss: 0.0540, acc: 0.9862
loss: 0.0540, acc: 0.9854
loss: 0.0546, acc: 0.9857
loss: 0.0630, acc: 0.9836
loss: 0.0608, acc: 0.9833
loss: 0.0635, acc: 0.9831
loss: 0.0623, acc: 0.9835
loss: 0.0605, acc: 0.9839
loss: 0.0628, acc: 0.9827
loss: 0.0644, acc: 0.9812
loss: 0.0644, acc: 0.9817
loss: 0.0648, acc: 0.9816
loss: 0.0650, acc: 0.9809
loss: 0.0662, acc: 0.9806
loss: 0.0669, acc: 0.9793
loss: 0.0667, acc: 0.9791
loss: 0.0662, acc: 0.9786
loss: 0.0653, acc: 0.9787
> val_acc: 0.8393, val_f1: 0.7478
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0195, acc: 1.0000
loss: 0.0548, acc: 0.9821
loss: 0.0736, acc: 0.9792
loss: 0.0606, acc: 0.9816
loss: 0.0608, acc: 0.9815
loss: 0.0626, acc: 0.9826
loss: 0.0587, acc: 0.9834
loss: 0.0577, acc: 0.9840
loss: 0.0597, acc: 0.9829
loss: 0.0579, acc: 0.9834
loss: 0.0532, acc: 0.9850
loss: 0.0503, acc: 0.9852
loss: 0.0508, acc: 0.9859
loss: 0.0504, acc: 0.9860
loss: 0.0492, acc: 0.9861
loss: 0.0470, acc: 0.9866
loss: 0.0477, acc: 0.9870
loss: 0.0501, acc: 0.9867
loss: 0.0494, acc: 0.9871
loss: 0.0485, acc: 0.9871
loss: 0.0489, acc: 0.9871
loss: 0.0482, acc: 0.9869
loss: 0.0473, acc: 0.9869
> val_acc: 0.8562, val_f1: 0.7919
>> saved: /media/b115/Backup/NLP/bert/rest14/acc_0.8562_f1_0.7919_230828-0452.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0096, acc: 1.0000
loss: 0.0188, acc: 0.9965
loss: 0.0225, acc: 0.9955
loss: 0.0227, acc: 0.9934
loss: 0.0228, acc: 0.9935
loss: 0.0234, acc: 0.9935
loss: 0.0231, acc: 0.9936
loss: 0.0255, acc: 0.9928
loss: 0.0263, acc: 0.9929
loss: 0.0267, acc: 0.9917
loss: 0.0264, acc: 0.9913
loss: 0.0296, acc: 0.9899
loss: 0.0352, acc: 0.9888
loss: 0.0342, acc: 0.9891
loss: 0.0341, acc: 0.9886
loss: 0.0346, acc: 0.9885
loss: 0.0346, acc: 0.9888
loss: 0.0354, acc: 0.9884
loss: 0.0362, acc: 0.9880
loss: 0.0381, acc: 0.9877
loss: 0.0400, acc: 0.9871
loss: 0.0401, acc: 0.9874
> val_acc: 0.8455, val_f1: 0.7661
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0074, acc: 1.0000
loss: 0.0134, acc: 0.9948
loss: 0.0190, acc: 0.9943
loss: 0.0173, acc: 0.9941
loss: 0.0222, acc: 0.9911
loss: 0.0225, acc: 0.9904
loss: 0.0204, acc: 0.9919
loss: 0.0188, acc: 0.9922
loss: 0.0202, acc: 0.9916
loss: 0.0267, acc: 0.9898
loss: 0.0260, acc: 0.9902
loss: 0.0253, acc: 0.9905
loss: 0.0243, acc: 0.9913
loss: 0.0232, acc: 0.9915
loss: 0.0233, acc: 0.9912
loss: 0.0251, acc: 0.9905
loss: 0.0315, acc: 0.9892
loss: 0.0333, acc: 0.9891
loss: 0.0343, acc: 0.9890
loss: 0.0344, acc: 0.9889
loss: 0.0337, acc: 0.9892
loss: 0.0347, acc: 0.9888
loss: 0.0344, acc: 0.9887
> val_acc: 0.8375, val_f1: 0.7436
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0170, acc: 1.0000
loss: 0.0273, acc: 0.9922
loss: 0.0269, acc: 0.9904
loss: 0.0277, acc: 0.9913
loss: 0.0257, acc: 0.9918
loss: 0.0222, acc: 0.9933
loss: 0.0211, acc: 0.9934
loss: 0.0214, acc: 0.9926
loss: 0.0213, acc: 0.9927
loss: 0.0194, acc: 0.9935
loss: 0.0223, acc: 0.9923
loss: 0.0226, acc: 0.9919
loss: 0.0247, acc: 0.9916
loss: 0.0233, acc: 0.9922
loss: 0.0231, acc: 0.9923
loss: 0.0229, acc: 0.9924
loss: 0.0228, acc: 0.9921
loss: 0.0223, acc: 0.9925
loss: 0.0216, acc: 0.9929
loss: 0.0210, acc: 0.9930
loss: 0.0212, acc: 0.9927
loss: 0.0214, acc: 0.9925
loss: 0.0236, acc: 0.9920
> val_acc: 0.8295, val_f1: 0.7221
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0116, acc: 0.9938
loss: 0.0134, acc: 0.9938
loss: 0.0227, acc: 0.9917
loss: 0.0181, acc: 0.9938
loss: 0.0166, acc: 0.9938
loss: 0.0145, acc: 0.9948
loss: 0.0158, acc: 0.9938
loss: 0.0143, acc: 0.9945
loss: 0.0131, acc: 0.9951
loss: 0.0142, acc: 0.9950
loss: 0.0136, acc: 0.9949
loss: 0.0128, acc: 0.9953
loss: 0.0131, acc: 0.9947
loss: 0.0160, acc: 0.9933
loss: 0.0198, acc: 0.9925
loss: 0.0248, acc: 0.9914
loss: 0.0252, acc: 0.9915
loss: 0.0254, acc: 0.9913
loss: 0.0258, acc: 0.9914
loss: 0.0259, acc: 0.9916
loss: 0.0255, acc: 0.9920
loss: 0.0252, acc: 0.9920
> val_acc: 0.8384, val_f1: 0.7484
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.0046, acc: 1.0000
loss: 0.0061, acc: 1.0000
loss: 0.0063, acc: 1.0000
loss: 0.0056, acc: 1.0000
loss: 0.0112, acc: 0.9986
loss: 0.0294, acc: 0.9931
loss: 0.0307, acc: 0.9922
loss: 0.0436, acc: 0.9873
loss: 0.0425, acc: 0.9866
loss: 0.0393, acc: 0.9880
loss: 0.0371, acc: 0.9892
loss: 0.0345, acc: 0.9901
loss: 0.0358, acc: 0.9894
loss: 0.0343, acc: 0.9897
loss: 0.0339, acc: 0.9900
loss: 0.0370, acc: 0.9890
loss: 0.0364, acc: 0.9889
loss: 0.0351, acc: 0.9892
loss: 0.0348, acc: 0.9895
loss: 0.0331, acc: 0.9900
loss: 0.0333, acc: 0.9899
loss: 0.0327, acc: 0.9901
loss: 0.0320, acc: 0.9902
> val_acc: 0.8384, val_f1: 0.7445
>> early stop.
>> test_acc: 0.8562, test_f1: 0.7919
>> test_acc: 0.8473, test_f1: 0.7688
>> test_acc: 0.8562, test_f1: 0.7926
>> test_acc: 0.8562, test_f1: 0.7919

>> avg_test_acc: 0.8533, avg_test_f1: 0.7844
>> max_test_acc: 0.8562, max_test_f1: 0.7926
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 499894784
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0269, acc: 0.5813
loss: 1.0012, acc: 0.5844
loss: 0.9932, acc: 0.5833
loss: 0.9668, acc: 0.5922
loss: 0.9218, acc: 0.6050
loss: 0.8813, acc: 0.6115
loss: 0.8553, acc: 0.6268
loss: 0.8484, acc: 0.6383
loss: 0.8299, acc: 0.6507
loss: 0.8073, acc: 0.6544
loss: 0.8038, acc: 0.6517
loss: 0.7986, acc: 0.6526
loss: 0.7963, acc: 0.6534
loss: 0.7889, acc: 0.6527
loss: 0.7735, acc: 0.6617
loss: 0.7688, acc: 0.6633
loss: 0.7635, acc: 0.6680
loss: 0.7528, acc: 0.6722
loss: 0.7476, acc: 0.6743
loss: 0.7357, acc: 0.6794
loss: 0.7238, acc: 0.6863
loss: 0.7213, acc: 0.6875
> val_acc: 0.8187, val_f1: 0.7201
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8187_f1_0.7201_230828-0457.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5562, acc: 0.8438
loss: 0.4549, acc: 0.8616
loss: 0.4692, acc: 0.8333
loss: 0.4638, acc: 0.8327
loss: 0.4823, acc: 0.8210
loss: 0.4931, acc: 0.8125
loss: 0.4940, acc: 0.8096
loss: 0.5006, acc: 0.8057
loss: 0.4917, acc: 0.8095
loss: 0.4854, acc: 0.8132
loss: 0.4790, acc: 0.8137
loss: 0.4740, acc: 0.8185
loss: 0.4724, acc: 0.8191
loss: 0.4680, acc: 0.8190
loss: 0.4719, acc: 0.8168
loss: 0.4694, acc: 0.8194
loss: 0.4691, acc: 0.8190
loss: 0.4648, acc: 0.8204
loss: 0.4601, acc: 0.8217
loss: 0.4571, acc: 0.8231
loss: 0.4480, acc: 0.8269
loss: 0.4488, acc: 0.8259
loss: 0.4457, acc: 0.8265
> val_acc: 0.8232, val_f1: 0.6663
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8232_f1_0.6663_230828-0457.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3810, acc: 0.8359
loss: 0.3139, acc: 0.8681
loss: 0.2881, acc: 0.8884
loss: 0.3072, acc: 0.8832
loss: 0.3092, acc: 0.8815
loss: 0.3151, acc: 0.8772
loss: 0.3085, acc: 0.8833
loss: 0.3037, acc: 0.8822
loss: 0.3034, acc: 0.8842
loss: 0.3034, acc: 0.8846
loss: 0.2991, acc: 0.8866
loss: 0.3032, acc: 0.8840
loss: 0.3058, acc: 0.8804
loss: 0.3082, acc: 0.8809
loss: 0.3064, acc: 0.8826
loss: 0.3035, acc: 0.8833
loss: 0.3041, acc: 0.8836
loss: 0.3065, acc: 0.8838
loss: 0.3008, acc: 0.8853
loss: 0.3010, acc: 0.8851
loss: 0.2973, acc: 0.8873
loss: 0.2977, acc: 0.8879
> val_acc: 0.8625, val_f1: 0.7821
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8625_f1_0.7821_230828-0458.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1434, acc: 0.9062
loss: 0.1114, acc: 0.9583
loss: 0.1237, acc: 0.9545
loss: 0.1690, acc: 0.9355
loss: 0.1678, acc: 0.9330
loss: 0.1676, acc: 0.9387
loss: 0.1779, acc: 0.9365
loss: 0.2014, acc: 0.9280
loss: 0.2012, acc: 0.9291
loss: 0.2001, acc: 0.9287
loss: 0.2001, acc: 0.9265
loss: 0.2002, acc: 0.9269
loss: 0.1927, acc: 0.9303
loss: 0.1917, acc: 0.9304
loss: 0.1908, acc: 0.9305
loss: 0.1961, acc: 0.9285
loss: 0.2059, acc: 0.9240
loss: 0.2064, acc: 0.9241
loss: 0.2034, acc: 0.9251
loss: 0.2021, acc: 0.9255
loss: 0.2025, acc: 0.9251
loss: 0.2074, acc: 0.9231
loss: 0.2097, acc: 0.9223
> val_acc: 0.8554, val_f1: 0.7730
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1611, acc: 0.9271
loss: 0.1289, acc: 0.9531
loss: 0.1078, acc: 0.9615
loss: 0.1096, acc: 0.9601
loss: 0.1125, acc: 0.9592
loss: 0.1105, acc: 0.9587
loss: 0.1082, acc: 0.9564
loss: 0.1104, acc: 0.9581
loss: 0.1208, acc: 0.9528
loss: 0.1192, acc: 0.9538
loss: 0.1176, acc: 0.9552
loss: 0.1144, acc: 0.9574
loss: 0.1165, acc: 0.9573
loss: 0.1286, acc: 0.9550
loss: 0.1277, acc: 0.9546
loss: 0.1262, acc: 0.9559
loss: 0.1333, acc: 0.9537
loss: 0.1344, acc: 0.9538
loss: 0.1341, acc: 0.9540
loss: 0.1355, acc: 0.9531
loss: 0.1360, acc: 0.9527
loss: 0.1379, acc: 0.9525
loss: 0.1366, acc: 0.9523
> val_acc: 0.8661, val_f1: 0.7972
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8661_f1_0.7972_230828-0500.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0724, acc: 0.9750
loss: 0.0614, acc: 0.9812
loss: 0.0910, acc: 0.9750
loss: 0.0814, acc: 0.9781
loss: 0.0816, acc: 0.9788
loss: 0.0941, acc: 0.9740
loss: 0.1046, acc: 0.9696
loss: 0.1000, acc: 0.9688
loss: 0.0980, acc: 0.9694
loss: 0.0994, acc: 0.9688
loss: 0.0951, acc: 0.9693
loss: 0.0925, acc: 0.9698
loss: 0.0941, acc: 0.9702
loss: 0.0962, acc: 0.9688
loss: 0.0920, acc: 0.9700
loss: 0.0923, acc: 0.9703
loss: 0.0895, acc: 0.9710
loss: 0.0911, acc: 0.9701
loss: 0.0917, acc: 0.9701
loss: 0.0994, acc: 0.9678
loss: 0.0971, acc: 0.9690
loss: 0.0990, acc: 0.9688
> val_acc: 0.8598, val_f1: 0.7702
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0498, acc: 1.0000
loss: 0.0871, acc: 0.9732
loss: 0.0828, acc: 0.9766
loss: 0.0736, acc: 0.9779
loss: 0.0696, acc: 0.9773
loss: 0.0659, acc: 0.9780
loss: 0.0666, acc: 0.9795
loss: 0.0612, acc: 0.9814
loss: 0.0566, acc: 0.9829
loss: 0.0546, acc: 0.9840
loss: 0.0522, acc: 0.9838
loss: 0.0542, acc: 0.9836
loss: 0.0585, acc: 0.9819
loss: 0.0557, acc: 0.9827
loss: 0.0589, acc: 0.9813
loss: 0.0595, acc: 0.9813
loss: 0.0633, acc: 0.9790
loss: 0.0626, acc: 0.9788
loss: 0.0638, acc: 0.9783
loss: 0.0692, acc: 0.9774
loss: 0.0715, acc: 0.9770
loss: 0.0743, acc: 0.9755
loss: 0.0761, acc: 0.9746
> val_acc: 0.8571, val_f1: 0.7670
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0978, acc: 0.9609
loss: 0.0659, acc: 0.9757
loss: 0.0484, acc: 0.9844
loss: 0.0492, acc: 0.9819
loss: 0.0510, acc: 0.9792
loss: 0.0497, acc: 0.9806
loss: 0.0473, acc: 0.9816
loss: 0.0474, acc: 0.9816
loss: 0.0493, acc: 0.9830
loss: 0.0475, acc: 0.9834
loss: 0.0487, acc: 0.9826
loss: 0.0530, acc: 0.9820
loss: 0.0540, acc: 0.9810
loss: 0.0568, acc: 0.9796
loss: 0.0571, acc: 0.9789
loss: 0.0599, acc: 0.9778
loss: 0.0607, acc: 0.9784
loss: 0.0641, acc: 0.9772
loss: 0.0660, acc: 0.9767
loss: 0.0692, acc: 0.9754
loss: 0.0711, acc: 0.9742
loss: 0.0754, acc: 0.9725
> val_acc: 0.8554, val_f1: 0.7856
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0916, acc: 0.9688
loss: 0.0506, acc: 0.9740
loss: 0.0682, acc: 0.9716
loss: 0.0802, acc: 0.9668
loss: 0.0708, acc: 0.9717
loss: 0.0708, acc: 0.9736
loss: 0.0846, acc: 0.9708
loss: 0.0910, acc: 0.9688
loss: 0.0905, acc: 0.9710
loss: 0.0873, acc: 0.9721
loss: 0.0866, acc: 0.9724
loss: 0.0829, acc: 0.9732
loss: 0.0787, acc: 0.9744
loss: 0.0777, acc: 0.9744
loss: 0.0746, acc: 0.9749
loss: 0.0724, acc: 0.9757
loss: 0.0724, acc: 0.9761
loss: 0.0724, acc: 0.9760
loss: 0.0713, acc: 0.9760
loss: 0.0696, acc: 0.9766
loss: 0.0688, acc: 0.9768
loss: 0.0668, acc: 0.9776
loss: 0.0653, acc: 0.9780
> val_acc: 0.8670, val_f1: 0.7944
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.867_f1_0.7944_230828-0502.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0203, acc: 1.0000
loss: 0.0429, acc: 0.9883
loss: 0.0402, acc: 0.9880
loss: 0.0429, acc: 0.9861
loss: 0.0430, acc: 0.9851
loss: 0.0611, acc: 0.9810
loss: 0.0642, acc: 0.9792
loss: 0.0639, acc: 0.9794
loss: 0.0627, acc: 0.9797
loss: 0.0696, acc: 0.9766
loss: 0.0695, acc: 0.9758
loss: 0.0681, acc: 0.9763
loss: 0.0711, acc: 0.9747
loss: 0.0730, acc: 0.9743
loss: 0.0724, acc: 0.9747
loss: 0.0730, acc: 0.9744
loss: 0.0705, acc: 0.9752
loss: 0.0677, acc: 0.9766
loss: 0.0668, acc: 0.9775
loss: 0.0681, acc: 0.9774
loss: 0.0674, acc: 0.9775
loss: 0.0659, acc: 0.9780
loss: 0.0659, acc: 0.9784
> val_acc: 0.8607, val_f1: 0.7830
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0585, acc: 0.9938
loss: 0.0486, acc: 0.9938
loss: 0.0352, acc: 0.9958
loss: 0.0308, acc: 0.9953
loss: 0.0293, acc: 0.9938
loss: 0.0282, acc: 0.9948
loss: 0.0321, acc: 0.9929
loss: 0.0313, acc: 0.9930
loss: 0.0286, acc: 0.9938
loss: 0.0305, acc: 0.9925
loss: 0.0403, acc: 0.9892
loss: 0.0545, acc: 0.9865
loss: 0.0556, acc: 0.9851
loss: 0.0582, acc: 0.9835
loss: 0.0571, acc: 0.9833
loss: 0.0621, acc: 0.9824
loss: 0.0644, acc: 0.9809
loss: 0.0625, acc: 0.9816
loss: 0.0612, acc: 0.9819
loss: 0.0589, acc: 0.9825
loss: 0.0581, acc: 0.9824
loss: 0.0578, acc: 0.9824
> val_acc: 0.8634, val_f1: 0.7801
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.0550, acc: 0.9844
loss: 0.0259, acc: 0.9911
loss: 0.0173, acc: 0.9948
loss: 0.0145, acc: 0.9963
loss: 0.0139, acc: 0.9972
loss: 0.0143, acc: 0.9965
loss: 0.0128, acc: 0.9971
loss: 0.0138, acc: 0.9966
loss: 0.0175, acc: 0.9963
loss: 0.0189, acc: 0.9953
loss: 0.0192, acc: 0.9952
loss: 0.0181, acc: 0.9956
loss: 0.0173, acc: 0.9960
loss: 0.0218, acc: 0.9949
loss: 0.0247, acc: 0.9939
loss: 0.0261, acc: 0.9935
loss: 0.0265, acc: 0.9931
loss: 0.0262, acc: 0.9932
loss: 0.0269, acc: 0.9929
loss: 0.0266, acc: 0.9926
loss: 0.0283, acc: 0.9923
loss: 0.0278, acc: 0.9924
loss: 0.0292, acc: 0.9916
> val_acc: 0.8705, val_f1: 0.8042
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8705_f1_0.8042_230828-0505.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.0106, acc: 1.0000
loss: 0.0290, acc: 0.9931
loss: 0.0252, acc: 0.9933
loss: 0.0211, acc: 0.9951
loss: 0.0297, acc: 0.9922
loss: 0.0256, acc: 0.9935
loss: 0.0229, acc: 0.9945
loss: 0.0238, acc: 0.9936
loss: 0.0222, acc: 0.9936
loss: 0.0218, acc: 0.9936
loss: 0.0226, acc: 0.9936
loss: 0.0255, acc: 0.9931
loss: 0.0244, acc: 0.9937
loss: 0.0256, acc: 0.9923
loss: 0.0266, acc: 0.9920
loss: 0.0295, acc: 0.9909
loss: 0.0283, acc: 0.9911
loss: 0.0291, acc: 0.9909
loss: 0.0278, acc: 0.9914
loss: 0.0267, acc: 0.9918
loss: 0.0268, acc: 0.9916
loss: 0.0270, acc: 0.9914
> val_acc: 0.8768, val_f1: 0.8163
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8768_f1_0.8163_230828-0505.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.0166, acc: 1.0000
loss: 0.0103, acc: 1.0000
loss: 0.0095, acc: 1.0000
loss: 0.0169, acc: 0.9941
loss: 0.0170, acc: 0.9926
loss: 0.0172, acc: 0.9916
loss: 0.0226, acc: 0.9909
loss: 0.0279, acc: 0.9878
loss: 0.0267, acc: 0.9886
loss: 0.0296, acc: 0.9878
loss: 0.0330, acc: 0.9877
loss: 0.0325, acc: 0.9883
loss: 0.0368, acc: 0.9872
loss: 0.0369, acc: 0.9872
loss: 0.0361, acc: 0.9877
loss: 0.0353, acc: 0.9881
loss: 0.0357, acc: 0.9880
loss: 0.0362, acc: 0.9873
loss: 0.0373, acc: 0.9870
loss: 0.0387, acc: 0.9867
loss: 0.0387, acc: 0.9867
loss: 0.0391, acc: 0.9867
loss: 0.0384, acc: 0.9868
> val_acc: 0.8625, val_f1: 0.7872
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 0.0071, acc: 1.0000
loss: 0.0296, acc: 0.9961
loss: 0.0299, acc: 0.9928
loss: 0.0292, acc: 0.9913
loss: 0.0283, acc: 0.9891
loss: 0.0351, acc: 0.9855
loss: 0.0358, acc: 0.9867
loss: 0.0333, acc: 0.9877
loss: 0.0319, acc: 0.9884
loss: 0.0314, acc: 0.9883
loss: 0.0353, acc: 0.9870
loss: 0.0362, acc: 0.9876
loss: 0.0355, acc: 0.9876
loss: 0.0353, acc: 0.9876
loss: 0.0356, acc: 0.9867
loss: 0.0343, acc: 0.9872
loss: 0.0331, acc: 0.9876
loss: 0.0333, acc: 0.9872
loss: 0.0339, acc: 0.9876
loss: 0.0343, acc: 0.9876
loss: 0.0353, acc: 0.9863
loss: 0.0347, acc: 0.9867
loss: 0.0360, acc: 0.9859
> val_acc: 0.8536, val_f1: 0.7756
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 15
loss: 0.0531, acc: 0.9688
loss: 0.0599, acc: 0.9750
loss: 0.0705, acc: 0.9771
loss: 0.0688, acc: 0.9766
loss: 0.0660, acc: 0.9762
loss: 0.0577, acc: 0.9802
loss: 0.0559, acc: 0.9786
loss: 0.0527, acc: 0.9797
loss: 0.0473, acc: 0.9819
loss: 0.0451, acc: 0.9819
loss: 0.0464, acc: 0.9824
loss: 0.0479, acc: 0.9807
loss: 0.0452, acc: 0.9822
loss: 0.0468, acc: 0.9808
loss: 0.0453, acc: 0.9817
loss: 0.0449, acc: 0.9816
loss: 0.0433, acc: 0.9824
loss: 0.0436, acc: 0.9816
loss: 0.0465, acc: 0.9812
loss: 0.0447, acc: 0.9819
loss: 0.0463, acc: 0.9815
loss: 0.0455, acc: 0.9815
> val_acc: 0.8679, val_f1: 0.7911
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 16
loss: 0.0060, acc: 1.0000
loss: 0.0360, acc: 0.9911
loss: 0.0291, acc: 0.9896
loss: 0.0275, acc: 0.9908
loss: 0.0251, acc: 0.9915
loss: 0.0256, acc: 0.9907
loss: 0.0277, acc: 0.9902
loss: 0.0354, acc: 0.9890
loss: 0.0353, acc: 0.9881
loss: 0.0334, acc: 0.9887
loss: 0.0339, acc: 0.9880
loss: 0.0313, acc: 0.9890
loss: 0.0318, acc: 0.9889
loss: 0.0299, acc: 0.9897
loss: 0.0295, acc: 0.9896
loss: 0.0295, acc: 0.9894
loss: 0.0285, acc: 0.9897
loss: 0.0279, acc: 0.9896
loss: 0.0273, acc: 0.9898
loss: 0.0271, acc: 0.9900
loss: 0.0265, acc: 0.9902
loss: 0.0261, acc: 0.9901
loss: 0.0260, acc: 0.9902
> val_acc: 0.8750, val_f1: 0.8114
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 17
loss: 0.0106, acc: 1.0000
loss: 0.0092, acc: 1.0000
loss: 0.0111, acc: 0.9978
loss: 0.0089, acc: 0.9984
loss: 0.0074, acc: 0.9987
loss: 0.0078, acc: 0.9978
loss: 0.0105, acc: 0.9954
loss: 0.0097, acc: 0.9960
loss: 0.0116, acc: 0.9957
loss: 0.0154, acc: 0.9936
loss: 0.0173, acc: 0.9931
loss: 0.0192, acc: 0.9926
loss: 0.0201, acc: 0.9922
loss: 0.0230, acc: 0.9923
loss: 0.0230, acc: 0.9924
loss: 0.0237, acc: 0.9917
loss: 0.0240, acc: 0.9918
loss: 0.0231, acc: 0.9923
loss: 0.0240, acc: 0.9924
loss: 0.0251, acc: 0.9915
loss: 0.0242, acc: 0.9919
loss: 0.0276, acc: 0.9911
> val_acc: 0.8634, val_f1: 0.7797
>> early stop.
>> test_acc: 0.8768, test_f1: 0.8163
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2558812672
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0119, acc: 0.6062
loss: 1.0069, acc: 0.5875
loss: 0.9646, acc: 0.5958
loss: 0.9141, acc: 0.6109
loss: 0.8853, acc: 0.6288
loss: 0.8727, acc: 0.6375
loss: 0.8681, acc: 0.6455
loss: 0.8551, acc: 0.6539
loss: 0.8334, acc: 0.6632
loss: 0.8182, acc: 0.6687
loss: 0.7967, acc: 0.6784
loss: 0.7855, acc: 0.6828
loss: 0.7762, acc: 0.6851
loss: 0.7714, acc: 0.6875
loss: 0.7656, acc: 0.6879
loss: 0.7535, acc: 0.6945
loss: 0.7438, acc: 0.6982
loss: 0.7307, acc: 0.7042
loss: 0.7229, acc: 0.7066
loss: 0.7112, acc: 0.7100
loss: 0.7000, acc: 0.7152
loss: 0.6962, acc: 0.7165
> val_acc: 0.8482, val_f1: 0.7590
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8482_f1_0.759_230828-0510.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.3434, acc: 0.9062
loss: 0.4413, acc: 0.8036
loss: 0.4365, acc: 0.8255
loss: 0.4658, acc: 0.8199
loss: 0.4789, acc: 0.8125
loss: 0.4747, acc: 0.8160
loss: 0.4795, acc: 0.8154
loss: 0.4719, acc: 0.8201
loss: 0.4828, acc: 0.8185
loss: 0.4719, acc: 0.8218
loss: 0.4611, acc: 0.8251
loss: 0.4520, acc: 0.8257
loss: 0.4513, acc: 0.8256
loss: 0.4562, acc: 0.8237
loss: 0.4539, acc: 0.8242
loss: 0.4513, acc: 0.8239
loss: 0.4451, acc: 0.8262
loss: 0.4420, acc: 0.8272
loss: 0.4391, acc: 0.8285
loss: 0.4322, acc: 0.8318
loss: 0.4289, acc: 0.8339
loss: 0.4304, acc: 0.8318
loss: 0.4268, acc: 0.8340
> val_acc: 0.8455, val_f1: 0.7438
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2806, acc: 0.8984
loss: 0.3113, acc: 0.8750
loss: 0.2935, acc: 0.8884
loss: 0.2986, acc: 0.8849
loss: 0.2915, acc: 0.8893
loss: 0.2870, acc: 0.8933
loss: 0.2955, acc: 0.8934
loss: 0.2841, acc: 0.8974
loss: 0.2816, acc: 0.8977
loss: 0.2774, acc: 0.8992
loss: 0.2757, acc: 0.9016
loss: 0.2688, acc: 0.9052
loss: 0.2649, acc: 0.9077
loss: 0.2685, acc: 0.9058
loss: 0.2647, acc: 0.9071
loss: 0.2634, acc: 0.9082
loss: 0.2573, acc: 0.9092
loss: 0.2606, acc: 0.9077
loss: 0.2579, acc: 0.9076
loss: 0.2633, acc: 0.9040
loss: 0.2700, acc: 0.9014
loss: 0.2680, acc: 0.9019
> val_acc: 0.8411, val_f1: 0.7327
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1882, acc: 0.9062
loss: 0.2675, acc: 0.8958
loss: 0.2096, acc: 0.9261
loss: 0.2287, acc: 0.9180
loss: 0.2146, acc: 0.9196
loss: 0.2198, acc: 0.9123
loss: 0.2128, acc: 0.9133
loss: 0.2043, acc: 0.9193
loss: 0.2015, acc: 0.9215
loss: 0.1969, acc: 0.9219
loss: 0.1967, acc: 0.9228
loss: 0.1926, acc: 0.9247
loss: 0.1954, acc: 0.9232
loss: 0.1958, acc: 0.9233
loss: 0.1942, acc: 0.9234
loss: 0.1953, acc: 0.9235
loss: 0.1952, acc: 0.9240
loss: 0.1967, acc: 0.9230
loss: 0.1994, acc: 0.9220
loss: 0.1979, acc: 0.9219
loss: 0.1956, acc: 0.9220
loss: 0.1939, acc: 0.9233
loss: 0.1947, acc: 0.9231
> val_acc: 0.8438, val_f1: 0.7508
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1517, acc: 0.9688
loss: 0.1689, acc: 0.9570
loss: 0.1756, acc: 0.9447
loss: 0.1621, acc: 0.9531
loss: 0.1551, acc: 0.9538
loss: 0.1524, acc: 0.9520
loss: 0.1477, acc: 0.9517
loss: 0.1420, acc: 0.9539
loss: 0.1443, acc: 0.9542
loss: 0.1505, acc: 0.9531
loss: 0.1461, acc: 0.9546
loss: 0.1443, acc: 0.9542
loss: 0.1408, acc: 0.9534
loss: 0.1413, acc: 0.9531
loss: 0.1425, acc: 0.9538
loss: 0.1418, acc: 0.9543
loss: 0.1388, acc: 0.9556
loss: 0.1392, acc: 0.9556
loss: 0.1366, acc: 0.9567
loss: 0.1383, acc: 0.9563
loss: 0.1398, acc: 0.9551
loss: 0.1397, acc: 0.9546
loss: 0.1392, acc: 0.9545
> val_acc: 0.8580, val_f1: 0.7800
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.858_f1_0.78_230828-0513.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0797, acc: 0.9688
loss: 0.0933, acc: 0.9688
loss: 0.0878, acc: 0.9688
loss: 0.0773, acc: 0.9719
loss: 0.0740, acc: 0.9738
loss: 0.0673, acc: 0.9760
loss: 0.0681, acc: 0.9759
loss: 0.0757, acc: 0.9742
loss: 0.0737, acc: 0.9757
loss: 0.0770, acc: 0.9731
loss: 0.0763, acc: 0.9739
loss: 0.0783, acc: 0.9745
loss: 0.0777, acc: 0.9745
loss: 0.0836, acc: 0.9732
loss: 0.0824, acc: 0.9733
loss: 0.0862, acc: 0.9715
loss: 0.0879, acc: 0.9713
loss: 0.0892, acc: 0.9705
loss: 0.0933, acc: 0.9697
loss: 0.0967, acc: 0.9678
loss: 0.0960, acc: 0.9679
loss: 0.0973, acc: 0.9673
> val_acc: 0.8688, val_f1: 0.8016
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8688_f1_0.8016_230828-0513.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0561, acc: 0.9844
loss: 0.1212, acc: 0.9643
loss: 0.0931, acc: 0.9740
loss: 0.0917, acc: 0.9743
loss: 0.0833, acc: 0.9773
loss: 0.0828, acc: 0.9769
loss: 0.0869, acc: 0.9736
loss: 0.0988, acc: 0.9671
loss: 0.1062, acc: 0.9643
loss: 0.1165, acc: 0.9614
loss: 0.1183, acc: 0.9603
loss: 0.1159, acc: 0.9611
loss: 0.1159, acc: 0.9607
loss: 0.1141, acc: 0.9613
loss: 0.1118, acc: 0.9627
loss: 0.1129, acc: 0.9631
loss: 0.1117, acc: 0.9634
loss: 0.1071, acc: 0.9652
loss: 0.1021, acc: 0.9667
loss: 0.1031, acc: 0.9662
loss: 0.1044, acc: 0.9660
loss: 0.1074, acc: 0.9641
loss: 0.1059, acc: 0.9648
> val_acc: 0.8688, val_f1: 0.8004
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0337, acc: 1.0000
loss: 0.0298, acc: 0.9931
loss: 0.0402, acc: 0.9911
loss: 0.0445, acc: 0.9901
loss: 0.0481, acc: 0.9896
loss: 0.0473, acc: 0.9892
loss: 0.0518, acc: 0.9862
loss: 0.0520, acc: 0.9864
loss: 0.0501, acc: 0.9865
loss: 0.0562, acc: 0.9853
loss: 0.0572, acc: 0.9855
loss: 0.0598, acc: 0.9846
loss: 0.0687, acc: 0.9810
loss: 0.0693, acc: 0.9810
loss: 0.0694, acc: 0.9806
loss: 0.0717, acc: 0.9794
loss: 0.0726, acc: 0.9788
loss: 0.0701, acc: 0.9796
loss: 0.0685, acc: 0.9797
loss: 0.0692, acc: 0.9795
loss: 0.0676, acc: 0.9799
loss: 0.0704, acc: 0.9791
> val_acc: 0.8714, val_f1: 0.8077
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8714_f1_0.8077_230828-0515.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0325, acc: 1.0000
loss: 0.0328, acc: 0.9948
loss: 0.0271, acc: 0.9943
loss: 0.0250, acc: 0.9922
loss: 0.0297, acc: 0.9881
loss: 0.0366, acc: 0.9856
loss: 0.0341, acc: 0.9869
loss: 0.0390, acc: 0.9852
loss: 0.0354, acc: 0.9870
loss: 0.0337, acc: 0.9878
loss: 0.0377, acc: 0.9859
loss: 0.0390, acc: 0.9860
loss: 0.0423, acc: 0.9846
loss: 0.0516, acc: 0.9830
loss: 0.0555, acc: 0.9811
loss: 0.0580, acc: 0.9803
loss: 0.0628, acc: 0.9792
loss: 0.0620, acc: 0.9793
loss: 0.0609, acc: 0.9791
loss: 0.0601, acc: 0.9795
loss: 0.0601, acc: 0.9796
loss: 0.0592, acc: 0.9802
loss: 0.0591, acc: 0.9806
> val_acc: 0.8661, val_f1: 0.7989
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0334, acc: 0.9896
loss: 0.0437, acc: 0.9883
loss: 0.0438, acc: 0.9904
loss: 0.0479, acc: 0.9896
loss: 0.0522, acc: 0.9864
loss: 0.0488, acc: 0.9877
loss: 0.0440, acc: 0.9886
loss: 0.0392, acc: 0.9901
loss: 0.0370, acc: 0.9906
loss: 0.0352, acc: 0.9909
loss: 0.0444, acc: 0.9894
loss: 0.0451, acc: 0.9881
loss: 0.0467, acc: 0.9871
loss: 0.0460, acc: 0.9871
loss: 0.0479, acc: 0.9859
loss: 0.0511, acc: 0.9852
loss: 0.0516, acc: 0.9849
loss: 0.0510, acc: 0.9854
loss: 0.0499, acc: 0.9852
loss: 0.0486, acc: 0.9857
loss: 0.0469, acc: 0.9863
loss: 0.0467, acc: 0.9864
loss: 0.0483, acc: 0.9856
> val_acc: 0.8670, val_f1: 0.7942
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0354, acc: 0.9938
loss: 0.0374, acc: 0.9906
loss: 0.0392, acc: 0.9875
loss: 0.0439, acc: 0.9891
loss: 0.0377, acc: 0.9912
loss: 0.0397, acc: 0.9896
loss: 0.0399, acc: 0.9884
loss: 0.0440, acc: 0.9867
loss: 0.0428, acc: 0.9875
loss: 0.0439, acc: 0.9869
loss: 0.0491, acc: 0.9852
loss: 0.0504, acc: 0.9849
loss: 0.0508, acc: 0.9851
loss: 0.0507, acc: 0.9844
loss: 0.0554, acc: 0.9833
loss: 0.0527, acc: 0.9844
loss: 0.0542, acc: 0.9846
loss: 0.0558, acc: 0.9833
loss: 0.0560, acc: 0.9829
loss: 0.0547, acc: 0.9834
loss: 0.0601, acc: 0.9818
loss: 0.0592, acc: 0.9818
> val_acc: 0.8554, val_f1: 0.7926
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1320, acc: 0.9688
loss: 0.1029, acc: 0.9732
loss: 0.0675, acc: 0.9818
loss: 0.0629, acc: 0.9816
loss: 0.0544, acc: 0.9830
loss: 0.0528, acc: 0.9826
loss: 0.0486, acc: 0.9844
loss: 0.0471, acc: 0.9848
loss: 0.0430, acc: 0.9859
loss: 0.0559, acc: 0.9807
loss: 0.0523, acc: 0.9820
loss: 0.0502, acc: 0.9825
loss: 0.0468, acc: 0.9839
loss: 0.0557, acc: 0.9823
loss: 0.0546, acc: 0.9822
loss: 0.0525, acc: 0.9830
loss: 0.0511, acc: 0.9832
loss: 0.0520, acc: 0.9828
loss: 0.0550, acc: 0.9823
loss: 0.0575, acc: 0.9813
loss: 0.0593, acc: 0.9804
loss: 0.0586, acc: 0.9804
loss: 0.0568, acc: 0.9810
> val_acc: 0.8536, val_f1: 0.7786
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.0575, acc: 0.9844
loss: 0.0603, acc: 0.9757
loss: 0.0445, acc: 0.9821
loss: 0.0396, acc: 0.9852
loss: 0.0357, acc: 0.9870
loss: 0.0326, acc: 0.9892
loss: 0.0406, acc: 0.9881
loss: 0.0434, acc: 0.9872
loss: 0.0427, acc: 0.9865
loss: 0.0462, acc: 0.9853
loss: 0.0533, acc: 0.9821
loss: 0.0501, acc: 0.9836
loss: 0.0505, acc: 0.9834
loss: 0.0487, acc: 0.9841
loss: 0.0489, acc: 0.9840
loss: 0.0471, acc: 0.9846
loss: 0.0461, acc: 0.9847
loss: 0.0480, acc: 0.9838
loss: 0.0465, acc: 0.9844
loss: 0.0472, acc: 0.9848
loss: 0.0464, acc: 0.9853
loss: 0.0469, acc: 0.9848
> val_acc: 0.8652, val_f1: 0.7915
>> early stop.
>> test_acc: 0.8714, test_f1: 0.8077
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2569167360
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0283, acc: 0.5375
loss: 0.9638, acc: 0.6031
loss: 0.9793, acc: 0.5854
loss: 0.9779, acc: 0.5828
loss: 0.9672, acc: 0.5787
loss: 0.9348, acc: 0.5802
loss: 0.9177, acc: 0.5982
loss: 0.8916, acc: 0.6109
loss: 0.8751, acc: 0.6222
loss: 0.8559, acc: 0.6325
loss: 0.8298, acc: 0.6460
loss: 0.8140, acc: 0.6562
loss: 0.8049, acc: 0.6615
loss: 0.7990, acc: 0.6656
loss: 0.7904, acc: 0.6679
loss: 0.7859, acc: 0.6711
loss: 0.7792, acc: 0.6743
loss: 0.7703, acc: 0.6774
loss: 0.7593, acc: 0.6849
loss: 0.7473, acc: 0.6903
loss: 0.7419, acc: 0.6923
loss: 0.7325, acc: 0.6974
> val_acc: 0.8045, val_f1: 0.6804
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8045_f1_0.6804_230828-0519.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4820, acc: 0.8281
loss: 0.4241, acc: 0.8348
loss: 0.4858, acc: 0.8151
loss: 0.5013, acc: 0.8070
loss: 0.5062, acc: 0.8082
loss: 0.4967, acc: 0.8079
loss: 0.4971, acc: 0.8086
loss: 0.4957, acc: 0.8074
loss: 0.4868, acc: 0.8103
loss: 0.4848, acc: 0.8105
loss: 0.4758, acc: 0.8137
loss: 0.4734, acc: 0.8158
loss: 0.4725, acc: 0.8175
loss: 0.4742, acc: 0.8162
loss: 0.4735, acc: 0.8164
loss: 0.4709, acc: 0.8166
loss: 0.4649, acc: 0.8201
loss: 0.4585, acc: 0.8244
loss: 0.4542, acc: 0.8251
loss: 0.4500, acc: 0.8254
loss: 0.4543, acc: 0.8223
loss: 0.4526, acc: 0.8218
loss: 0.4489, acc: 0.8239
> val_acc: 0.8375, val_f1: 0.7248
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.8375_f1_0.7248_230828-0520.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2434, acc: 0.8984
loss: 0.2758, acc: 0.8958
loss: 0.2767, acc: 0.8951
loss: 0.2872, acc: 0.8947
loss: 0.2807, acc: 0.8958
loss: 0.2940, acc: 0.8912
loss: 0.2881, acc: 0.8943
loss: 0.2959, acc: 0.8918
loss: 0.2980, acc: 0.8892
loss: 0.3016, acc: 0.8865
loss: 0.2922, acc: 0.8900
loss: 0.2924, acc: 0.8904
loss: 0.2900, acc: 0.8911
loss: 0.2890, acc: 0.8895
loss: 0.2945, acc: 0.8868
loss: 0.2931, acc: 0.8877
loss: 0.2939, acc: 0.8876
loss: 0.2904, acc: 0.8894
loss: 0.2912, acc: 0.8886
loss: 0.2936, acc: 0.8879
loss: 0.2992, acc: 0.8852
loss: 0.2984, acc: 0.8862
> val_acc: 0.8830, val_f1: 0.8253
>> saved: /media/b115/Backup/NLP/roberta/rest14/acc_0.883_f1_0.8253_230828-0521.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2607, acc: 0.9062
loss: 0.2199, acc: 0.9323
loss: 0.2096, acc: 0.9261
loss: 0.2163, acc: 0.9238
loss: 0.2012, acc: 0.9301
loss: 0.2062, acc: 0.9303
loss: 0.2100, acc: 0.9274
loss: 0.1986, acc: 0.9306
loss: 0.1982, acc: 0.9291
loss: 0.2140, acc: 0.9239
loss: 0.2140, acc: 0.9234
loss: 0.2092, acc: 0.9263
loss: 0.2040, acc: 0.9283
loss: 0.1981, acc: 0.9304
loss: 0.1983, acc: 0.9309
loss: 0.1993, acc: 0.9289
loss: 0.2035, acc: 0.9267
loss: 0.2034, acc: 0.9262
loss: 0.2058, acc: 0.9248
loss: 0.2044, acc: 0.9245
loss: 0.2026, acc: 0.9248
loss: 0.2051, acc: 0.9236
loss: 0.2042, acc: 0.9245
> val_acc: 0.8759, val_f1: 0.8093
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0986, acc: 0.9688
loss: 0.1681, acc: 0.9414
loss: 0.1551, acc: 0.9447
loss: 0.1422, acc: 0.9514
loss: 0.1361, acc: 0.9552
loss: 0.1277, acc: 0.9554
loss: 0.1291, acc: 0.9517
loss: 0.1404, acc: 0.9515
loss: 0.1490, acc: 0.9506
loss: 0.1479, acc: 0.9499
loss: 0.1465, acc: 0.9499
loss: 0.1486, acc: 0.9504
loss: 0.1456, acc: 0.9514
loss: 0.1427, acc: 0.9517
loss: 0.1480, acc: 0.9491
loss: 0.1480, acc: 0.9487
loss: 0.1519, acc: 0.9469
loss: 0.1489, acc: 0.9485
loss: 0.1505, acc: 0.9489
loss: 0.1499, acc: 0.9487
loss: 0.1532, acc: 0.9478
loss: 0.1514, acc: 0.9485
loss: 0.1513, acc: 0.9484
> val_acc: 0.8598, val_f1: 0.7873
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.1034, acc: 0.9750
loss: 0.0955, acc: 0.9750
loss: 0.0860, acc: 0.9729
loss: 0.0816, acc: 0.9719
loss: 0.0888, acc: 0.9650
loss: 0.0828, acc: 0.9667
loss: 0.0837, acc: 0.9679
loss: 0.0892, acc: 0.9664
loss: 0.0871, acc: 0.9688
loss: 0.0859, acc: 0.9688
loss: 0.0881, acc: 0.9688
loss: 0.0866, acc: 0.9693
loss: 0.0823, acc: 0.9712
loss: 0.0882, acc: 0.9692
loss: 0.0856, acc: 0.9700
loss: 0.0847, acc: 0.9703
loss: 0.0842, acc: 0.9702
loss: 0.0880, acc: 0.9684
loss: 0.0930, acc: 0.9664
loss: 0.0971, acc: 0.9647
loss: 0.0977, acc: 0.9646
loss: 0.1005, acc: 0.9634
> val_acc: 0.8670, val_f1: 0.7964
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0397, acc: 1.0000
loss: 0.1193, acc: 0.9688
loss: 0.1017, acc: 0.9740
loss: 0.0957, acc: 0.9761
loss: 0.0830, acc: 0.9773
loss: 0.0900, acc: 0.9745
loss: 0.0848, acc: 0.9736
loss: 0.0834, acc: 0.9755
loss: 0.0866, acc: 0.9740
loss: 0.0836, acc: 0.9747
loss: 0.0866, acc: 0.9724
loss: 0.0832, acc: 0.9731
loss: 0.0816, acc: 0.9733
loss: 0.0902, acc: 0.9711
loss: 0.0917, acc: 0.9701
loss: 0.0919, acc: 0.9704
loss: 0.0921, acc: 0.9707
loss: 0.0908, acc: 0.9716
loss: 0.0882, acc: 0.9721
loss: 0.0888, acc: 0.9716
loss: 0.0885, acc: 0.9712
loss: 0.0868, acc: 0.9723
loss: 0.0887, acc: 0.9718
> val_acc: 0.8634, val_f1: 0.7969
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0415, acc: 0.9844
loss: 0.0339, acc: 0.9896
loss: 0.0493, acc: 0.9844
loss: 0.0560, acc: 0.9819
loss: 0.0602, acc: 0.9818
loss: 0.0542, acc: 0.9838
loss: 0.0804, acc: 0.9779
loss: 0.0772, acc: 0.9784
loss: 0.0802, acc: 0.9773
loss: 0.0838, acc: 0.9770
loss: 0.0792, acc: 0.9774
loss: 0.0737, acc: 0.9793
loss: 0.0749, acc: 0.9790
loss: 0.0743, acc: 0.9787
loss: 0.0732, acc: 0.9785
loss: 0.0772, acc: 0.9755
loss: 0.0790, acc: 0.9751
loss: 0.0800, acc: 0.9751
loss: 0.0790, acc: 0.9754
loss: 0.0805, acc: 0.9751
loss: 0.0806, acc: 0.9739
loss: 0.0806, acc: 0.9736
> val_acc: 0.8580, val_f1: 0.7762
>> early stop.
>> test_acc: 0.8830, test_f1: 0.8253
>> test_acc: 0.8768, test_f1: 0.8163
>> test_acc: 0.8714, test_f1: 0.8077
>> test_acc: 0.8830, test_f1: 0.8253

>> avg_test_acc: 0.8771, avg_test_f1: 0.8164
>> max_test_acc: 0.8830, max_test_f1: 0.8253
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 441448448
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0811, acc: 0.5938
loss: 0.9937, acc: 0.6125
loss: 0.9702, acc: 0.6125
loss: 0.9542, acc: 0.6078
loss: 0.9381, acc: 0.6125
loss: 0.9098, acc: 0.6260
loss: 0.8848, acc: 0.6384
loss: 0.8606, acc: 0.6477
loss: 0.8483, acc: 0.6556
loss: 0.8360, acc: 0.6606
loss: 0.8334, acc: 0.6619
loss: 0.8202, acc: 0.6656
loss: 0.7942, acc: 0.6769
loss: 0.7924, acc: 0.6768
loss: 0.7820, acc: 0.6808
loss: 0.7823, acc: 0.6797
loss: 0.7796, acc: 0.6805
loss: 0.7724, acc: 0.6823
loss: 0.7711, acc: 0.6839
loss: 0.7657, acc: 0.6856
loss: 0.7625, acc: 0.6854
loss: 0.7518, acc: 0.6906
> val_acc: 0.7911, val_f1: 0.6377
>> saved: peft/bert_lora/rest14//acc_0.7911_f1_0.6377_230828-0525
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5734, acc: 0.8125
loss: 0.5237, acc: 0.7991
loss: 0.5130, acc: 0.8151
loss: 0.5702, acc: 0.7739
loss: 0.5616, acc: 0.7741
loss: 0.5794, acc: 0.7743
loss: 0.5764, acc: 0.7725
loss: 0.5734, acc: 0.7770
loss: 0.5683, acc: 0.7783
loss: 0.5718, acc: 0.7753
loss: 0.5756, acc: 0.7710
loss: 0.5761, acc: 0.7708
loss: 0.5729, acc: 0.7712
loss: 0.5760, acc: 0.7710
loss: 0.5769, acc: 0.7700
loss: 0.5787, acc: 0.7691
loss: 0.5797, acc: 0.7683
loss: 0.5769, acc: 0.7687
loss: 0.5742, acc: 0.7683
loss: 0.5701, acc: 0.7703
loss: 0.5663, acc: 0.7693
loss: 0.5672, acc: 0.7690
loss: 0.5655, acc: 0.7687
> val_acc: 0.8241, val_f1: 0.7064
>> saved: peft/bert_lora/rest14//acc_0.8241_f1_0.7064_230828-0526
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4914, acc: 0.7891
loss: 0.4240, acc: 0.8194
loss: 0.3971, acc: 0.8371
loss: 0.4045, acc: 0.8405
loss: 0.4111, acc: 0.8385
loss: 0.4339, acc: 0.8157
loss: 0.4298, acc: 0.8171
loss: 0.4194, acc: 0.8189
loss: 0.4257, acc: 0.8175
loss: 0.4235, acc: 0.8221
loss: 0.4223, acc: 0.8235
loss: 0.4199, acc: 0.8247
loss: 0.4250, acc: 0.8252
loss: 0.4239, acc: 0.8243
loss: 0.4208, acc: 0.8252
loss: 0.4269, acc: 0.8236
loss: 0.4261, acc: 0.8244
loss: 0.4321, acc: 0.8220
loss: 0.4351, acc: 0.8245
loss: 0.4319, acc: 0.8264
loss: 0.4387, acc: 0.8215
loss: 0.4367, acc: 0.8222
> val_acc: 0.8063, val_f1: 0.7009
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3394, acc: 0.8750
loss: 0.3651, acc: 0.8542
loss: 0.3139, acc: 0.8778
loss: 0.3059, acc: 0.8809
loss: 0.3523, acc: 0.8676
loss: 0.3527, acc: 0.8702
loss: 0.3706, acc: 0.8669
loss: 0.3720, acc: 0.8637
loss: 0.3837, acc: 0.8544
loss: 0.3949, acc: 0.8492
loss: 0.3825, acc: 0.8542
loss: 0.3825, acc: 0.8532
loss: 0.3842, acc: 0.8519
loss: 0.3887, acc: 0.8485
loss: 0.3867, acc: 0.8486
loss: 0.3914, acc: 0.8450
loss: 0.3866, acc: 0.8457
loss: 0.3824, acc: 0.8481
loss: 0.3826, acc: 0.8489
loss: 0.3829, acc: 0.8473
loss: 0.3870, acc: 0.8456
loss: 0.3901, acc: 0.8440
loss: 0.3893, acc: 0.8435
> val_acc: 0.8482, val_f1: 0.7724
>> saved: peft/bert_lora/rest14//acc_0.8482_f1_0.7724_230828-0527
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2363, acc: 0.9271
loss: 0.2828, acc: 0.9023
loss: 0.2611, acc: 0.9014
loss: 0.2888, acc: 0.8906
loss: 0.2868, acc: 0.8886
loss: 0.2877, acc: 0.8895
loss: 0.2883, acc: 0.8911
loss: 0.2836, acc: 0.8898
loss: 0.2724, acc: 0.8939
loss: 0.2700, acc: 0.8952
loss: 0.2622, acc: 0.8980
loss: 0.2648, acc: 0.8971
loss: 0.2621, acc: 0.8973
loss: 0.2677, acc: 0.8980
loss: 0.2777, acc: 0.8943
loss: 0.2865, acc: 0.8890
loss: 0.2871, acc: 0.8886
loss: 0.2958, acc: 0.8853
loss: 0.3037, acc: 0.8837
loss: 0.3054, acc: 0.8823
loss: 0.3147, acc: 0.8771
loss: 0.3145, acc: 0.8764
loss: 0.3164, acc: 0.8767
> val_acc: 0.8464, val_f1: 0.7707
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2674, acc: 0.9125
loss: 0.2715, acc: 0.8906
loss: 0.2630, acc: 0.9083
loss: 0.2490, acc: 0.9141
loss: 0.2729, acc: 0.9025
loss: 0.2837, acc: 0.8958
loss: 0.2924, acc: 0.8902
loss: 0.2895, acc: 0.8930
loss: 0.2813, acc: 0.8958
loss: 0.2875, acc: 0.8938
loss: 0.2888, acc: 0.8926
loss: 0.2875, acc: 0.8927
loss: 0.2913, acc: 0.8885
loss: 0.2941, acc: 0.8862
loss: 0.2944, acc: 0.8862
loss: 0.3005, acc: 0.8840
loss: 0.2992, acc: 0.8838
loss: 0.2982, acc: 0.8847
loss: 0.2973, acc: 0.8849
loss: 0.2979, acc: 0.8847
loss: 0.2980, acc: 0.8857
loss: 0.2951, acc: 0.8861
> val_acc: 0.8295, val_f1: 0.7202
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2206, acc: 0.8906
loss: 0.1852, acc: 0.9152
loss: 0.1760, acc: 0.9297
loss: 0.2234, acc: 0.9210
loss: 0.2168, acc: 0.9247
loss: 0.2086, acc: 0.9282
loss: 0.2061, acc: 0.9287
loss: 0.1943, acc: 0.9333
loss: 0.2001, acc: 0.9315
loss: 0.2007, acc: 0.9275
loss: 0.2016, acc: 0.9243
loss: 0.2068, acc: 0.9232
loss: 0.2160, acc: 0.9199
loss: 0.2156, acc: 0.9202
loss: 0.2192, acc: 0.9206
loss: 0.2186, acc: 0.9209
loss: 0.2154, acc: 0.9211
loss: 0.2187, acc: 0.9199
loss: 0.2232, acc: 0.9181
loss: 0.2216, acc: 0.9185
loss: 0.2230, acc: 0.9176
loss: 0.2221, acc: 0.9185
loss: 0.2248, acc: 0.9169
> val_acc: 0.8259, val_f1: 0.7275
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2385, acc: 0.8984
loss: 0.2216, acc: 0.9062
loss: 0.2710, acc: 0.8929
loss: 0.2510, acc: 0.9046
loss: 0.2404, acc: 0.9062
loss: 0.2300, acc: 0.9073
loss: 0.2218, acc: 0.9108
loss: 0.2192, acc: 0.9111
loss: 0.2185, acc: 0.9134
loss: 0.2174, acc: 0.9139
loss: 0.2210, acc: 0.9132
loss: 0.2179, acc: 0.9153
loss: 0.2258, acc: 0.9111
loss: 0.2280, acc: 0.9099
loss: 0.2363, acc: 0.9079
loss: 0.2312, acc: 0.9106
loss: 0.2279, acc: 0.9118
loss: 0.2313, acc: 0.9119
loss: 0.2288, acc: 0.9122
loss: 0.2299, acc: 0.9116
loss: 0.2308, acc: 0.9117
loss: 0.2285, acc: 0.9114
> val_acc: 0.8357, val_f1: 0.7471
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1482, acc: 0.9688
loss: 0.2299, acc: 0.9167
loss: 0.1770, acc: 0.9403
loss: 0.1729, acc: 0.9375
loss: 0.1770, acc: 0.9345
loss: 0.1649, acc: 0.9375
loss: 0.1642, acc: 0.9365
loss: 0.1717, acc: 0.9349
loss: 0.1712, acc: 0.9345
loss: 0.1742, acc: 0.9341
loss: 0.1801, acc: 0.9326
loss: 0.1816, acc: 0.9325
loss: 0.1882, acc: 0.9278
loss: 0.1906, acc: 0.9280
loss: 0.1872, acc: 0.9291
loss: 0.1906, acc: 0.9268
loss: 0.1876, acc: 0.9279
loss: 0.1882, acc: 0.9277
loss: 0.1977, acc: 0.9251
loss: 0.1937, acc: 0.9258
loss: 0.1957, acc: 0.9251
loss: 0.1968, acc: 0.9251
loss: 0.1949, acc: 0.9260
> val_acc: 0.8464, val_f1: 0.7722
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8482, test_f1: 0.7724
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 484357632
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0990, acc: 0.4562
loss: 1.0304, acc: 0.5281
loss: 1.0257, acc: 0.5292
loss: 0.9957, acc: 0.5516
loss: 0.9578, acc: 0.5625
loss: 0.9397, acc: 0.5833
loss: 0.9282, acc: 0.5946
loss: 0.9043, acc: 0.6094
loss: 0.8742, acc: 0.6222
loss: 0.8567, acc: 0.6300
loss: 0.8416, acc: 0.6369
loss: 0.8285, acc: 0.6432
loss: 0.8257, acc: 0.6447
loss: 0.8112, acc: 0.6527
loss: 0.8068, acc: 0.6521
loss: 0.8007, acc: 0.6535
loss: 0.7907, acc: 0.6577
loss: 0.7865, acc: 0.6611
loss: 0.7801, acc: 0.6668
loss: 0.7749, acc: 0.6687
loss: 0.7643, acc: 0.6726
loss: 0.7548, acc: 0.6759
> val_acc: 0.7652, val_f1: 0.6057
>> saved: peft/bert_lora/rest14//acc_0.7652_f1_0.6057_230828-0530
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4849, acc: 0.8281
loss: 0.5663, acc: 0.7500
loss: 0.5724, acc: 0.7422
loss: 0.6004, acc: 0.7316
loss: 0.5932, acc: 0.7358
loss: 0.6017, acc: 0.7338
loss: 0.5972, acc: 0.7402
loss: 0.5900, acc: 0.7466
loss: 0.5931, acc: 0.7455
loss: 0.6069, acc: 0.7374
loss: 0.6000, acc: 0.7434
loss: 0.6115, acc: 0.7418
loss: 0.6120, acc: 0.7419
loss: 0.6038, acc: 0.7467
loss: 0.6058, acc: 0.7452
loss: 0.6051, acc: 0.7443
loss: 0.6073, acc: 0.7416
loss: 0.6059, acc: 0.7432
loss: 0.6126, acc: 0.7418
loss: 0.6101, acc: 0.7445
loss: 0.6090, acc: 0.7439
loss: 0.6102, acc: 0.7447
loss: 0.6110, acc: 0.7433
> val_acc: 0.8098, val_f1: 0.6839
>> saved: peft/bert_lora/rest14//acc_0.8098_f1_0.6839_230828-0531
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4700, acc: 0.8125
loss: 0.5184, acc: 0.7812
loss: 0.4813, acc: 0.7969
loss: 0.4629, acc: 0.8059
loss: 0.4612, acc: 0.8125
loss: 0.4595, acc: 0.8125
loss: 0.4655, acc: 0.8116
loss: 0.4625, acc: 0.8117
loss: 0.4746, acc: 0.8068
loss: 0.4783, acc: 0.8055
loss: 0.4722, acc: 0.8050
loss: 0.4751, acc: 0.8035
loss: 0.4741, acc: 0.8022
loss: 0.4688, acc: 0.8048
loss: 0.4652, acc: 0.8062
loss: 0.4667, acc: 0.8042
loss: 0.4734, acc: 0.8010
loss: 0.4674, acc: 0.8044
loss: 0.4704, acc: 0.8009
loss: 0.4678, acc: 0.8011
loss: 0.4719, acc: 0.8011
loss: 0.4764, acc: 0.7990
> val_acc: 0.8036, val_f1: 0.6608
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.5386, acc: 0.7812
loss: 0.4104, acc: 0.8125
loss: 0.4920, acc: 0.7812
loss: 0.4613, acc: 0.7969
loss: 0.4235, acc: 0.8095
loss: 0.4065, acc: 0.8269
loss: 0.4065, acc: 0.8296
loss: 0.4071, acc: 0.8325
loss: 0.3990, acc: 0.8323
loss: 0.4030, acc: 0.8336
loss: 0.3970, acc: 0.8370
loss: 0.3898, acc: 0.8382
loss: 0.3907, acc: 0.8391
loss: 0.3991, acc: 0.8362
loss: 0.4024, acc: 0.8341
loss: 0.3964, acc: 0.8355
loss: 0.3951, acc: 0.8353
loss: 0.3902, acc: 0.8372
loss: 0.3849, acc: 0.8396
loss: 0.3882, acc: 0.8402
loss: 0.3872, acc: 0.8397
loss: 0.3896, acc: 0.8376
loss: 0.3907, acc: 0.8376
> val_acc: 0.8330, val_f1: 0.7535
>> saved: peft/bert_lora/rest14//acc_0.833_f1_0.7535_230828-0532
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3503, acc: 0.8646
loss: 0.3418, acc: 0.8516
loss: 0.3345, acc: 0.8582
loss: 0.3467, acc: 0.8542
loss: 0.3321, acc: 0.8573
loss: 0.3242, acc: 0.8638
loss: 0.3209, acc: 0.8655
loss: 0.3287, acc: 0.8618
loss: 0.3239, acc: 0.8656
loss: 0.3258, acc: 0.8652
loss: 0.3334, acc: 0.8620
loss: 0.3303, acc: 0.8626
loss: 0.3334, acc: 0.8616
loss: 0.3267, acc: 0.8640
loss: 0.3266, acc: 0.8643
loss: 0.3332, acc: 0.8638
loss: 0.3331, acc: 0.8645
loss: 0.3355, acc: 0.8636
loss: 0.3341, acc: 0.8639
loss: 0.3333, acc: 0.8638
loss: 0.3313, acc: 0.8644
loss: 0.3344, acc: 0.8655
loss: 0.3329, acc: 0.8667
> val_acc: 0.8420, val_f1: 0.7595
>> saved: peft/bert_lora/rest14//acc_0.842_f1_0.7595_230828-0532
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2608, acc: 0.8938
loss: 0.2484, acc: 0.9125
loss: 0.2525, acc: 0.9000
loss: 0.2225, acc: 0.9141
loss: 0.2395, acc: 0.9050
loss: 0.2584, acc: 0.8948
loss: 0.2602, acc: 0.8964
loss: 0.2692, acc: 0.8906
loss: 0.2738, acc: 0.8889
loss: 0.2923, acc: 0.8869
loss: 0.3042, acc: 0.8818
loss: 0.2989, acc: 0.8844
loss: 0.3006, acc: 0.8861
loss: 0.3022, acc: 0.8848
loss: 0.3046, acc: 0.8829
loss: 0.3031, acc: 0.8832
loss: 0.3032, acc: 0.8842
loss: 0.3063, acc: 0.8840
loss: 0.3102, acc: 0.8812
loss: 0.3136, acc: 0.8797
loss: 0.3099, acc: 0.8812
loss: 0.3113, acc: 0.8807
> val_acc: 0.8357, val_f1: 0.7373
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2325, acc: 0.9062
loss: 0.2277, acc: 0.9018
loss: 0.2113, acc: 0.9036
loss: 0.2259, acc: 0.9118
loss: 0.2460, acc: 0.8991
loss: 0.2571, acc: 0.8970
loss: 0.2678, acc: 0.8945
loss: 0.2661, acc: 0.8953
loss: 0.2607, acc: 0.8996
loss: 0.2593, acc: 0.9003
loss: 0.2576, acc: 0.8984
loss: 0.2577, acc: 0.8964
loss: 0.2575, acc: 0.8967
loss: 0.2561, acc: 0.8960
loss: 0.2586, acc: 0.8976
loss: 0.2603, acc: 0.8977
loss: 0.2593, acc: 0.8986
loss: 0.2607, acc: 0.8991
loss: 0.2644, acc: 0.8974
loss: 0.2632, acc: 0.8985
loss: 0.2647, acc: 0.8983
loss: 0.2689, acc: 0.8969
loss: 0.2757, acc: 0.8948
> val_acc: 0.8500, val_f1: 0.7717
>> saved: peft/bert_lora/rest14//acc_0.85_f1_0.7717_230828-0533
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1543, acc: 0.9609
loss: 0.1626, acc: 0.9479
loss: 0.1802, acc: 0.9375
loss: 0.1937, acc: 0.9342
loss: 0.1949, acc: 0.9336
loss: 0.1883, acc: 0.9343
loss: 0.2003, acc: 0.9256
loss: 0.1997, acc: 0.9263
loss: 0.2153, acc: 0.9233
loss: 0.2144, acc: 0.9209
loss: 0.2183, acc: 0.9190
loss: 0.2163, acc: 0.9195
loss: 0.2182, acc: 0.9170
loss: 0.2142, acc: 0.9180
loss: 0.2195, acc: 0.9164
loss: 0.2157, acc: 0.9173
loss: 0.2212, acc: 0.9144
loss: 0.2194, acc: 0.9154
loss: 0.2215, acc: 0.9149
loss: 0.2236, acc: 0.9138
loss: 0.2278, acc: 0.9120
loss: 0.2314, acc: 0.9100
> val_acc: 0.8357, val_f1: 0.7592
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0476, acc: 1.0000
loss: 0.1368, acc: 0.9427
loss: 0.1539, acc: 0.9347
loss: 0.1820, acc: 0.9199
loss: 0.1854, acc: 0.9241
loss: 0.1962, acc: 0.9255
loss: 0.1934, acc: 0.9264
loss: 0.1986, acc: 0.9253
loss: 0.2087, acc: 0.9215
loss: 0.2093, acc: 0.9192
loss: 0.2096, acc: 0.9197
loss: 0.2137, acc: 0.9180
loss: 0.2178, acc: 0.9160
loss: 0.2138, acc: 0.9181
loss: 0.2209, acc: 0.9146
loss: 0.2280, acc: 0.9124
loss: 0.2295, acc: 0.9097
loss: 0.2377, acc: 0.9052
loss: 0.2362, acc: 0.9056
loss: 0.2371, acc: 0.9062
loss: 0.2374, acc: 0.9062
loss: 0.2415, acc: 0.9042
loss: 0.2401, acc: 0.9054
> val_acc: 0.8304, val_f1: 0.7359
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2934, acc: 0.8646
loss: 0.2222, acc: 0.9062
loss: 0.2162, acc: 0.9087
loss: 0.1960, acc: 0.9201
loss: 0.1956, acc: 0.9171
loss: 0.2047, acc: 0.9141
loss: 0.1920, acc: 0.9195
loss: 0.1867, acc: 0.9235
loss: 0.1889, acc: 0.9230
loss: 0.1971, acc: 0.9206
loss: 0.2003, acc: 0.9198
loss: 0.1990, acc: 0.9197
loss: 0.2017, acc: 0.9191
loss: 0.2044, acc: 0.9187
loss: 0.2041, acc: 0.9178
loss: 0.2022, acc: 0.9199
loss: 0.2001, acc: 0.9209
loss: 0.2011, acc: 0.9215
loss: 0.2027, acc: 0.9217
loss: 0.2067, acc: 0.9193
loss: 0.2101, acc: 0.9184
loss: 0.2133, acc: 0.9170
loss: 0.2153, acc: 0.9163
> val_acc: 0.8295, val_f1: 0.7444
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.1654, acc: 0.9500
loss: 0.1721, acc: 0.9469
loss: 0.1710, acc: 0.9458
loss: 0.1765, acc: 0.9391
loss: 0.1768, acc: 0.9350
loss: 0.1705, acc: 0.9396
loss: 0.1745, acc: 0.9366
loss: 0.1749, acc: 0.9352
loss: 0.1845, acc: 0.9285
loss: 0.1928, acc: 0.9250
loss: 0.1917, acc: 0.9239
loss: 0.1968, acc: 0.9208
loss: 0.1977, acc: 0.9212
loss: 0.2039, acc: 0.9210
loss: 0.2012, acc: 0.9213
loss: 0.1996, acc: 0.9223
loss: 0.1981, acc: 0.9232
loss: 0.1957, acc: 0.9247
loss: 0.1995, acc: 0.9237
loss: 0.1974, acc: 0.9244
loss: 0.1974, acc: 0.9250
loss: 0.2003, acc: 0.9236
> val_acc: 0.8411, val_f1: 0.7577
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1477, acc: 0.9531
loss: 0.1327, acc: 0.9688
loss: 0.1191, acc: 0.9635
loss: 0.1373, acc: 0.9559
loss: 0.1585, acc: 0.9460
loss: 0.1567, acc: 0.9479
loss: 0.1642, acc: 0.9404
loss: 0.1550, acc: 0.9443
loss: 0.1588, acc: 0.9427
loss: 0.1639, acc: 0.9395
loss: 0.1587, acc: 0.9417
loss: 0.1561, acc: 0.9430
loss: 0.1574, acc: 0.9435
loss: 0.1591, acc: 0.9422
loss: 0.1627, acc: 0.9405
loss: 0.1620, acc: 0.9399
loss: 0.1622, acc: 0.9409
loss: 0.1642, acc: 0.9404
loss: 0.1614, acc: 0.9412
loss: 0.1649, acc: 0.9398
loss: 0.1663, acc: 0.9396
loss: 0.1669, acc: 0.9404
loss: 0.1683, acc: 0.9403
> val_acc: 0.8402, val_f1: 0.7635
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8500, test_f1: 0.7717
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 467580416
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0036, acc: 0.5938
loss: 1.0076, acc: 0.5563
loss: 0.9897, acc: 0.5750
loss: 0.9754, acc: 0.5750
loss: 0.9406, acc: 0.5925
loss: 0.9280, acc: 0.5979
loss: 0.9130, acc: 0.6062
loss: 0.8903, acc: 0.6156
loss: 0.8611, acc: 0.6292
loss: 0.8455, acc: 0.6356
loss: 0.8277, acc: 0.6415
loss: 0.8047, acc: 0.6516
loss: 0.8011, acc: 0.6572
loss: 0.7985, acc: 0.6576
loss: 0.7878, acc: 0.6637
loss: 0.7767, acc: 0.6703
loss: 0.7644, acc: 0.6765
loss: 0.7609, acc: 0.6778
loss: 0.7530, acc: 0.6813
loss: 0.7471, acc: 0.6850
loss: 0.7421, acc: 0.6875
loss: 0.7361, acc: 0.6895
> val_acc: 0.7768, val_f1: 0.5824
>> saved: peft/bert_lora/rest14//acc_0.7768_f1_0.5824_230828-0536
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4973, acc: 0.7812
loss: 0.5293, acc: 0.7679
loss: 0.5183, acc: 0.7891
loss: 0.5070, acc: 0.7923
loss: 0.5072, acc: 0.7940
loss: 0.5054, acc: 0.8009
loss: 0.5042, acc: 0.8027
loss: 0.5202, acc: 0.8007
loss: 0.5296, acc: 0.7984
loss: 0.5272, acc: 0.7965
loss: 0.5309, acc: 0.7939
loss: 0.5299, acc: 0.7928
loss: 0.5335, acc: 0.7868
loss: 0.5269, acc: 0.7892
loss: 0.5305, acc: 0.7878
loss: 0.5323, acc: 0.7873
loss: 0.5305, acc: 0.7873
loss: 0.5277, acc: 0.7881
loss: 0.5228, acc: 0.7901
loss: 0.5253, acc: 0.7867
loss: 0.5239, acc: 0.7877
loss: 0.5237, acc: 0.7865
loss: 0.5256, acc: 0.7849
> val_acc: 0.8295, val_f1: 0.7430
>> saved: peft/bert_lora/rest14//acc_0.8295_f1_0.743_230828-0537
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3660, acc: 0.8750
loss: 0.3620, acc: 0.8576
loss: 0.3491, acc: 0.8594
loss: 0.3658, acc: 0.8487
loss: 0.3768, acc: 0.8477
loss: 0.3943, acc: 0.8405
loss: 0.4052, acc: 0.8364
loss: 0.4015, acc: 0.8381
loss: 0.4107, acc: 0.8352
loss: 0.4111, acc: 0.8374
loss: 0.4190, acc: 0.8339
loss: 0.4245, acc: 0.8289
loss: 0.4285, acc: 0.8262
loss: 0.4271, acc: 0.8274
loss: 0.4240, acc: 0.8290
loss: 0.4179, acc: 0.8315
loss: 0.4207, acc: 0.8292
loss: 0.4207, acc: 0.8287
loss: 0.4223, acc: 0.8288
loss: 0.4227, acc: 0.8283
loss: 0.4186, acc: 0.8299
loss: 0.4193, acc: 0.8303
> val_acc: 0.8152, val_f1: 0.6920
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3184, acc: 0.8438
loss: 0.3405, acc: 0.8750
loss: 0.2950, acc: 0.8892
loss: 0.2693, acc: 0.9004
loss: 0.2766, acc: 0.8899
loss: 0.2960, acc: 0.8846
loss: 0.2999, acc: 0.8861
loss: 0.3119, acc: 0.8863
loss: 0.3217, acc: 0.8811
loss: 0.3217, acc: 0.8818
loss: 0.3212, acc: 0.8799
loss: 0.3222, acc: 0.8767
loss: 0.3304, acc: 0.8765
loss: 0.3365, acc: 0.8750
loss: 0.3369, acc: 0.8754
loss: 0.3378, acc: 0.8758
loss: 0.3356, acc: 0.8769
loss: 0.3372, acc: 0.8765
loss: 0.3367, acc: 0.8760
loss: 0.3373, acc: 0.8757
loss: 0.3393, acc: 0.8747
loss: 0.3398, acc: 0.8741
loss: 0.3417, acc: 0.8736
> val_acc: 0.8321, val_f1: 0.7401
>> saved: peft/bert_lora/rest14//acc_0.8321_f1_0.7401_230828-0538
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2278, acc: 0.8750
loss: 0.2167, acc: 0.9062
loss: 0.2282, acc: 0.9087
loss: 0.2161, acc: 0.9115
loss: 0.2273, acc: 0.9049
loss: 0.2508, acc: 0.9018
loss: 0.2437, acc: 0.9062
loss: 0.2421, acc: 0.9046
loss: 0.2427, acc: 0.9041
loss: 0.2453, acc: 0.9030
loss: 0.2513, acc: 0.9027
loss: 0.2570, acc: 0.9009
loss: 0.2611, acc: 0.8998
loss: 0.2666, acc: 0.8980
loss: 0.2713, acc: 0.8964
loss: 0.2754, acc: 0.8946
loss: 0.2758, acc: 0.8931
loss: 0.2789, acc: 0.8931
loss: 0.2820, acc: 0.8918
loss: 0.2853, acc: 0.8897
loss: 0.2817, acc: 0.8911
loss: 0.2876, acc: 0.8895
loss: 0.2877, acc: 0.8894
> val_acc: 0.8286, val_f1: 0.7463
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3362, acc: 0.8688
loss: 0.2784, acc: 0.8875
loss: 0.2706, acc: 0.8958
loss: 0.2454, acc: 0.9109
loss: 0.2224, acc: 0.9163
loss: 0.2264, acc: 0.9125
loss: 0.2364, acc: 0.9089
loss: 0.2383, acc: 0.9047
loss: 0.2412, acc: 0.9021
loss: 0.2378, acc: 0.9038
loss: 0.2377, acc: 0.9040
loss: 0.2472, acc: 0.8995
loss: 0.2466, acc: 0.9014
loss: 0.2469, acc: 0.9018
loss: 0.2478, acc: 0.9012
loss: 0.2588, acc: 0.8980
loss: 0.2620, acc: 0.8971
loss: 0.2680, acc: 0.8955
loss: 0.2638, acc: 0.8974
loss: 0.2656, acc: 0.8972
loss: 0.2690, acc: 0.8961
loss: 0.2698, acc: 0.8960
> val_acc: 0.8330, val_f1: 0.7585
>> saved: peft/bert_lora/rest14//acc_0.833_f1_0.7585_230828-0539
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.1813, acc: 0.9375
loss: 0.2337, acc: 0.9196
loss: 0.1837, acc: 0.9349
loss: 0.1931, acc: 0.9301
loss: 0.2068, acc: 0.9219
loss: 0.2197, acc: 0.9190
loss: 0.2170, acc: 0.9170
loss: 0.2233, acc: 0.9147
loss: 0.2282, acc: 0.9137
loss: 0.2265, acc: 0.9149
loss: 0.2323, acc: 0.9129
loss: 0.2382, acc: 0.9123
loss: 0.2367, acc: 0.9133
loss: 0.2370, acc: 0.9118
loss: 0.2345, acc: 0.9123
loss: 0.2348, acc: 0.9115
loss: 0.2371, acc: 0.9112
loss: 0.2434, acc: 0.9084
loss: 0.2422, acc: 0.9093
loss: 0.2409, acc: 0.9108
loss: 0.2403, acc: 0.9105
loss: 0.2406, acc: 0.9100
loss: 0.2432, acc: 0.9090
> val_acc: 0.8429, val_f1: 0.7610
>> saved: peft/bert_lora/rest14//acc_0.8429_f1_0.761_230828-0540
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.1378, acc: 0.9453
loss: 0.1471, acc: 0.9479
loss: 0.1481, acc: 0.9487
loss: 0.1845, acc: 0.9391
loss: 0.1762, acc: 0.9427
loss: 0.1743, acc: 0.9397
loss: 0.1845, acc: 0.9347
loss: 0.1801, acc: 0.9375
loss: 0.1763, acc: 0.9389
loss: 0.1821, acc: 0.9369
loss: 0.1807, acc: 0.9369
loss: 0.1856, acc: 0.9338
loss: 0.1921, acc: 0.9312
loss: 0.1869, acc: 0.9339
loss: 0.1855, acc: 0.9350
loss: 0.1856, acc: 0.9351
loss: 0.1915, acc: 0.9345
loss: 0.1943, acc: 0.9326
loss: 0.1979, acc: 0.9309
loss: 0.1989, acc: 0.9302
loss: 0.1990, acc: 0.9288
loss: 0.2025, acc: 0.9272
> val_acc: 0.8313, val_f1: 0.7541
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0954, acc: 1.0000
loss: 0.1707, acc: 0.9323
loss: 0.1548, acc: 0.9347
loss: 0.1253, acc: 0.9531
loss: 0.1371, acc: 0.9479
loss: 0.1441, acc: 0.9471
loss: 0.1425, acc: 0.9506
loss: 0.1592, acc: 0.9410
loss: 0.1544, acc: 0.9421
loss: 0.1520, acc: 0.9436
loss: 0.1561, acc: 0.9424
loss: 0.1551, acc: 0.9436
loss: 0.1590, acc: 0.9416
loss: 0.1580, acc: 0.9408
loss: 0.1555, acc: 0.9419
loss: 0.1586, acc: 0.9400
loss: 0.1575, acc: 0.9406
loss: 0.1588, acc: 0.9397
loss: 0.1597, acc: 0.9385
loss: 0.1654, acc: 0.9368
loss: 0.1697, acc: 0.9353
loss: 0.1714, acc: 0.9351
loss: 0.1712, acc: 0.9347
> val_acc: 0.8152, val_f1: 0.7301
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1479, acc: 0.9479
loss: 0.1541, acc: 0.9570
loss: 0.1634, acc: 0.9495
loss: 0.1607, acc: 0.9514
loss: 0.1611, acc: 0.9484
loss: 0.1500, acc: 0.9509
loss: 0.1506, acc: 0.9517
loss: 0.1515, acc: 0.9523
loss: 0.1491, acc: 0.9528
loss: 0.1489, acc: 0.9518
loss: 0.1556, acc: 0.9499
loss: 0.1531, acc: 0.9494
loss: 0.1551, acc: 0.9489
loss: 0.1510, acc: 0.9499
loss: 0.1484, acc: 0.9512
loss: 0.1500, acc: 0.9503
loss: 0.1604, acc: 0.9458
loss: 0.1612, acc: 0.9446
loss: 0.1592, acc: 0.9439
loss: 0.1580, acc: 0.9436
loss: 0.1616, acc: 0.9421
loss: 0.1622, acc: 0.9424
loss: 0.1639, acc: 0.9424
> val_acc: 0.8366, val_f1: 0.7528
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.1364, acc: 0.9500
loss: 0.1412, acc: 0.9437
loss: 0.1519, acc: 0.9417
loss: 0.1394, acc: 0.9484
loss: 0.1533, acc: 0.9437
loss: 0.1545, acc: 0.9437
loss: 0.1640, acc: 0.9411
loss: 0.1597, acc: 0.9414
loss: 0.1603, acc: 0.9396
loss: 0.1676, acc: 0.9387
loss: 0.1746, acc: 0.9381
loss: 0.1780, acc: 0.9365
loss: 0.1729, acc: 0.9380
loss: 0.1675, acc: 0.9402
loss: 0.1670, acc: 0.9413
loss: 0.1708, acc: 0.9406
loss: 0.1733, acc: 0.9401
loss: 0.1714, acc: 0.9413
loss: 0.1742, acc: 0.9401
loss: 0.1753, acc: 0.9406
loss: 0.1721, acc: 0.9420
loss: 0.1705, acc: 0.9420
> val_acc: 0.8357, val_f1: 0.7609
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1649, acc: 0.9531
loss: 0.1333, acc: 0.9420
loss: 0.1382, acc: 0.9401
loss: 0.1249, acc: 0.9485
loss: 0.1263, acc: 0.9432
loss: 0.1297, acc: 0.9421
loss: 0.1639, acc: 0.9307
loss: 0.1636, acc: 0.9307
loss: 0.1701, acc: 0.9308
loss: 0.1722, acc: 0.9295
loss: 0.1698, acc: 0.9333
loss: 0.1747, acc: 0.9315
loss: 0.1810, acc: 0.9289
loss: 0.1916, acc: 0.9272
loss: 0.1960, acc: 0.9253
loss: 0.1935, acc: 0.9274
loss: 0.1966, acc: 0.9264
loss: 0.1958, acc: 0.9264
loss: 0.1955, acc: 0.9276
loss: 0.1933, acc: 0.9285
loss: 0.1924, acc: 0.9286
loss: 0.1891, acc: 0.9302
loss: 0.1880, acc: 0.9305
> val_acc: 0.8223, val_f1: 0.7340
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8429, test_f1: 0.7610
>> test_acc: 0.8482, test_f1: 0.7724
>> test_acc: 0.8500, test_f1: 0.7717
>> test_acc: 0.8429, test_f1: 0.7610

>> avg_test_acc: 0.8470, avg_test_f1: 0.7684
>> max_test_acc: 0.8500, max_test_f1: 0.7724
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 504626176
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0532, acc: 0.5750
loss: 1.0208, acc: 0.5938
loss: 1.0080, acc: 0.5771
loss: 0.9386, acc: 0.6078
loss: 0.8903, acc: 0.6362
loss: 0.8488, acc: 0.6562
loss: 0.8377, acc: 0.6571
loss: 0.8243, acc: 0.6609
loss: 0.8110, acc: 0.6681
loss: 0.7914, acc: 0.6725
loss: 0.7778, acc: 0.6767
loss: 0.7679, acc: 0.6844
loss: 0.7578, acc: 0.6889
loss: 0.7449, acc: 0.6924
loss: 0.7445, acc: 0.6921
loss: 0.7326, acc: 0.6965
loss: 0.7200, acc: 0.7029
loss: 0.7139, acc: 0.7045
loss: 0.7105, acc: 0.7043
loss: 0.7012, acc: 0.7091
loss: 0.6961, acc: 0.7107
loss: 0.6905, acc: 0.7139
> val_acc: 0.8357, val_f1: 0.7307
>> saved: peft/roberta_lora/rest14//acc_0.8357_f1_0.7307_230828-0543
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6267, acc: 0.7656
loss: 0.6083, acc: 0.7634
loss: 0.5756, acc: 0.7656
loss: 0.5983, acc: 0.7610
loss: 0.5855, acc: 0.7685
loss: 0.5692, acc: 0.7720
loss: 0.5760, acc: 0.7744
loss: 0.5762, acc: 0.7736
loss: 0.5631, acc: 0.7783
loss: 0.5472, acc: 0.7872
loss: 0.5387, acc: 0.7915
loss: 0.5275, acc: 0.7933
loss: 0.5227, acc: 0.7979
loss: 0.5172, acc: 0.7999
loss: 0.5118, acc: 0.8012
loss: 0.5094, acc: 0.8003
loss: 0.5092, acc: 0.7995
loss: 0.5073, acc: 0.8003
loss: 0.5026, acc: 0.8020
loss: 0.4988, acc: 0.8025
loss: 0.4979, acc: 0.8036
loss: 0.4966, acc: 0.8032
loss: 0.4956, acc: 0.8022
> val_acc: 0.8179, val_f1: 0.6566
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4439, acc: 0.8203
loss: 0.4790, acc: 0.8021
loss: 0.4566, acc: 0.8237
loss: 0.4600, acc: 0.8141
loss: 0.4686, acc: 0.8086
loss: 0.4688, acc: 0.8114
loss: 0.4534, acc: 0.8217
loss: 0.4556, acc: 0.8237
loss: 0.4487, acc: 0.8281
loss: 0.4448, acc: 0.8278
loss: 0.4528, acc: 0.8241
loss: 0.4454, acc: 0.8263
loss: 0.4529, acc: 0.8208
loss: 0.4490, acc: 0.8225
loss: 0.4473, acc: 0.8214
loss: 0.4476, acc: 0.8204
loss: 0.4468, acc: 0.8222
loss: 0.4437, acc: 0.8234
loss: 0.4487, acc: 0.8215
loss: 0.4496, acc: 0.8204
loss: 0.4503, acc: 0.8209
loss: 0.4464, acc: 0.8222
> val_acc: 0.8464, val_f1: 0.7410
>> saved: peft/roberta_lora/rest14//acc_0.8464_f1_0.741_230828-0544
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4331, acc: 0.8125
loss: 0.4967, acc: 0.8021
loss: 0.4462, acc: 0.8210
loss: 0.4158, acc: 0.8281
loss: 0.3895, acc: 0.8438
loss: 0.3788, acc: 0.8534
loss: 0.3731, acc: 0.8569
loss: 0.3834, acc: 0.8524
loss: 0.3867, acc: 0.8506
loss: 0.3942, acc: 0.8485
loss: 0.4016, acc: 0.8487
loss: 0.3997, acc: 0.8477
loss: 0.4115, acc: 0.8427
loss: 0.4073, acc: 0.8428
loss: 0.4060, acc: 0.8433
loss: 0.4012, acc: 0.8446
loss: 0.3974, acc: 0.8465
loss: 0.3947, acc: 0.8470
loss: 0.3879, acc: 0.8486
loss: 0.3836, acc: 0.8503
loss: 0.3822, acc: 0.8496
loss: 0.3808, acc: 0.8496
loss: 0.3868, acc: 0.8460
> val_acc: 0.8482, val_f1: 0.7554
>> saved: peft/roberta_lora/rest14//acc_0.8482_f1_0.7554_230828-0544
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2849, acc: 0.8958
loss: 0.2710, acc: 0.9023
loss: 0.2708, acc: 0.8966
loss: 0.2997, acc: 0.8924
loss: 0.3544, acc: 0.8736
loss: 0.3648, acc: 0.8672
loss: 0.3638, acc: 0.8646
loss: 0.3707, acc: 0.8643
loss: 0.3662, acc: 0.8670
loss: 0.3693, acc: 0.8665
loss: 0.3656, acc: 0.8667
loss: 0.3605, acc: 0.8696
loss: 0.3536, acc: 0.8700
loss: 0.3572, acc: 0.8681
loss: 0.3583, acc: 0.8660
loss: 0.3614, acc: 0.8646
loss: 0.3690, acc: 0.8626
loss: 0.3668, acc: 0.8640
loss: 0.3667, acc: 0.8639
loss: 0.3657, acc: 0.8629
loss: 0.3580, acc: 0.8662
loss: 0.3582, acc: 0.8649
loss: 0.3671, acc: 0.8609
> val_acc: 0.8438, val_f1: 0.7470
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.4180, acc: 0.8375
loss: 0.3672, acc: 0.8594
loss: 0.3437, acc: 0.8604
loss: 0.3324, acc: 0.8719
loss: 0.3417, acc: 0.8700
loss: 0.3460, acc: 0.8604
loss: 0.3596, acc: 0.8562
loss: 0.3516, acc: 0.8570
loss: 0.3455, acc: 0.8562
loss: 0.3395, acc: 0.8612
loss: 0.3375, acc: 0.8653
loss: 0.3364, acc: 0.8661
loss: 0.3380, acc: 0.8663
loss: 0.3394, acc: 0.8634
loss: 0.3365, acc: 0.8638
loss: 0.3350, acc: 0.8645
loss: 0.3302, acc: 0.8662
loss: 0.3235, acc: 0.8691
loss: 0.3278, acc: 0.8658
loss: 0.3331, acc: 0.8625
loss: 0.3314, acc: 0.8637
loss: 0.3297, acc: 0.8651
> val_acc: 0.8598, val_f1: 0.8025
>> saved: peft/roberta_lora/rest14//acc_0.8598_f1_0.8025_230828-0545
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.3214, acc: 0.8594
loss: 0.2989, acc: 0.8661
loss: 0.2788, acc: 0.8724
loss: 0.2791, acc: 0.8860
loss: 0.2944, acc: 0.8821
loss: 0.2982, acc: 0.8808
loss: 0.2987, acc: 0.8828
loss: 0.3106, acc: 0.8809
loss: 0.2970, acc: 0.8862
loss: 0.2962, acc: 0.8856
loss: 0.2857, acc: 0.8888
loss: 0.2978, acc: 0.8854
loss: 0.2978, acc: 0.8861
loss: 0.3011, acc: 0.8867
loss: 0.2974, acc: 0.8880
loss: 0.2956, acc: 0.8896
loss: 0.2896, acc: 0.8914
loss: 0.2958, acc: 0.8879
loss: 0.2956, acc: 0.8872
loss: 0.2913, acc: 0.8901
loss: 0.2955, acc: 0.8885
loss: 0.2955, acc: 0.8879
loss: 0.2969, acc: 0.8876
> val_acc: 0.8688, val_f1: 0.7982
>> saved: peft/roberta_lora/rest14//acc_0.8688_f1_0.7982_230828-0546
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2209, acc: 0.9375
loss: 0.2521, acc: 0.9062
loss: 0.2255, acc: 0.9152
loss: 0.2398, acc: 0.9062
loss: 0.2447, acc: 0.9036
loss: 0.2613, acc: 0.8987
loss: 0.2483, acc: 0.9053
loss: 0.2381, acc: 0.9087
loss: 0.2271, acc: 0.9134
loss: 0.2215, acc: 0.9139
loss: 0.2260, acc: 0.9120
loss: 0.2225, acc: 0.9121
loss: 0.2245, acc: 0.9106
loss: 0.2300, acc: 0.9085
loss: 0.2315, acc: 0.9105
loss: 0.2308, acc: 0.9110
loss: 0.2336, acc: 0.9096
loss: 0.2409, acc: 0.9066
loss: 0.2501, acc: 0.9023
loss: 0.2529, acc: 0.9012
loss: 0.2580, acc: 0.8999
loss: 0.2567, acc: 0.8997
> val_acc: 0.8759, val_f1: 0.8105
>> saved: peft/roberta_lora/rest14//acc_0.8759_f1_0.8105_230828-0546
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2957, acc: 0.9062
loss: 0.2508, acc: 0.9010
loss: 0.2486, acc: 0.9091
loss: 0.2333, acc: 0.9043
loss: 0.2415, acc: 0.9062
loss: 0.2290, acc: 0.9135
loss: 0.2252, acc: 0.9143
loss: 0.2266, acc: 0.9149
loss: 0.2193, acc: 0.9184
loss: 0.2173, acc: 0.9192
loss: 0.2144, acc: 0.9173
loss: 0.2147, acc: 0.9146
loss: 0.2248, acc: 0.9114
loss: 0.2287, acc: 0.9077
loss: 0.2333, acc: 0.9062
loss: 0.2316, acc: 0.9075
loss: 0.2330, acc: 0.9062
loss: 0.2427, acc: 0.9030
loss: 0.2418, acc: 0.9049
loss: 0.2439, acc: 0.9033
loss: 0.2437, acc: 0.9041
loss: 0.2422, acc: 0.9051
loss: 0.2472, acc: 0.9032
> val_acc: 0.8643, val_f1: 0.8148
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2381, acc: 0.8958
loss: 0.2236, acc: 0.9102
loss: 0.2288, acc: 0.9111
loss: 0.2402, acc: 0.9080
loss: 0.2414, acc: 0.9117
loss: 0.2442, acc: 0.9129
loss: 0.2411, acc: 0.9119
loss: 0.2376, acc: 0.9153
loss: 0.2334, acc: 0.9193
loss: 0.2419, acc: 0.9154
loss: 0.2421, acc: 0.9145
loss: 0.2431, acc: 0.9106
loss: 0.2398, acc: 0.9117
loss: 0.2429, acc: 0.9118
loss: 0.2442, acc: 0.9105
loss: 0.2483, acc: 0.9075
loss: 0.2448, acc: 0.9078
loss: 0.2498, acc: 0.9066
loss: 0.2488, acc: 0.9062
loss: 0.2522, acc: 0.9047
loss: 0.2539, acc: 0.9047
loss: 0.2596, acc: 0.9019
loss: 0.2590, acc: 0.9024
> val_acc: 0.8313, val_f1: 0.7199
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2146, acc: 0.9187
loss: 0.2008, acc: 0.9281
loss: 0.2135, acc: 0.9271
loss: 0.2304, acc: 0.9187
loss: 0.2659, acc: 0.9113
loss: 0.2617, acc: 0.9115
loss: 0.2556, acc: 0.9143
loss: 0.2559, acc: 0.9141
loss: 0.2631, acc: 0.9125
loss: 0.2831, acc: 0.9075
loss: 0.2779, acc: 0.9080
loss: 0.2741, acc: 0.9078
loss: 0.2693, acc: 0.9096
loss: 0.2698, acc: 0.9085
loss: 0.2749, acc: 0.9071
loss: 0.2768, acc: 0.9066
loss: 0.2729, acc: 0.9081
loss: 0.2711, acc: 0.9076
loss: 0.2757, acc: 0.9056
loss: 0.2710, acc: 0.9066
loss: 0.2717, acc: 0.9051
loss: 0.2720, acc: 0.9048
> val_acc: 0.8545, val_f1: 0.7742
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1599, acc: 0.9219
loss: 0.2608, acc: 0.8973
loss: 0.2490, acc: 0.8958
loss: 0.2495, acc: 0.8989
loss: 0.2317, acc: 0.9176
loss: 0.2139, acc: 0.9213
loss: 0.2173, acc: 0.9219
loss: 0.2184, acc: 0.9223
loss: 0.2292, acc: 0.9182
loss: 0.2166, acc: 0.9242
loss: 0.2148, acc: 0.9243
loss: 0.2054, acc: 0.9276
loss: 0.2016, acc: 0.9264
loss: 0.2025, acc: 0.9254
loss: 0.2049, acc: 0.9240
loss: 0.2070, acc: 0.9241
loss: 0.2088, acc: 0.9226
loss: 0.2074, acc: 0.9221
loss: 0.2106, acc: 0.9226
loss: 0.2126, acc: 0.9207
loss: 0.2128, acc: 0.9203
loss: 0.2168, acc: 0.9197
loss: 0.2195, acc: 0.9194
> val_acc: 0.8473, val_f1: 0.7751
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.2367, acc: 0.9062
loss: 0.2514, acc: 0.8854
loss: 0.2232, acc: 0.9085
loss: 0.2241, acc: 0.9095
loss: 0.2467, acc: 0.9049
loss: 0.2427, acc: 0.9062
loss: 0.2320, acc: 0.9108
loss: 0.2372, acc: 0.9071
loss: 0.2372, acc: 0.9070
loss: 0.2412, acc: 0.9069
loss: 0.2338, acc: 0.9080
loss: 0.2290, acc: 0.9089
loss: 0.2248, acc: 0.9121
loss: 0.2222, acc: 0.9121
loss: 0.2162, acc: 0.9139
loss: 0.2101, acc: 0.9153
loss: 0.2111, acc: 0.9156
loss: 0.2246, acc: 0.9115
loss: 0.2317, acc: 0.9089
loss: 0.2308, acc: 0.9091
loss: 0.2330, acc: 0.9078
loss: 0.2323, acc: 0.9077
> val_acc: 0.8473, val_f1: 0.7911
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8759, test_f1: 0.8105
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 560526848
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0899, acc: 0.5625
loss: 1.0589, acc: 0.5750
loss: 1.0325, acc: 0.5708
loss: 0.9852, acc: 0.5875
loss: 0.9334, acc: 0.6100
loss: 0.8908, acc: 0.6292
loss: 0.8598, acc: 0.6429
loss: 0.8310, acc: 0.6555
loss: 0.8049, acc: 0.6660
loss: 0.7881, acc: 0.6750
loss: 0.7719, acc: 0.6841
loss: 0.7627, acc: 0.6875
loss: 0.7487, acc: 0.6957
loss: 0.7358, acc: 0.7000
loss: 0.7275, acc: 0.7037
loss: 0.7232, acc: 0.7035
loss: 0.7152, acc: 0.7074
loss: 0.7122, acc: 0.7083
loss: 0.7127, acc: 0.7076
loss: 0.7060, acc: 0.7109
loss: 0.6999, acc: 0.7131
loss: 0.6879, acc: 0.7182
> val_acc: 0.8080, val_f1: 0.6648
>> saved: peft/roberta_lora/rest14//acc_0.808_f1_0.6648_230828-0550
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6819, acc: 0.7188
loss: 0.5694, acc: 0.7723
loss: 0.5770, acc: 0.7708
loss: 0.5466, acc: 0.7831
loss: 0.5497, acc: 0.7713
loss: 0.5413, acc: 0.7789
loss: 0.5430, acc: 0.7725
loss: 0.5424, acc: 0.7762
loss: 0.5499, acc: 0.7753
loss: 0.5380, acc: 0.7819
loss: 0.5450, acc: 0.7788
loss: 0.5422, acc: 0.7818
loss: 0.5357, acc: 0.7873
loss: 0.5363, acc: 0.7864
loss: 0.5320, acc: 0.7882
loss: 0.5303, acc: 0.7877
loss: 0.5263, acc: 0.7881
loss: 0.5309, acc: 0.7852
loss: 0.5270, acc: 0.7874
loss: 0.5244, acc: 0.7880
loss: 0.5222, acc: 0.7886
loss: 0.5147, acc: 0.7926
loss: 0.5088, acc: 0.7941
> val_acc: 0.8589, val_f1: 0.7846
>> saved: peft/roberta_lora/rest14//acc_0.8589_f1_0.7846_230828-0550
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4289, acc: 0.8281
loss: 0.3771, acc: 0.8611
loss: 0.4340, acc: 0.8415
loss: 0.4391, acc: 0.8388
loss: 0.4490, acc: 0.8255
loss: 0.4324, acc: 0.8330
loss: 0.4411, acc: 0.8217
loss: 0.4438, acc: 0.8189
loss: 0.4476, acc: 0.8175
loss: 0.4514, acc: 0.8151
loss: 0.4519, acc: 0.8142
loss: 0.4540, acc: 0.8125
loss: 0.4516, acc: 0.8130
loss: 0.4443, acc: 0.8175
loss: 0.4375, acc: 0.8201
loss: 0.4393, acc: 0.8200
loss: 0.4351, acc: 0.8237
loss: 0.4399, acc: 0.8206
loss: 0.4441, acc: 0.8185
loss: 0.4438, acc: 0.8194
loss: 0.4450, acc: 0.8182
loss: 0.4417, acc: 0.8214
> val_acc: 0.8500, val_f1: 0.7590
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2569, acc: 0.9062
loss: 0.3070, acc: 0.8802
loss: 0.3399, acc: 0.8665
loss: 0.3277, acc: 0.8711
loss: 0.3336, acc: 0.8690
loss: 0.3332, acc: 0.8666
loss: 0.3474, acc: 0.8619
loss: 0.3489, acc: 0.8602
loss: 0.3637, acc: 0.8567
loss: 0.3573, acc: 0.8573
loss: 0.3597, acc: 0.8572
loss: 0.3708, acc: 0.8555
loss: 0.3792, acc: 0.8499
loss: 0.3894, acc: 0.8456
loss: 0.3869, acc: 0.8468
loss: 0.3864, acc: 0.8479
loss: 0.3899, acc: 0.8465
loss: 0.3939, acc: 0.8445
loss: 0.3986, acc: 0.8407
loss: 0.3969, acc: 0.8411
loss: 0.3965, acc: 0.8416
loss: 0.4004, acc: 0.8370
loss: 0.3958, acc: 0.8398
> val_acc: 0.8598, val_f1: 0.7712
>> saved: peft/roberta_lora/rest14//acc_0.8598_f1_0.7712_230828-0551
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2732, acc: 0.8958
loss: 0.2831, acc: 0.9023
loss: 0.2783, acc: 0.8990
loss: 0.2932, acc: 0.8941
loss: 0.2945, acc: 0.8940
loss: 0.3091, acc: 0.8839
loss: 0.3133, acc: 0.8816
loss: 0.3168, acc: 0.8824
loss: 0.3199, acc: 0.8779
loss: 0.3257, acc: 0.8783
loss: 0.3301, acc: 0.8791
loss: 0.3278, acc: 0.8815
loss: 0.3243, acc: 0.8795
loss: 0.3245, acc: 0.8801
loss: 0.3256, acc: 0.8793
loss: 0.3308, acc: 0.8762
loss: 0.3361, acc: 0.8712
loss: 0.3368, acc: 0.8707
loss: 0.3354, acc: 0.8716
loss: 0.3348, acc: 0.8709
loss: 0.3347, acc: 0.8704
loss: 0.3348, acc: 0.8707
loss: 0.3375, acc: 0.8695
> val_acc: 0.8661, val_f1: 0.7903
>> saved: peft/roberta_lora/rest14//acc_0.8661_f1_0.7903_230828-0552
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2610, acc: 0.8875
loss: 0.2393, acc: 0.9031
loss: 0.2782, acc: 0.8917
loss: 0.2949, acc: 0.8797
loss: 0.2920, acc: 0.8825
loss: 0.2926, acc: 0.8844
loss: 0.2934, acc: 0.8821
loss: 0.2912, acc: 0.8844
loss: 0.2944, acc: 0.8826
loss: 0.2891, acc: 0.8831
loss: 0.2979, acc: 0.8812
loss: 0.2966, acc: 0.8828
loss: 0.2973, acc: 0.8832
loss: 0.3043, acc: 0.8808
loss: 0.3018, acc: 0.8829
loss: 0.3001, acc: 0.8828
loss: 0.3048, acc: 0.8798
loss: 0.3067, acc: 0.8795
loss: 0.3122, acc: 0.8783
loss: 0.3129, acc: 0.8775
loss: 0.3115, acc: 0.8777
loss: 0.3100, acc: 0.8787
> val_acc: 0.8625, val_f1: 0.7939
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2469, acc: 0.9375
loss: 0.2274, acc: 0.9152
loss: 0.2716, acc: 0.9036
loss: 0.2579, acc: 0.9044
loss: 0.2518, acc: 0.9020
loss: 0.2670, acc: 0.8877
loss: 0.2592, acc: 0.8906
loss: 0.2676, acc: 0.8885
loss: 0.2643, acc: 0.8914
loss: 0.2640, acc: 0.8936
loss: 0.2711, acc: 0.8912
loss: 0.2703, acc: 0.8898
loss: 0.2728, acc: 0.8906
loss: 0.2751, acc: 0.8913
loss: 0.2758, acc: 0.8924
loss: 0.2814, acc: 0.8896
loss: 0.2824, acc: 0.8880
loss: 0.2839, acc: 0.8865
loss: 0.2822, acc: 0.8865
loss: 0.2780, acc: 0.8889
loss: 0.2780, acc: 0.8885
loss: 0.2732, acc: 0.8902
loss: 0.2759, acc: 0.8903
> val_acc: 0.8277, val_f1: 0.7120
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3522, acc: 0.8359
loss: 0.2958, acc: 0.8611
loss: 0.2864, acc: 0.8795
loss: 0.2790, acc: 0.8816
loss: 0.2792, acc: 0.8815
loss: 0.2694, acc: 0.8847
loss: 0.2548, acc: 0.8915
loss: 0.2485, acc: 0.8966
loss: 0.2433, acc: 0.9013
loss: 0.2452, acc: 0.9043
loss: 0.2452, acc: 0.9039
loss: 0.2512, acc: 0.8994
loss: 0.2552, acc: 0.8975
loss: 0.2563, acc: 0.8972
loss: 0.2585, acc: 0.8974
loss: 0.2634, acc: 0.8932
loss: 0.2679, acc: 0.8925
loss: 0.2706, acc: 0.8897
loss: 0.2736, acc: 0.8886
loss: 0.2757, acc: 0.8870
loss: 0.2757, acc: 0.8879
loss: 0.2757, acc: 0.8893
> val_acc: 0.8688, val_f1: 0.8005
>> saved: peft/roberta_lora/rest14//acc_0.8688_f1_0.8005_230828-0553
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.1932, acc: 0.9375
loss: 0.2583, acc: 0.9010
loss: 0.2196, acc: 0.9148
loss: 0.2068, acc: 0.9141
loss: 0.2055, acc: 0.9211
loss: 0.2032, acc: 0.9171
loss: 0.2223, acc: 0.9133
loss: 0.2168, acc: 0.9141
loss: 0.2360, acc: 0.9108
loss: 0.2399, acc: 0.9096
loss: 0.2424, acc: 0.9112
loss: 0.2396, acc: 0.9113
loss: 0.2386, acc: 0.9134
loss: 0.2442, acc: 0.9119
loss: 0.2442, acc: 0.9124
loss: 0.2459, acc: 0.9128
loss: 0.2425, acc: 0.9147
loss: 0.2424, acc: 0.9161
loss: 0.2433, acc: 0.9152
loss: 0.2451, acc: 0.9147
loss: 0.2507, acc: 0.9121
loss: 0.2534, acc: 0.9104
loss: 0.2614, acc: 0.9093
> val_acc: 0.8491, val_f1: 0.7475
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3280, acc: 0.8750
loss: 0.2299, acc: 0.9180
loss: 0.2242, acc: 0.9183
loss: 0.2254, acc: 0.9149
loss: 0.2300, acc: 0.9117
loss: 0.2371, acc: 0.9062
loss: 0.2404, acc: 0.9081
loss: 0.2318, acc: 0.9120
loss: 0.2306, acc: 0.9135
loss: 0.2323, acc: 0.9128
loss: 0.2271, acc: 0.9133
loss: 0.2294, acc: 0.9122
loss: 0.2245, acc: 0.9132
loss: 0.2299, acc: 0.9118
loss: 0.2347, acc: 0.9097
loss: 0.2384, acc: 0.9079
loss: 0.2369, acc: 0.9096
loss: 0.2391, acc: 0.9091
loss: 0.2373, acc: 0.9099
loss: 0.2350, acc: 0.9117
loss: 0.2347, acc: 0.9117
loss: 0.2356, acc: 0.9117
loss: 0.2343, acc: 0.9124
> val_acc: 0.8446, val_f1: 0.7721
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2843, acc: 0.8562
loss: 0.2308, acc: 0.8938
loss: 0.2268, acc: 0.9000
loss: 0.1999, acc: 0.9156
loss: 0.1880, acc: 0.9237
loss: 0.1970, acc: 0.9250
loss: 0.2121, acc: 0.9214
loss: 0.2143, acc: 0.9203
loss: 0.2183, acc: 0.9187
loss: 0.2252, acc: 0.9150
loss: 0.2327, acc: 0.9108
loss: 0.2359, acc: 0.9104
loss: 0.2410, acc: 0.9091
loss: 0.2415, acc: 0.9071
loss: 0.2405, acc: 0.9062
loss: 0.2411, acc: 0.9070
loss: 0.2398, acc: 0.9081
loss: 0.2436, acc: 0.9076
loss: 0.2442, acc: 0.9069
loss: 0.2443, acc: 0.9062
loss: 0.2437, acc: 0.9060
loss: 0.2455, acc: 0.9068
> val_acc: 0.8562, val_f1: 0.7789
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1432, acc: 0.9688
loss: 0.2366, acc: 0.9196
loss: 0.2222, acc: 0.9271
loss: 0.2072, acc: 0.9301
loss: 0.2086, acc: 0.9304
loss: 0.2162, acc: 0.9282
loss: 0.2100, acc: 0.9258
loss: 0.2110, acc: 0.9198
loss: 0.2033, acc: 0.9196
loss: 0.2086, acc: 0.9195
loss: 0.2074, acc: 0.9213
loss: 0.2076, acc: 0.9194
loss: 0.2090, acc: 0.9194
loss: 0.2110, acc: 0.9179
loss: 0.2158, acc: 0.9171
loss: 0.2247, acc: 0.9127
loss: 0.2240, acc: 0.9131
loss: 0.2235, acc: 0.9127
loss: 0.2230, acc: 0.9134
loss: 0.2297, acc: 0.9108
loss: 0.2332, acc: 0.9105
loss: 0.2313, acc: 0.9118
loss: 0.2351, acc: 0.9107
> val_acc: 0.8509, val_f1: 0.7734
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1463, acc: 0.9609
loss: 0.1421, acc: 0.9653
loss: 0.1301, acc: 0.9621
loss: 0.1404, acc: 0.9539
loss: 0.1447, acc: 0.9544
loss: 0.1492, acc: 0.9504
loss: 0.1647, acc: 0.9403
loss: 0.1721, acc: 0.9383
loss: 0.1755, acc: 0.9375
loss: 0.1837, acc: 0.9318
loss: 0.1832, acc: 0.9329
loss: 0.1876, acc: 0.9311
loss: 0.1862, acc: 0.9331
loss: 0.1893, acc: 0.9321
loss: 0.1930, acc: 0.9295
loss: 0.1926, acc: 0.9292
loss: 0.1959, acc: 0.9282
loss: 0.1955, acc: 0.9280
loss: 0.1966, acc: 0.9275
loss: 0.1985, acc: 0.9265
loss: 0.1988, acc: 0.9267
loss: 0.1989, acc: 0.9266
> val_acc: 0.8402, val_f1: 0.7347
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8688, test_f1: 0.8005
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 543618560
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: rest14
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 0.9574, acc: 0.6125
loss: 0.9563, acc: 0.6000
loss: 0.9300, acc: 0.6146
loss: 0.8644, acc: 0.6406
loss: 0.8267, acc: 0.6575
loss: 0.7975, acc: 0.6729
loss: 0.7884, acc: 0.6723
loss: 0.7766, acc: 0.6805
loss: 0.7651, acc: 0.6854
loss: 0.7499, acc: 0.6906
loss: 0.7517, acc: 0.6966
loss: 0.7445, acc: 0.6974
loss: 0.7382, acc: 0.7000
loss: 0.7236, acc: 0.7067
loss: 0.7132, acc: 0.7117
loss: 0.7017, acc: 0.7172
loss: 0.6952, acc: 0.7191
loss: 0.6862, acc: 0.7219
loss: 0.6839, acc: 0.7253
loss: 0.6797, acc: 0.7266
loss: 0.6713, acc: 0.7307
loss: 0.6672, acc: 0.7324
> val_acc: 0.8330, val_f1: 0.7173
>> saved: peft/roberta_lora/rest14//acc_0.833_f1_0.7173_230828-0556
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5020, acc: 0.8125
loss: 0.5195, acc: 0.7812
loss: 0.5368, acc: 0.7682
loss: 0.5081, acc: 0.7868
loss: 0.4966, acc: 0.7912
loss: 0.5031, acc: 0.7928
loss: 0.5169, acc: 0.7861
loss: 0.5196, acc: 0.7855
loss: 0.5115, acc: 0.7879
loss: 0.5021, acc: 0.7926
loss: 0.4891, acc: 0.7993
loss: 0.5052, acc: 0.7917
loss: 0.5105, acc: 0.7893
loss: 0.4989, acc: 0.7957
loss: 0.4984, acc: 0.7943
loss: 0.5031, acc: 0.7930
loss: 0.5014, acc: 0.7973
loss: 0.4988, acc: 0.7992
loss: 0.4982, acc: 0.7986
loss: 0.5006, acc: 0.7977
loss: 0.5006, acc: 0.7981
loss: 0.5014, acc: 0.7991
loss: 0.5059, acc: 0.7960
> val_acc: 0.8071, val_f1: 0.6575
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3616, acc: 0.8438
loss: 0.4389, acc: 0.8264
loss: 0.4217, acc: 0.8393
loss: 0.3946, acc: 0.8536
loss: 0.4180, acc: 0.8411
loss: 0.4158, acc: 0.8405
loss: 0.4038, acc: 0.8410
loss: 0.4066, acc: 0.8365
loss: 0.4038, acc: 0.8395
loss: 0.4105, acc: 0.8348
loss: 0.4278, acc: 0.8252
loss: 0.4264, acc: 0.8273
loss: 0.4222, acc: 0.8271
loss: 0.4210, acc: 0.8297
loss: 0.4226, acc: 0.8307
loss: 0.4330, acc: 0.8248
loss: 0.4280, acc: 0.8263
loss: 0.4275, acc: 0.8258
loss: 0.4252, acc: 0.8271
loss: 0.4213, acc: 0.8295
loss: 0.4221, acc: 0.8293
loss: 0.4236, acc: 0.8300
> val_acc: 0.8116, val_f1: 0.6616
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4460, acc: 0.8438
loss: 0.3943, acc: 0.8594
loss: 0.4009, acc: 0.8551
loss: 0.3860, acc: 0.8535
loss: 0.3864, acc: 0.8542
loss: 0.3904, acc: 0.8486
loss: 0.3916, acc: 0.8508
loss: 0.3845, acc: 0.8533
loss: 0.3842, acc: 0.8498
loss: 0.3884, acc: 0.8451
loss: 0.3809, acc: 0.8480
loss: 0.3674, acc: 0.8504
loss: 0.3682, acc: 0.8499
loss: 0.3653, acc: 0.8518
loss: 0.3716, acc: 0.8504
loss: 0.3674, acc: 0.8540
loss: 0.3754, acc: 0.8515
loss: 0.3774, acc: 0.8496
loss: 0.3760, acc: 0.8503
loss: 0.3748, acc: 0.8496
loss: 0.3746, acc: 0.8509
loss: 0.3815, acc: 0.8467
loss: 0.3854, acc: 0.8454
> val_acc: 0.8455, val_f1: 0.7479
>> saved: peft/roberta_lora/rest14//acc_0.8455_f1_0.7479_230828-0558
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3366, acc: 0.8750
loss: 0.3747, acc: 0.8516
loss: 0.3271, acc: 0.8678
loss: 0.3192, acc: 0.8767
loss: 0.2977, acc: 0.8859
loss: 0.3045, acc: 0.8862
loss: 0.3123, acc: 0.8769
loss: 0.3102, acc: 0.8791
loss: 0.3100, acc: 0.8779
loss: 0.3071, acc: 0.8796
loss: 0.3243, acc: 0.8715
loss: 0.3317, acc: 0.8702
loss: 0.3336, acc: 0.8686
loss: 0.3278, acc: 0.8722
loss: 0.3361, acc: 0.8694
loss: 0.3469, acc: 0.8658
loss: 0.3451, acc: 0.8675
loss: 0.3484, acc: 0.8675
loss: 0.3478, acc: 0.8679
loss: 0.3498, acc: 0.8658
loss: 0.3472, acc: 0.8680
loss: 0.3459, acc: 0.8689
loss: 0.3417, acc: 0.8711
> val_acc: 0.8545, val_f1: 0.7576
>> saved: peft/roberta_lora/rest14//acc_0.8545_f1_0.7576_230828-0558
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2514, acc: 0.9000
loss: 0.2650, acc: 0.9031
loss: 0.2521, acc: 0.9000
loss: 0.2320, acc: 0.9078
loss: 0.2306, acc: 0.9050
loss: 0.2349, acc: 0.9052
loss: 0.2278, acc: 0.9107
loss: 0.2439, acc: 0.9062
loss: 0.2765, acc: 0.8917
loss: 0.2810, acc: 0.8894
loss: 0.2826, acc: 0.8892
loss: 0.2812, acc: 0.8911
loss: 0.2820, acc: 0.8933
loss: 0.2865, acc: 0.8902
loss: 0.2947, acc: 0.8850
loss: 0.2922, acc: 0.8855
loss: 0.2969, acc: 0.8824
loss: 0.2981, acc: 0.8816
loss: 0.3013, acc: 0.8809
loss: 0.3044, acc: 0.8803
loss: 0.3051, acc: 0.8810
loss: 0.3066, acc: 0.8807
> val_acc: 0.8554, val_f1: 0.7801
>> saved: peft/roberta_lora/rest14//acc_0.8554_f1_0.7801_230828-0559
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2029, acc: 0.9219
loss: 0.2088, acc: 0.9286
loss: 0.2247, acc: 0.9297
loss: 0.2323, acc: 0.9246
loss: 0.2586, acc: 0.9105
loss: 0.2682, acc: 0.9039
loss: 0.2637, acc: 0.9053
loss: 0.2612, acc: 0.9046
loss: 0.2752, acc: 0.8973
loss: 0.2798, acc: 0.8956
loss: 0.2897, acc: 0.8894
loss: 0.2840, acc: 0.8925
loss: 0.2793, acc: 0.8947
loss: 0.2758, acc: 0.8965
loss: 0.2788, acc: 0.8971
loss: 0.2864, acc: 0.8916
loss: 0.2877, acc: 0.8906
loss: 0.2865, acc: 0.8908
loss: 0.2862, acc: 0.8913
loss: 0.2916, acc: 0.8898
loss: 0.2912, acc: 0.8900
loss: 0.2903, acc: 0.8908
loss: 0.2940, acc: 0.8906
> val_acc: 0.8473, val_f1: 0.7716
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2456, acc: 0.8984
loss: 0.2270, acc: 0.9167
loss: 0.2176, acc: 0.9219
loss: 0.2240, acc: 0.9211
loss: 0.2226, acc: 0.9193
loss: 0.2247, acc: 0.9127
loss: 0.2273, acc: 0.9053
loss: 0.2371, acc: 0.8990
loss: 0.2349, acc: 0.9020
loss: 0.2530, acc: 0.8960
loss: 0.2629, acc: 0.8924
loss: 0.2695, acc: 0.8914
loss: 0.2806, acc: 0.8892
loss: 0.2824, acc: 0.8872
loss: 0.2815, acc: 0.8868
loss: 0.2825, acc: 0.8849
loss: 0.2838, acc: 0.8850
loss: 0.2854, acc: 0.8834
loss: 0.2808, acc: 0.8850
loss: 0.2798, acc: 0.8867
loss: 0.2782, acc: 0.8879
loss: 0.2767, acc: 0.8890
> val_acc: 0.8670, val_f1: 0.7920
>> saved: peft/roberta_lora/rest14//acc_0.867_f1_0.792_230828-0600
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2439, acc: 0.8750
loss: 0.1484, acc: 0.9375
loss: 0.1476, acc: 0.9489
loss: 0.1887, acc: 0.9336
loss: 0.2054, acc: 0.9271
loss: 0.2070, acc: 0.9267
loss: 0.1889, acc: 0.9335
loss: 0.1963, acc: 0.9332
loss: 0.1987, acc: 0.9306
loss: 0.2038, acc: 0.9287
loss: 0.2079, acc: 0.9271
loss: 0.2064, acc: 0.9286
loss: 0.2021, acc: 0.9293
loss: 0.2017, acc: 0.9280
loss: 0.2015, acc: 0.9274
loss: 0.2088, acc: 0.9256
loss: 0.2155, acc: 0.9236
loss: 0.2238, acc: 0.9204
loss: 0.2295, acc: 0.9176
loss: 0.2280, acc: 0.9176
loss: 0.2269, acc: 0.9180
loss: 0.2319, acc: 0.9151
loss: 0.2326, acc: 0.9144
> val_acc: 0.8634, val_f1: 0.8067
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2640, acc: 0.8854
loss: 0.2351, acc: 0.9102
loss: 0.2397, acc: 0.9062
loss: 0.2344, acc: 0.9132
loss: 0.2231, acc: 0.9185
loss: 0.2325, acc: 0.9152
loss: 0.2302, acc: 0.9176
loss: 0.2143, acc: 0.9219
loss: 0.2126, acc: 0.9222
loss: 0.2111, acc: 0.9245
loss: 0.2096, acc: 0.9245
loss: 0.2087, acc: 0.9278
loss: 0.2081, acc: 0.9281
loss: 0.2188, acc: 0.9233
loss: 0.2200, acc: 0.9217
loss: 0.2243, acc: 0.9199
loss: 0.2196, acc: 0.9224
loss: 0.2213, acc: 0.9212
loss: 0.2281, acc: 0.9173
loss: 0.2271, acc: 0.9174
loss: 0.2317, acc: 0.9154
loss: 0.2320, acc: 0.9149
loss: 0.2303, acc: 0.9149
> val_acc: 0.8616, val_f1: 0.7786
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0973, acc: 0.9688
loss: 0.1615, acc: 0.9437
loss: 0.1579, acc: 0.9437
loss: 0.1694, acc: 0.9391
loss: 0.1653, acc: 0.9387
loss: 0.1702, acc: 0.9375
loss: 0.1604, acc: 0.9411
loss: 0.1596, acc: 0.9422
loss: 0.1716, acc: 0.9361
loss: 0.1773, acc: 0.9306
loss: 0.1758, acc: 0.9324
loss: 0.1719, acc: 0.9333
loss: 0.1692, acc: 0.9351
loss: 0.1732, acc: 0.9321
loss: 0.1737, acc: 0.9325
loss: 0.1839, acc: 0.9289
loss: 0.1868, acc: 0.9261
loss: 0.1884, acc: 0.9250
loss: 0.1891, acc: 0.9253
loss: 0.1906, acc: 0.9241
loss: 0.1960, acc: 0.9220
loss: 0.2017, acc: 0.9210
> val_acc: 0.8598, val_f1: 0.7890
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.2605, acc: 0.9062
loss: 0.1782, acc: 0.9420
loss: 0.1608, acc: 0.9453
loss: 0.1757, acc: 0.9412
loss: 0.1828, acc: 0.9403
loss: 0.2037, acc: 0.9317
loss: 0.2025, acc: 0.9297
loss: 0.2042, acc: 0.9291
loss: 0.2014, acc: 0.9301
loss: 0.2154, acc: 0.9249
loss: 0.2084, acc: 0.9261
loss: 0.2145, acc: 0.9243
loss: 0.2187, acc: 0.9219
loss: 0.2227, acc: 0.9207
loss: 0.2226, acc: 0.9188
loss: 0.2171, acc: 0.9213
loss: 0.2147, acc: 0.9226
loss: 0.2206, acc: 0.9206
loss: 0.2237, acc: 0.9185
loss: 0.2268, acc: 0.9162
loss: 0.2280, acc: 0.9151
loss: 0.2244, acc: 0.9162
loss: 0.2231, acc: 0.9166
> val_acc: 0.8554, val_f1: 0.7769
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.2877, acc: 0.9141
loss: 0.2422, acc: 0.9167
loss: 0.1980, acc: 0.9353
loss: 0.1901, acc: 0.9424
loss: 0.1872, acc: 0.9427
loss: 0.1926, acc: 0.9375
loss: 0.1839, acc: 0.9393
loss: 0.1845, acc: 0.9399
loss: 0.1810, acc: 0.9403
loss: 0.1869, acc: 0.9356
loss: 0.1957, acc: 0.9317
loss: 0.1997, acc: 0.9285
loss: 0.1936, acc: 0.9307
loss: 0.1899, acc: 0.9330
loss: 0.1972, acc: 0.9316
loss: 0.1985, acc: 0.9320
loss: 0.1945, acc: 0.9338
loss: 0.1954, acc: 0.9336
loss: 0.1979, acc: 0.9332
loss: 0.1997, acc: 0.9306
loss: 0.1973, acc: 0.9315
loss: 0.1960, acc: 0.9329
> val_acc: 0.8482, val_f1: 0.7730
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8670, test_f1: 0.7920
>> test_acc: 0.8759, test_f1: 0.8105
>> test_acc: 0.8688, test_f1: 0.8005
>> test_acc: 0.8670, test_f1: 0.7920

>> avg_test_acc: 0.8705, avg_test_f1: 0.8010
>> max_test_acc: 0.8759, max_test_f1: 0.8105
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 439079424
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1272, acc: 0.2812
loss: 1.0799, acc: 0.3937
loss: 1.0779, acc: 0.4021
loss: 1.0747, acc: 0.4094
loss: 1.0718, acc: 0.4150
loss: 1.0629, acc: 0.4229
loss: 1.0548, acc: 0.4339
loss: 1.0525, acc: 0.4367
loss: 1.0467, acc: 0.4389
loss: 1.0391, acc: 0.4487
loss: 1.0308, acc: 0.4608
loss: 1.0190, acc: 0.4688
loss: 1.0061, acc: 0.4769
loss: 0.9937, acc: 0.4857
loss: 0.9857, acc: 0.4954
loss: 0.9786, acc: 0.5000
loss: 0.9755, acc: 0.5040
loss: 0.9700, acc: 0.5083
loss: 0.9667, acc: 0.5102
loss: 0.9634, acc: 0.5144
loss: 0.9590, acc: 0.5182
loss: 0.9523, acc: 0.5230
loss: 0.9468, acc: 0.5269
loss: 0.9412, acc: 0.5320
loss: 0.9359, acc: 0.5340
loss: 0.9293, acc: 0.5387
loss: 0.9230, acc: 0.5435
loss: 0.9192, acc: 0.5464
loss: 0.9135, acc: 0.5491
loss: 0.9101, acc: 0.5506
loss: 0.9056, acc: 0.5544
loss: 0.8998, acc: 0.5584
loss: 0.8959, acc: 0.5610
loss: 0.8923, acc: 0.5634
loss: 0.8874, acc: 0.5666
loss: 0.8838, acc: 0.5698
loss: 0.8809, acc: 0.5716
loss: 0.8772, acc: 0.5745
loss: 0.8714, acc: 0.5779
loss: 0.8696, acc: 0.5791
loss: 0.8623, acc: 0.5831
loss: 0.8562, acc: 0.5868
loss: 0.8505, acc: 0.5903
loss: 0.8446, acc: 0.5943
loss: 0.8393, acc: 0.5974
loss: 0.8335, acc: 0.6014
loss: 0.8276, acc: 0.6045
loss: 0.8207, acc: 0.6082
loss: 0.8161, acc: 0.6107
loss: 0.8106, acc: 0.6141
loss: 0.8057, acc: 0.6174
loss: 0.8014, acc: 0.6206
loss: 0.7944, acc: 0.6243
loss: 0.7895, acc: 0.6274
loss: 0.7853, acc: 0.6295
loss: 0.7837, acc: 0.6304
loss: 0.7803, acc: 0.6332
loss: 0.7761, acc: 0.6360
loss: 0.7724, acc: 0.6387
loss: 0.7669, acc: 0.6418
loss: 0.7646, acc: 0.6431
loss: 0.7596, acc: 0.6463
loss: 0.7550, acc: 0.6489
loss: 0.7518, acc: 0.6512
loss: 0.7494, acc: 0.6530
loss: 0.7458, acc: 0.6553
loss: 0.7411, acc: 0.6585
loss: 0.7384, acc: 0.6603
loss: 0.7364, acc: 0.6614
loss: 0.7337, acc: 0.6627
> val_acc: 0.7815, val_f1: 0.7789
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.7815_f1_0.7789_230828-0605.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4985, acc: 0.7937
loss: 0.4308, acc: 0.8344
loss: 0.3948, acc: 0.8542
loss: 0.3924, acc: 0.8562
loss: 0.4006, acc: 0.8575
loss: 0.3877, acc: 0.8583
loss: 0.4045, acc: 0.8527
loss: 0.4097, acc: 0.8484
loss: 0.4024, acc: 0.8507
loss: 0.4071, acc: 0.8494
loss: 0.4000, acc: 0.8517
loss: 0.4048, acc: 0.8495
loss: 0.3966, acc: 0.8529
loss: 0.3930, acc: 0.8549
loss: 0.3916, acc: 0.8542
loss: 0.3938, acc: 0.8543
loss: 0.4031, acc: 0.8507
loss: 0.4005, acc: 0.8524
loss: 0.3991, acc: 0.8536
loss: 0.3952, acc: 0.8559
loss: 0.3911, acc: 0.8589
loss: 0.3913, acc: 0.8597
loss: 0.3916, acc: 0.8598
loss: 0.3892, acc: 0.8591
loss: 0.3924, acc: 0.8570
loss: 0.3914, acc: 0.8577
loss: 0.3881, acc: 0.8595
loss: 0.3870, acc: 0.8600
loss: 0.3872, acc: 0.8597
loss: 0.3877, acc: 0.8598
loss: 0.3850, acc: 0.8597
loss: 0.3855, acc: 0.8594
loss: 0.3849, acc: 0.8593
loss: 0.3879, acc: 0.8588
loss: 0.3903, acc: 0.8568
loss: 0.3903, acc: 0.8571
loss: 0.3908, acc: 0.8561
loss: 0.3908, acc: 0.8554
loss: 0.3923, acc: 0.8550
loss: 0.3903, acc: 0.8559
loss: 0.3902, acc: 0.8558
loss: 0.3899, acc: 0.8558
loss: 0.3919, acc: 0.8548
loss: 0.3901, acc: 0.8554
loss: 0.3885, acc: 0.8560
loss: 0.3895, acc: 0.8558
loss: 0.3907, acc: 0.8553
loss: 0.3904, acc: 0.8551
loss: 0.3897, acc: 0.8548
loss: 0.3895, acc: 0.8549
loss: 0.3900, acc: 0.8549
loss: 0.3913, acc: 0.8540
loss: 0.3910, acc: 0.8540
loss: 0.3921, acc: 0.8531
loss: 0.3919, acc: 0.8538
loss: 0.3910, acc: 0.8546
loss: 0.3901, acc: 0.8553
loss: 0.3892, acc: 0.8558
loss: 0.3896, acc: 0.8552
loss: 0.3901, acc: 0.8547
loss: 0.3882, acc: 0.8555
loss: 0.3861, acc: 0.8559
loss: 0.3861, acc: 0.8557
loss: 0.3856, acc: 0.8561
loss: 0.3866, acc: 0.8556
loss: 0.3858, acc: 0.8561
loss: 0.3856, acc: 0.8561
loss: 0.3847, acc: 0.8564
loss: 0.3851, acc: 0.8565
loss: 0.3864, acc: 0.8561
> val_acc: 0.8146, val_f1: 0.8111
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.8146_f1_0.8111_230828-0607.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2655, acc: 0.9125
loss: 0.2299, acc: 0.9219
loss: 0.2633, acc: 0.9062
loss: 0.2443, acc: 0.9094
loss: 0.2335, acc: 0.9150
loss: 0.2386, acc: 0.9125
loss: 0.2390, acc: 0.9107
loss: 0.2354, acc: 0.9133
loss: 0.2262, acc: 0.9167
loss: 0.2302, acc: 0.9150
loss: 0.2279, acc: 0.9159
loss: 0.2274, acc: 0.9187
loss: 0.2175, acc: 0.9226
loss: 0.2182, acc: 0.9219
loss: 0.2237, acc: 0.9196
loss: 0.2190, acc: 0.9215
loss: 0.2171, acc: 0.9224
loss: 0.2137, acc: 0.9240
loss: 0.2145, acc: 0.9237
loss: 0.2139, acc: 0.9237
loss: 0.2133, acc: 0.9244
loss: 0.2139, acc: 0.9244
loss: 0.2117, acc: 0.9253
loss: 0.2113, acc: 0.9255
loss: 0.2123, acc: 0.9247
loss: 0.2112, acc: 0.9250
loss: 0.2110, acc: 0.9241
loss: 0.2126, acc: 0.9234
loss: 0.2137, acc: 0.9233
loss: 0.2113, acc: 0.9246
loss: 0.2089, acc: 0.9256
loss: 0.2097, acc: 0.9246
loss: 0.2104, acc: 0.9244
loss: 0.2090, acc: 0.9254
loss: 0.2107, acc: 0.9243
loss: 0.2120, acc: 0.9233
loss: 0.2130, acc: 0.9225
loss: 0.2128, acc: 0.9230
loss: 0.2126, acc: 0.9232
loss: 0.2123, acc: 0.9231
loss: 0.2113, acc: 0.9235
loss: 0.2117, acc: 0.9235
loss: 0.2125, acc: 0.9233
loss: 0.2136, acc: 0.9226
loss: 0.2150, acc: 0.9219
loss: 0.2150, acc: 0.9221
loss: 0.2156, acc: 0.9221
loss: 0.2166, acc: 0.9219
loss: 0.2179, acc: 0.9214
loss: 0.2178, acc: 0.9215
loss: 0.2177, acc: 0.9218
loss: 0.2181, acc: 0.9218
loss: 0.2193, acc: 0.9212
loss: 0.2188, acc: 0.9215
loss: 0.2181, acc: 0.9216
loss: 0.2193, acc: 0.9210
loss: 0.2177, acc: 0.9216
loss: 0.2172, acc: 0.9220
loss: 0.2184, acc: 0.9217
loss: 0.2185, acc: 0.9216
loss: 0.2182, acc: 0.9217
loss: 0.2174, acc: 0.9216
loss: 0.2188, acc: 0.9213
loss: 0.2202, acc: 0.9207
loss: 0.2219, acc: 0.9204
loss: 0.2227, acc: 0.9202
loss: 0.2224, acc: 0.9204
loss: 0.2216, acc: 0.9208
loss: 0.2223, acc: 0.9204
loss: 0.2224, acc: 0.9201
> val_acc: 0.8213, val_f1: 0.8152
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.8213_f1_0.8152_230828-0609.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1650, acc: 0.9437
loss: 0.1420, acc: 0.9500
loss: 0.1422, acc: 0.9521
loss: 0.1338, acc: 0.9609
loss: 0.1289, acc: 0.9625
loss: 0.1376, acc: 0.9594
loss: 0.1289, acc: 0.9616
loss: 0.1207, acc: 0.9648
loss: 0.1161, acc: 0.9667
loss: 0.1178, acc: 0.9669
loss: 0.1127, acc: 0.9682
loss: 0.1087, acc: 0.9688
loss: 0.1106, acc: 0.9688
loss: 0.1137, acc: 0.9674
loss: 0.1103, acc: 0.9683
loss: 0.1087, acc: 0.9688
loss: 0.1106, acc: 0.9676
loss: 0.1100, acc: 0.9677
loss: 0.1091, acc: 0.9678
loss: 0.1068, acc: 0.9684
loss: 0.1087, acc: 0.9676
loss: 0.1055, acc: 0.9685
loss: 0.1065, acc: 0.9682
loss: 0.1078, acc: 0.9677
loss: 0.1074, acc: 0.9675
loss: 0.1100, acc: 0.9671
loss: 0.1098, acc: 0.9669
loss: 0.1111, acc: 0.9665
loss: 0.1107, acc: 0.9662
loss: 0.1128, acc: 0.9656
loss: 0.1115, acc: 0.9663
loss: 0.1154, acc: 0.9654
loss: 0.1166, acc: 0.9648
loss: 0.1164, acc: 0.9649
loss: 0.1156, acc: 0.9648
loss: 0.1174, acc: 0.9644
loss: 0.1166, acc: 0.9647
loss: 0.1150, acc: 0.9651
loss: 0.1139, acc: 0.9652
loss: 0.1153, acc: 0.9648
loss: 0.1172, acc: 0.9642
loss: 0.1176, acc: 0.9637
loss: 0.1179, acc: 0.9634
loss: 0.1181, acc: 0.9634
loss: 0.1183, acc: 0.9631
loss: 0.1184, acc: 0.9628
loss: 0.1184, acc: 0.9629
loss: 0.1169, acc: 0.9634
loss: 0.1162, acc: 0.9635
loss: 0.1165, acc: 0.9636
loss: 0.1175, acc: 0.9634
loss: 0.1185, acc: 0.9632
loss: 0.1192, acc: 0.9630
loss: 0.1187, acc: 0.9631
loss: 0.1198, acc: 0.9628
loss: 0.1209, acc: 0.9622
loss: 0.1206, acc: 0.9622
loss: 0.1216, acc: 0.9620
loss: 0.1214, acc: 0.9621
loss: 0.1219, acc: 0.9616
loss: 0.1212, acc: 0.9618
loss: 0.1215, acc: 0.9616
loss: 0.1211, acc: 0.9618
loss: 0.1214, acc: 0.9611
loss: 0.1206, acc: 0.9615
loss: 0.1204, acc: 0.9614
loss: 0.1209, acc: 0.9608
loss: 0.1208, acc: 0.9609
loss: 0.1209, acc: 0.9611
loss: 0.1212, acc: 0.9607
> val_acc: 0.8161, val_f1: 0.8108
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0365, acc: 0.9812
loss: 0.0586, acc: 0.9812
loss: 0.0711, acc: 0.9750
loss: 0.0747, acc: 0.9750
loss: 0.0700, acc: 0.9762
loss: 0.0669, acc: 0.9771
loss: 0.0786, acc: 0.9741
loss: 0.0770, acc: 0.9758
loss: 0.0714, acc: 0.9778
loss: 0.0738, acc: 0.9769
loss: 0.0774, acc: 0.9761
loss: 0.0796, acc: 0.9750
loss: 0.0805, acc: 0.9750
loss: 0.0826, acc: 0.9741
loss: 0.0805, acc: 0.9746
loss: 0.0801, acc: 0.9742
loss: 0.0778, acc: 0.9750
loss: 0.0794, acc: 0.9743
loss: 0.0791, acc: 0.9747
loss: 0.0786, acc: 0.9744
loss: 0.0788, acc: 0.9741
loss: 0.0787, acc: 0.9747
loss: 0.0795, acc: 0.9742
loss: 0.0791, acc: 0.9742
loss: 0.0796, acc: 0.9742
loss: 0.0788, acc: 0.9745
loss: 0.0781, acc: 0.9748
loss: 0.0795, acc: 0.9741
loss: 0.0805, acc: 0.9737
loss: 0.0815, acc: 0.9733
loss: 0.0809, acc: 0.9732
loss: 0.0811, acc: 0.9732
loss: 0.0810, acc: 0.9733
loss: 0.0807, acc: 0.9733
loss: 0.0796, acc: 0.9738
loss: 0.0796, acc: 0.9738
loss: 0.0802, acc: 0.9738
loss: 0.0789, acc: 0.9743
loss: 0.0812, acc: 0.9739
loss: 0.0808, acc: 0.9739
loss: 0.0802, acc: 0.9742
loss: 0.0798, acc: 0.9746
loss: 0.0806, acc: 0.9746
loss: 0.0816, acc: 0.9744
loss: 0.0808, acc: 0.9746
loss: 0.0806, acc: 0.9742
loss: 0.0801, acc: 0.9745
loss: 0.0809, acc: 0.9745
loss: 0.0812, acc: 0.9745
loss: 0.0817, acc: 0.9744
loss: 0.0809, acc: 0.9745
loss: 0.0813, acc: 0.9744
loss: 0.0809, acc: 0.9745
loss: 0.0811, acc: 0.9740
loss: 0.0831, acc: 0.9735
loss: 0.0836, acc: 0.9737
loss: 0.0842, acc: 0.9734
loss: 0.0834, acc: 0.9737
loss: 0.0830, acc: 0.9738
loss: 0.0828, acc: 0.9739
loss: 0.0829, acc: 0.9737
loss: 0.0834, acc: 0.9733
loss: 0.0831, acc: 0.9733
loss: 0.0832, acc: 0.9731
loss: 0.0836, acc: 0.9727
loss: 0.0837, acc: 0.9724
loss: 0.0848, acc: 0.9721
loss: 0.0843, acc: 0.9722
loss: 0.0841, acc: 0.9724
loss: 0.0834, acc: 0.9727
> val_acc: 0.8131, val_f1: 0.8085
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0546, acc: 0.9875
loss: 0.0332, acc: 0.9938
loss: 0.0274, acc: 0.9958
loss: 0.0474, acc: 0.9891
loss: 0.0489, acc: 0.9875
loss: 0.0439, acc: 0.9885
loss: 0.0435, acc: 0.9884
loss: 0.0404, acc: 0.9891
loss: 0.0449, acc: 0.9882
loss: 0.0412, acc: 0.9894
loss: 0.0392, acc: 0.9898
loss: 0.0395, acc: 0.9896
loss: 0.0375, acc: 0.9904
loss: 0.0402, acc: 0.9897
loss: 0.0390, acc: 0.9900
loss: 0.0378, acc: 0.9902
loss: 0.0385, acc: 0.9897
loss: 0.0379, acc: 0.9892
loss: 0.0385, acc: 0.9885
loss: 0.0378, acc: 0.9884
loss: 0.0374, acc: 0.9881
loss: 0.0408, acc: 0.9875
loss: 0.0403, acc: 0.9878
loss: 0.0393, acc: 0.9880
loss: 0.0402, acc: 0.9875
loss: 0.0420, acc: 0.9868
loss: 0.0425, acc: 0.9863
loss: 0.0441, acc: 0.9857
loss: 0.0439, acc: 0.9858
loss: 0.0441, acc: 0.9858
loss: 0.0446, acc: 0.9859
loss: 0.0453, acc: 0.9857
loss: 0.0460, acc: 0.9852
loss: 0.0473, acc: 0.9849
loss: 0.0466, acc: 0.9852
loss: 0.0459, acc: 0.9854
loss: 0.0468, acc: 0.9853
loss: 0.0483, acc: 0.9847
loss: 0.0481, acc: 0.9845
loss: 0.0490, acc: 0.9839
loss: 0.0488, acc: 0.9841
loss: 0.0496, acc: 0.9838
loss: 0.0504, acc: 0.9836
loss: 0.0495, acc: 0.9839
loss: 0.0497, acc: 0.9838
loss: 0.0492, acc: 0.9838
loss: 0.0493, acc: 0.9836
loss: 0.0491, acc: 0.9836
loss: 0.0486, acc: 0.9838
loss: 0.0490, acc: 0.9836
loss: 0.0488, acc: 0.9836
loss: 0.0496, acc: 0.9831
loss: 0.0489, acc: 0.9833
loss: 0.0493, acc: 0.9830
loss: 0.0490, acc: 0.9832
loss: 0.0488, acc: 0.9833
loss: 0.0495, acc: 0.9829
loss: 0.0498, acc: 0.9825
loss: 0.0495, acc: 0.9826
loss: 0.0497, acc: 0.9826
loss: 0.0493, acc: 0.9828
loss: 0.0496, acc: 0.9826
loss: 0.0500, acc: 0.9824
loss: 0.0509, acc: 0.9820
loss: 0.0511, acc: 0.9818
loss: 0.0509, acc: 0.9819
loss: 0.0524, acc: 0.9814
loss: 0.0525, acc: 0.9814
loss: 0.0523, acc: 0.9816
loss: 0.0537, acc: 0.9813
> val_acc: 0.8161, val_f1: 0.8095
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0270, acc: 0.9875
loss: 0.0268, acc: 0.9906
loss: 0.0345, acc: 0.9875
loss: 0.0305, acc: 0.9891
loss: 0.0321, acc: 0.9875
loss: 0.0342, acc: 0.9875
loss: 0.0317, acc: 0.9884
loss: 0.0350, acc: 0.9852
loss: 0.0334, acc: 0.9854
loss: 0.0330, acc: 0.9862
loss: 0.0356, acc: 0.9864
loss: 0.0363, acc: 0.9870
loss: 0.0442, acc: 0.9851
loss: 0.0427, acc: 0.9853
loss: 0.0430, acc: 0.9850
loss: 0.0423, acc: 0.9855
loss: 0.0427, acc: 0.9857
loss: 0.0411, acc: 0.9861
loss: 0.0428, acc: 0.9855
loss: 0.0413, acc: 0.9859
loss: 0.0410, acc: 0.9863
loss: 0.0399, acc: 0.9869
loss: 0.0389, acc: 0.9872
loss: 0.0381, acc: 0.9875
loss: 0.0380, acc: 0.9878
loss: 0.0370, acc: 0.9880
loss: 0.0379, acc: 0.9880
loss: 0.0390, acc: 0.9875
loss: 0.0383, acc: 0.9877
loss: 0.0383, acc: 0.9877
loss: 0.0399, acc: 0.9873
loss: 0.0391, acc: 0.9877
loss: 0.0381, acc: 0.9881
loss: 0.0399, acc: 0.9881
loss: 0.0394, acc: 0.9882
loss: 0.0398, acc: 0.9880
loss: 0.0390, acc: 0.9883
loss: 0.0395, acc: 0.9880
loss: 0.0402, acc: 0.9875
loss: 0.0414, acc: 0.9872
loss: 0.0410, acc: 0.9872
loss: 0.0407, acc: 0.9872
loss: 0.0409, acc: 0.9872
loss: 0.0416, acc: 0.9871
loss: 0.0411, acc: 0.9872
loss: 0.0411, acc: 0.9872
loss: 0.0407, acc: 0.9874
loss: 0.0405, acc: 0.9874
loss: 0.0400, acc: 0.9875
loss: 0.0406, acc: 0.9872
loss: 0.0411, acc: 0.9870
loss: 0.0413, acc: 0.9868
loss: 0.0428, acc: 0.9863
loss: 0.0426, acc: 0.9865
loss: 0.0424, acc: 0.9864
loss: 0.0421, acc: 0.9865
loss: 0.0416, acc: 0.9867
loss: 0.0414, acc: 0.9869
loss: 0.0423, acc: 0.9865
loss: 0.0430, acc: 0.9864
loss: 0.0439, acc: 0.9861
loss: 0.0436, acc: 0.9863
loss: 0.0441, acc: 0.9861
loss: 0.0440, acc: 0.9860
loss: 0.0451, acc: 0.9855
loss: 0.0447, acc: 0.9857
loss: 0.0449, acc: 0.9856
loss: 0.0457, acc: 0.9854
loss: 0.0455, acc: 0.9854
loss: 0.0458, acc: 0.9852
> val_acc: 0.8131, val_f1: 0.8056
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0244, acc: 0.9875
loss: 0.0407, acc: 0.9812
loss: 0.0331, acc: 0.9854
loss: 0.0307, acc: 0.9859
loss: 0.0283, acc: 0.9888
loss: 0.0269, acc: 0.9896
loss: 0.0245, acc: 0.9902
loss: 0.0225, acc: 0.9914
loss: 0.0255, acc: 0.9910
loss: 0.0252, acc: 0.9906
loss: 0.0273, acc: 0.9903
loss: 0.0273, acc: 0.9901
loss: 0.0277, acc: 0.9904
loss: 0.0262, acc: 0.9911
loss: 0.0275, acc: 0.9908
loss: 0.0269, acc: 0.9910
loss: 0.0257, acc: 0.9915
loss: 0.0278, acc: 0.9910
loss: 0.0298, acc: 0.9905
loss: 0.0290, acc: 0.9906
loss: 0.0283, acc: 0.9908
loss: 0.0293, acc: 0.9906
loss: 0.0291, acc: 0.9905
loss: 0.0282, acc: 0.9909
loss: 0.0280, acc: 0.9908
loss: 0.0288, acc: 0.9906
loss: 0.0286, acc: 0.9903
loss: 0.0291, acc: 0.9904
loss: 0.0283, acc: 0.9907
loss: 0.0282, acc: 0.9908
loss: 0.0289, acc: 0.9903
loss: 0.0299, acc: 0.9900
loss: 0.0297, acc: 0.9902
loss: 0.0291, acc: 0.9904
loss: 0.0299, acc: 0.9902
loss: 0.0299, acc: 0.9903
loss: 0.0301, acc: 0.9902
loss: 0.0306, acc: 0.9903
loss: 0.0305, acc: 0.9902
loss: 0.0309, acc: 0.9902
loss: 0.0311, acc: 0.9899
loss: 0.0316, acc: 0.9897
loss: 0.0328, acc: 0.9891
loss: 0.0326, acc: 0.9891
loss: 0.0331, acc: 0.9886
loss: 0.0337, acc: 0.9883
loss: 0.0344, acc: 0.9880
loss: 0.0341, acc: 0.9880
loss: 0.0344, acc: 0.9879
loss: 0.0347, acc: 0.9879
loss: 0.0342, acc: 0.9881
loss: 0.0341, acc: 0.9880
loss: 0.0338, acc: 0.9882
loss: 0.0338, acc: 0.9881
loss: 0.0335, acc: 0.9882
loss: 0.0337, acc: 0.9882
loss: 0.0340, acc: 0.9880
loss: 0.0348, acc: 0.9880
loss: 0.0345, acc: 0.9882
loss: 0.0343, acc: 0.9883
loss: 0.0350, acc: 0.9882
loss: 0.0350, acc: 0.9881
loss: 0.0355, acc: 0.9880
loss: 0.0351, acc: 0.9882
loss: 0.0351, acc: 0.9881
loss: 0.0352, acc: 0.9882
loss: 0.0359, acc: 0.9880
loss: 0.0366, acc: 0.9878
loss: 0.0365, acc: 0.9878
loss: 0.0364, acc: 0.9878
> val_acc: 0.8086, val_f1: 0.8050
>> early stop.
>> test_acc: 0.8331, test_f1: 0.8286
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2240059904
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0369, acc: 0.4625
loss: 1.0531, acc: 0.4437
loss: 1.0671, acc: 0.4208
loss: 1.0641, acc: 0.4156
loss: 1.0478, acc: 0.4350
loss: 1.0335, acc: 0.4562
loss: 1.0260, acc: 0.4625
loss: 1.0069, acc: 0.4789
loss: 0.9920, acc: 0.4903
loss: 0.9877, acc: 0.4894
loss: 0.9783, acc: 0.4977
loss: 0.9699, acc: 0.5010
loss: 0.9537, acc: 0.5144
loss: 0.9481, acc: 0.5188
loss: 0.9357, acc: 0.5271
loss: 0.9196, acc: 0.5402
loss: 0.9127, acc: 0.5449
loss: 0.9085, acc: 0.5486
loss: 0.9026, acc: 0.5516
loss: 0.8955, acc: 0.5559
loss: 0.8880, acc: 0.5613
loss: 0.8799, acc: 0.5668
loss: 0.8727, acc: 0.5715
loss: 0.8708, acc: 0.5747
loss: 0.8670, acc: 0.5767
loss: 0.8598, acc: 0.5820
loss: 0.8548, acc: 0.5847
loss: 0.8493, acc: 0.5891
loss: 0.8426, acc: 0.5922
loss: 0.8359, acc: 0.5969
loss: 0.8318, acc: 0.5998
loss: 0.8236, acc: 0.6057
loss: 0.8144, acc: 0.6110
loss: 0.8078, acc: 0.6149
loss: 0.8007, acc: 0.6196
loss: 0.7929, acc: 0.6259
loss: 0.7867, acc: 0.6297
loss: 0.7809, acc: 0.6331
loss: 0.7762, acc: 0.6359
loss: 0.7702, acc: 0.6402
loss: 0.7662, acc: 0.6434
loss: 0.7629, acc: 0.6457
loss: 0.7598, acc: 0.6477
loss: 0.7552, acc: 0.6503
loss: 0.7512, acc: 0.6528
loss: 0.7476, acc: 0.6549
loss: 0.7412, acc: 0.6588
loss: 0.7367, acc: 0.6618
loss: 0.7315, acc: 0.6654
loss: 0.7253, acc: 0.6689
loss: 0.7227, acc: 0.6708
loss: 0.7170, acc: 0.6738
loss: 0.7142, acc: 0.6762
loss: 0.7117, acc: 0.6781
loss: 0.7075, acc: 0.6802
loss: 0.7038, acc: 0.6823
loss: 0.7009, acc: 0.6836
loss: 0.6990, acc: 0.6857
loss: 0.6952, acc: 0.6876
loss: 0.6925, acc: 0.6897
loss: 0.6891, acc: 0.6916
loss: 0.6856, acc: 0.6940
loss: 0.6817, acc: 0.6961
loss: 0.6774, acc: 0.6984
loss: 0.6744, acc: 0.7002
loss: 0.6715, acc: 0.7019
loss: 0.6685, acc: 0.7035
loss: 0.6663, acc: 0.7046
loss: 0.6648, acc: 0.7053
loss: 0.6624, acc: 0.7064
> val_acc: 0.8093, val_f1: 0.8054
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.8093_f1_0.8054_230828-0622.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.3271, acc: 0.8750
loss: 0.3413, acc: 0.8625
loss: 0.3322, acc: 0.8771
loss: 0.3199, acc: 0.8859
loss: 0.3270, acc: 0.8850
loss: 0.3275, acc: 0.8812
loss: 0.3404, acc: 0.8732
loss: 0.3393, acc: 0.8750
loss: 0.3444, acc: 0.8715
loss: 0.3443, acc: 0.8719
loss: 0.3475, acc: 0.8693
loss: 0.3474, acc: 0.8672
loss: 0.3387, acc: 0.8716
loss: 0.3371, acc: 0.8714
loss: 0.3418, acc: 0.8704
loss: 0.3394, acc: 0.8711
loss: 0.3371, acc: 0.8732
loss: 0.3424, acc: 0.8712
loss: 0.3436, acc: 0.8717
loss: 0.3446, acc: 0.8719
loss: 0.3462, acc: 0.8717
loss: 0.3477, acc: 0.8699
loss: 0.3515, acc: 0.8685
loss: 0.3537, acc: 0.8677
loss: 0.3529, acc: 0.8685
loss: 0.3534, acc: 0.8688
loss: 0.3571, acc: 0.8669
loss: 0.3571, acc: 0.8663
loss: 0.3572, acc: 0.8659
loss: 0.3598, acc: 0.8648
loss: 0.3660, acc: 0.8621
loss: 0.3662, acc: 0.8617
loss: 0.3648, acc: 0.8627
loss: 0.3652, acc: 0.8621
loss: 0.3671, acc: 0.8612
loss: 0.3675, acc: 0.8616
loss: 0.3659, acc: 0.8623
loss: 0.3661, acc: 0.8617
loss: 0.3638, acc: 0.8622
loss: 0.3633, acc: 0.8620
loss: 0.3638, acc: 0.8619
loss: 0.3650, acc: 0.8615
loss: 0.3644, acc: 0.8610
loss: 0.3628, acc: 0.8619
loss: 0.3632, acc: 0.8615
loss: 0.3642, acc: 0.8606
loss: 0.3620, acc: 0.8614
loss: 0.3615, acc: 0.8615
loss: 0.3608, acc: 0.8617
loss: 0.3602, acc: 0.8624
loss: 0.3601, acc: 0.8621
loss: 0.3617, acc: 0.8614
loss: 0.3604, acc: 0.8617
loss: 0.3617, acc: 0.8613
loss: 0.3618, acc: 0.8616
loss: 0.3608, acc: 0.8619
loss: 0.3607, acc: 0.8623
loss: 0.3590, acc: 0.8629
loss: 0.3602, acc: 0.8623
loss: 0.3607, acc: 0.8624
loss: 0.3617, acc: 0.8618
loss: 0.3618, acc: 0.8617
loss: 0.3618, acc: 0.8617
loss: 0.3618, acc: 0.8621
loss: 0.3617, acc: 0.8621
loss: 0.3611, acc: 0.8625
loss: 0.3617, acc: 0.8625
loss: 0.3605, acc: 0.8628
loss: 0.3611, acc: 0.8627
loss: 0.3609, acc: 0.8628
> val_acc: 0.8213, val_f1: 0.8171
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.8213_f1_0.8171_230828-0624.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.1723, acc: 0.9500
loss: 0.1816, acc: 0.9469
loss: 0.1776, acc: 0.9437
loss: 0.1766, acc: 0.9453
loss: 0.1761, acc: 0.9450
loss: 0.1704, acc: 0.9458
loss: 0.1709, acc: 0.9446
loss: 0.1692, acc: 0.9461
loss: 0.1691, acc: 0.9465
loss: 0.1852, acc: 0.9413
loss: 0.1858, acc: 0.9392
loss: 0.1815, acc: 0.9401
loss: 0.1797, acc: 0.9399
loss: 0.1807, acc: 0.9393
loss: 0.1870, acc: 0.9363
loss: 0.1908, acc: 0.9352
loss: 0.1924, acc: 0.9349
loss: 0.1919, acc: 0.9351
loss: 0.1942, acc: 0.9345
loss: 0.1960, acc: 0.9347
loss: 0.1987, acc: 0.9333
loss: 0.2002, acc: 0.9330
loss: 0.2004, acc: 0.9329
loss: 0.1994, acc: 0.9333
loss: 0.1978, acc: 0.9340
loss: 0.1975, acc: 0.9344
loss: 0.1975, acc: 0.9352
loss: 0.1991, acc: 0.9342
loss: 0.1974, acc: 0.9345
loss: 0.1951, acc: 0.9354
loss: 0.1968, acc: 0.9353
loss: 0.1937, acc: 0.9359
loss: 0.1933, acc: 0.9366
loss: 0.1924, acc: 0.9364
loss: 0.1905, acc: 0.9371
loss: 0.1898, acc: 0.9361
loss: 0.1912, acc: 0.9356
loss: 0.1936, acc: 0.9352
loss: 0.1948, acc: 0.9346
loss: 0.1973, acc: 0.9334
loss: 0.1971, acc: 0.9337
loss: 0.1983, acc: 0.9338
loss: 0.1988, acc: 0.9336
loss: 0.1996, acc: 0.9331
loss: 0.1996, acc: 0.9329
loss: 0.1983, acc: 0.9338
loss: 0.1997, acc: 0.9324
loss: 0.2002, acc: 0.9320
loss: 0.2000, acc: 0.9320
loss: 0.1993, acc: 0.9325
loss: 0.2008, acc: 0.9322
loss: 0.1999, acc: 0.9327
loss: 0.2005, acc: 0.9320
loss: 0.2001, acc: 0.9319
loss: 0.2000, acc: 0.9316
loss: 0.1987, acc: 0.9320
loss: 0.1986, acc: 0.9319
loss: 0.1988, acc: 0.9320
loss: 0.1987, acc: 0.9320
loss: 0.2004, acc: 0.9316
loss: 0.2023, acc: 0.9307
loss: 0.2029, acc: 0.9305
loss: 0.2023, acc: 0.9306
loss: 0.2032, acc: 0.9304
loss: 0.2028, acc: 0.9305
loss: 0.2015, acc: 0.9309
loss: 0.2010, acc: 0.9310
loss: 0.2005, acc: 0.9308
loss: 0.2011, acc: 0.9304
loss: 0.2024, acc: 0.9301
> val_acc: 0.8198, val_f1: 0.8135
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1823, acc: 0.9437
loss: 0.1301, acc: 0.9625
loss: 0.1127, acc: 0.9625
loss: 0.1018, acc: 0.9656
loss: 0.1203, acc: 0.9600
loss: 0.1123, acc: 0.9615
loss: 0.1133, acc: 0.9589
loss: 0.1146, acc: 0.9586
loss: 0.1185, acc: 0.9569
loss: 0.1144, acc: 0.9587
loss: 0.1155, acc: 0.9585
loss: 0.1149, acc: 0.9589
loss: 0.1166, acc: 0.9587
loss: 0.1167, acc: 0.9585
loss: 0.1137, acc: 0.9592
loss: 0.1144, acc: 0.9590
loss: 0.1123, acc: 0.9596
loss: 0.1112, acc: 0.9597
loss: 0.1107, acc: 0.9599
loss: 0.1138, acc: 0.9591
loss: 0.1184, acc: 0.9583
loss: 0.1186, acc: 0.9585
loss: 0.1169, acc: 0.9595
loss: 0.1136, acc: 0.9609
loss: 0.1122, acc: 0.9613
loss: 0.1089, acc: 0.9625
loss: 0.1090, acc: 0.9620
loss: 0.1093, acc: 0.9616
loss: 0.1135, acc: 0.9599
loss: 0.1117, acc: 0.9604
loss: 0.1121, acc: 0.9603
loss: 0.1137, acc: 0.9590
loss: 0.1154, acc: 0.9593
loss: 0.1148, acc: 0.9599
loss: 0.1133, acc: 0.9607
loss: 0.1121, acc: 0.9611
loss: 0.1119, acc: 0.9615
loss: 0.1130, acc: 0.9609
loss: 0.1132, acc: 0.9607
loss: 0.1130, acc: 0.9606
loss: 0.1137, acc: 0.9607
loss: 0.1127, acc: 0.9612
loss: 0.1118, acc: 0.9615
loss: 0.1121, acc: 0.9615
loss: 0.1112, acc: 0.9617
loss: 0.1121, acc: 0.9615
loss: 0.1122, acc: 0.9613
loss: 0.1115, acc: 0.9616
loss: 0.1124, acc: 0.9615
loss: 0.1128, acc: 0.9615
loss: 0.1125, acc: 0.9616
loss: 0.1135, acc: 0.9613
loss: 0.1132, acc: 0.9617
loss: 0.1124, acc: 0.9619
loss: 0.1123, acc: 0.9620
loss: 0.1143, acc: 0.9617
loss: 0.1164, acc: 0.9612
loss: 0.1160, acc: 0.9614
loss: 0.1158, acc: 0.9614
loss: 0.1155, acc: 0.9616
loss: 0.1161, acc: 0.9615
loss: 0.1155, acc: 0.9618
loss: 0.1156, acc: 0.9618
loss: 0.1156, acc: 0.9616
loss: 0.1150, acc: 0.9619
loss: 0.1166, acc: 0.9612
loss: 0.1170, acc: 0.9610
loss: 0.1171, acc: 0.9608
loss: 0.1174, acc: 0.9606
loss: 0.1178, acc: 0.9604
> val_acc: 0.8191, val_f1: 0.8142
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.0831, acc: 0.9812
loss: 0.1146, acc: 0.9688
loss: 0.1114, acc: 0.9646
loss: 0.1125, acc: 0.9625
loss: 0.0999, acc: 0.9675
loss: 0.1009, acc: 0.9667
loss: 0.0949, acc: 0.9688
loss: 0.0914, acc: 0.9703
loss: 0.0892, acc: 0.9708
loss: 0.0893, acc: 0.9712
loss: 0.0837, acc: 0.9739
loss: 0.0801, acc: 0.9745
loss: 0.0861, acc: 0.9740
loss: 0.0860, acc: 0.9737
loss: 0.0844, acc: 0.9746
loss: 0.0827, acc: 0.9754
loss: 0.0831, acc: 0.9750
loss: 0.0830, acc: 0.9757
loss: 0.0812, acc: 0.9760
loss: 0.0797, acc: 0.9766
loss: 0.0796, acc: 0.9768
loss: 0.0784, acc: 0.9770
loss: 0.0764, acc: 0.9774
loss: 0.0770, acc: 0.9771
loss: 0.0754, acc: 0.9772
loss: 0.0751, acc: 0.9776
loss: 0.0736, acc: 0.9782
loss: 0.0742, acc: 0.9783
loss: 0.0770, acc: 0.9772
loss: 0.0758, acc: 0.9777
loss: 0.0750, acc: 0.9780
loss: 0.0736, acc: 0.9785
loss: 0.0749, acc: 0.9780
loss: 0.0736, acc: 0.9783
loss: 0.0742, acc: 0.9780
loss: 0.0743, acc: 0.9780
loss: 0.0738, acc: 0.9782
loss: 0.0733, acc: 0.9781
loss: 0.0723, acc: 0.9784
loss: 0.0719, acc: 0.9781
loss: 0.0736, acc: 0.9773
loss: 0.0757, acc: 0.9768
loss: 0.0748, acc: 0.9772
loss: 0.0771, acc: 0.9767
loss: 0.0764, acc: 0.9767
loss: 0.0766, acc: 0.9765
loss: 0.0761, acc: 0.9767
loss: 0.0757, acc: 0.9767
loss: 0.0756, acc: 0.9769
loss: 0.0746, acc: 0.9771
loss: 0.0749, acc: 0.9772
loss: 0.0755, acc: 0.9767
loss: 0.0767, acc: 0.9764
loss: 0.0766, acc: 0.9765
loss: 0.0775, acc: 0.9761
loss: 0.0778, acc: 0.9761
loss: 0.0784, acc: 0.9759
loss: 0.0796, acc: 0.9758
loss: 0.0794, acc: 0.9757
loss: 0.0793, acc: 0.9757
loss: 0.0806, acc: 0.9754
loss: 0.0798, acc: 0.9756
loss: 0.0792, acc: 0.9758
loss: 0.0785, acc: 0.9760
loss: 0.0796, acc: 0.9757
loss: 0.0789, acc: 0.9759
loss: 0.0799, acc: 0.9757
loss: 0.0807, acc: 0.9756
loss: 0.0806, acc: 0.9755
loss: 0.0802, acc: 0.9756
> val_acc: 0.8153, val_f1: 0.8106
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0686, acc: 0.9688
loss: 0.0683, acc: 0.9750
loss: 0.0555, acc: 0.9812
loss: 0.0527, acc: 0.9844
loss: 0.0445, acc: 0.9875
loss: 0.0409, acc: 0.9875
loss: 0.0410, acc: 0.9884
loss: 0.0382, acc: 0.9891
loss: 0.0358, acc: 0.9903
loss: 0.0355, acc: 0.9900
loss: 0.0358, acc: 0.9898
loss: 0.0355, acc: 0.9896
loss: 0.0350, acc: 0.9894
loss: 0.0384, acc: 0.9884
loss: 0.0407, acc: 0.9875
loss: 0.0387, acc: 0.9883
loss: 0.0414, acc: 0.9879
loss: 0.0399, acc: 0.9885
loss: 0.0401, acc: 0.9885
loss: 0.0395, acc: 0.9884
loss: 0.0410, acc: 0.9875
loss: 0.0408, acc: 0.9875
loss: 0.0417, acc: 0.9872
loss: 0.0414, acc: 0.9872
loss: 0.0414, acc: 0.9872
loss: 0.0407, acc: 0.9873
loss: 0.0422, acc: 0.9868
loss: 0.0419, acc: 0.9868
loss: 0.0421, acc: 0.9864
loss: 0.0426, acc: 0.9862
loss: 0.0447, acc: 0.9859
loss: 0.0448, acc: 0.9855
loss: 0.0441, acc: 0.9858
loss: 0.0447, acc: 0.9857
loss: 0.0453, acc: 0.9850
loss: 0.0454, acc: 0.9852
loss: 0.0446, acc: 0.9855
loss: 0.0448, acc: 0.9852
loss: 0.0442, acc: 0.9854
loss: 0.0445, acc: 0.9853
loss: 0.0443, acc: 0.9855
loss: 0.0444, acc: 0.9854
loss: 0.0468, acc: 0.9849
loss: 0.0464, acc: 0.9849
loss: 0.0463, acc: 0.9847
loss: 0.0476, acc: 0.9845
loss: 0.0479, acc: 0.9842
loss: 0.0482, acc: 0.9840
loss: 0.0497, acc: 0.9835
loss: 0.0496, acc: 0.9836
loss: 0.0496, acc: 0.9838
loss: 0.0502, acc: 0.9834
loss: 0.0512, acc: 0.9831
loss: 0.0512, acc: 0.9831
loss: 0.0509, acc: 0.9832
loss: 0.0509, acc: 0.9833
loss: 0.0504, acc: 0.9834
loss: 0.0504, acc: 0.9835
loss: 0.0499, acc: 0.9837
loss: 0.0498, acc: 0.9835
loss: 0.0494, acc: 0.9835
loss: 0.0496, acc: 0.9834
loss: 0.0504, acc: 0.9831
loss: 0.0502, acc: 0.9831
loss: 0.0504, acc: 0.9832
loss: 0.0511, acc: 0.9830
loss: 0.0515, acc: 0.9829
loss: 0.0515, acc: 0.9830
loss: 0.0519, acc: 0.9828
loss: 0.0519, acc: 0.9829
> val_acc: 0.8153, val_f1: 0.8107
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0437, acc: 0.9875
loss: 0.0474, acc: 0.9875
loss: 0.0452, acc: 0.9875
loss: 0.0367, acc: 0.9906
loss: 0.0419, acc: 0.9862
loss: 0.0437, acc: 0.9854
loss: 0.0383, acc: 0.9875
loss: 0.0370, acc: 0.9883
loss: 0.0359, acc: 0.9882
loss: 0.0354, acc: 0.9888
loss: 0.0326, acc: 0.9898
loss: 0.0326, acc: 0.9896
loss: 0.0315, acc: 0.9899
loss: 0.0298, acc: 0.9906
loss: 0.0303, acc: 0.9904
loss: 0.0290, acc: 0.9910
loss: 0.0313, acc: 0.9908
loss: 0.0341, acc: 0.9899
loss: 0.0338, acc: 0.9898
loss: 0.0362, acc: 0.9888
loss: 0.0364, acc: 0.9887
loss: 0.0418, acc: 0.9875
loss: 0.0410, acc: 0.9875
loss: 0.0405, acc: 0.9875
loss: 0.0422, acc: 0.9868
loss: 0.0422, acc: 0.9863
loss: 0.0419, acc: 0.9861
loss: 0.0437, acc: 0.9862
loss: 0.0434, acc: 0.9864
loss: 0.0423, acc: 0.9869
loss: 0.0412, acc: 0.9873
loss: 0.0421, acc: 0.9873
loss: 0.0433, acc: 0.9871
loss: 0.0445, acc: 0.9869
loss: 0.0454, acc: 0.9866
loss: 0.0458, acc: 0.9863
loss: 0.0455, acc: 0.9863
loss: 0.0451, acc: 0.9863
loss: 0.0450, acc: 0.9865
loss: 0.0452, acc: 0.9864
loss: 0.0452, acc: 0.9864
loss: 0.0455, acc: 0.9865
loss: 0.0459, acc: 0.9863
loss: 0.0459, acc: 0.9861
loss: 0.0458, acc: 0.9860
loss: 0.0453, acc: 0.9860
loss: 0.0459, acc: 0.9856
loss: 0.0458, acc: 0.9858
loss: 0.0458, acc: 0.9857
loss: 0.0455, acc: 0.9859
loss: 0.0455, acc: 0.9860
loss: 0.0449, acc: 0.9862
loss: 0.0452, acc: 0.9860
loss: 0.0446, acc: 0.9862
loss: 0.0445, acc: 0.9862
loss: 0.0446, acc: 0.9860
loss: 0.0443, acc: 0.9862
loss: 0.0455, acc: 0.9857
loss: 0.0461, acc: 0.9856
loss: 0.0464, acc: 0.9854
loss: 0.0461, acc: 0.9853
loss: 0.0482, acc: 0.9848
loss: 0.0497, acc: 0.9845
loss: 0.0502, acc: 0.9845
loss: 0.0505, acc: 0.9844
loss: 0.0506, acc: 0.9842
loss: 0.0508, acc: 0.9841
loss: 0.0512, acc: 0.9841
loss: 0.0516, acc: 0.9840
loss: 0.0515, acc: 0.9840
> val_acc: 0.8191, val_f1: 0.8133
>> early stop.
>> test_acc: 0.8286, test_f1: 0.8218
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2237307392
> n_trainable_params: 109484547, n_nontrainable_params: 0
> training arguments:
>>> model_name: bert
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0796, acc: 0.4000
loss: 1.0634, acc: 0.4156
loss: 1.0704, acc: 0.4250
loss: 1.0617, acc: 0.4328
loss: 1.0570, acc: 0.4400
loss: 1.0510, acc: 0.4458
loss: 1.0370, acc: 0.4554
loss: 1.0314, acc: 0.4617
loss: 1.0152, acc: 0.4750
loss: 1.0039, acc: 0.4888
loss: 0.9927, acc: 0.4960
loss: 0.9881, acc: 0.5031
loss: 0.9835, acc: 0.5067
loss: 0.9746, acc: 0.5161
loss: 0.9663, acc: 0.5225
loss: 0.9560, acc: 0.5297
loss: 0.9475, acc: 0.5360
loss: 0.9395, acc: 0.5427
loss: 0.9311, acc: 0.5487
loss: 0.9231, acc: 0.5531
loss: 0.9149, acc: 0.5586
loss: 0.9082, acc: 0.5631
loss: 0.8990, acc: 0.5685
loss: 0.8869, acc: 0.5750
loss: 0.8813, acc: 0.5800
loss: 0.8743, acc: 0.5844
loss: 0.8747, acc: 0.5843
loss: 0.8683, acc: 0.5884
loss: 0.8630, acc: 0.5907
loss: 0.8553, acc: 0.5958
loss: 0.8473, acc: 0.6004
loss: 0.8421, acc: 0.6027
loss: 0.8330, acc: 0.6076
loss: 0.8231, acc: 0.6132
loss: 0.8155, acc: 0.6186
loss: 0.8094, acc: 0.6229
loss: 0.8028, acc: 0.6275
loss: 0.7953, acc: 0.6321
loss: 0.7902, acc: 0.6345
loss: 0.7838, acc: 0.6380
loss: 0.7789, acc: 0.6405
loss: 0.7748, acc: 0.6436
loss: 0.7688, acc: 0.6471
loss: 0.7636, acc: 0.6500
loss: 0.7584, acc: 0.6528
loss: 0.7549, acc: 0.6549
loss: 0.7511, acc: 0.6569
loss: 0.7474, acc: 0.6591
loss: 0.7434, acc: 0.6611
loss: 0.7400, acc: 0.6633
loss: 0.7384, acc: 0.6641
loss: 0.7348, acc: 0.6665
loss: 0.7307, acc: 0.6687
loss: 0.7298, acc: 0.6698
loss: 0.7257, acc: 0.6719
loss: 0.7220, acc: 0.6740
loss: 0.7194, acc: 0.6758
loss: 0.7165, acc: 0.6773
loss: 0.7131, acc: 0.6796
loss: 0.7109, acc: 0.6808
loss: 0.7085, acc: 0.6823
loss: 0.7033, acc: 0.6846
loss: 0.6996, acc: 0.6861
loss: 0.6973, acc: 0.6877
loss: 0.6949, acc: 0.6894
loss: 0.6927, acc: 0.6909
loss: 0.6894, acc: 0.6922
loss: 0.6862, acc: 0.6938
loss: 0.6845, acc: 0.6950
loss: 0.6812, acc: 0.6969
> val_acc: 0.7800, val_f1: 0.7790
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.78_f1_0.779_230828-0637.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4616, acc: 0.8063
loss: 0.4247, acc: 0.8250
loss: 0.4078, acc: 0.8417
loss: 0.3931, acc: 0.8500
loss: 0.3987, acc: 0.8500
loss: 0.3869, acc: 0.8542
loss: 0.3875, acc: 0.8527
loss: 0.3705, acc: 0.8578
loss: 0.3658, acc: 0.8597
loss: 0.3582, acc: 0.8644
loss: 0.3672, acc: 0.8591
loss: 0.3710, acc: 0.8568
loss: 0.3718, acc: 0.8562
loss: 0.3718, acc: 0.8562
loss: 0.3707, acc: 0.8583
loss: 0.3780, acc: 0.8566
loss: 0.3749, acc: 0.8581
loss: 0.3714, acc: 0.8580
loss: 0.3711, acc: 0.8586
loss: 0.3705, acc: 0.8588
loss: 0.3731, acc: 0.8580
loss: 0.3700, acc: 0.8585
loss: 0.3725, acc: 0.8576
loss: 0.3757, acc: 0.8549
loss: 0.3793, acc: 0.8525
loss: 0.3790, acc: 0.8531
loss: 0.3780, acc: 0.8539
loss: 0.3743, acc: 0.8558
loss: 0.3730, acc: 0.8560
loss: 0.3709, acc: 0.8571
loss: 0.3725, acc: 0.8567
loss: 0.3709, acc: 0.8576
loss: 0.3680, acc: 0.8593
loss: 0.3694, acc: 0.8586
loss: 0.3726, acc: 0.8566
loss: 0.3739, acc: 0.8566
loss: 0.3725, acc: 0.8573
loss: 0.3743, acc: 0.8561
loss: 0.3748, acc: 0.8558
loss: 0.3736, acc: 0.8562
loss: 0.3746, acc: 0.8559
loss: 0.3757, acc: 0.8562
loss: 0.3744, acc: 0.8573
loss: 0.3747, acc: 0.8574
loss: 0.3750, acc: 0.8578
loss: 0.3730, acc: 0.8583
loss: 0.3717, acc: 0.8585
loss: 0.3743, acc: 0.8576
loss: 0.3732, acc: 0.8578
loss: 0.3745, acc: 0.8572
loss: 0.3748, acc: 0.8574
loss: 0.3758, acc: 0.8569
loss: 0.3752, acc: 0.8571
loss: 0.3758, acc: 0.8569
loss: 0.3762, acc: 0.8564
loss: 0.3759, acc: 0.8560
loss: 0.3756, acc: 0.8562
loss: 0.3751, acc: 0.8560
loss: 0.3739, acc: 0.8562
loss: 0.3749, acc: 0.8560
loss: 0.3753, acc: 0.8556
loss: 0.3750, acc: 0.8557
loss: 0.3743, acc: 0.8561
loss: 0.3742, acc: 0.8562
loss: 0.3731, acc: 0.8564
loss: 0.3713, acc: 0.8572
loss: 0.3716, acc: 0.8575
loss: 0.3728, acc: 0.8571
loss: 0.3718, acc: 0.8575
loss: 0.3714, acc: 0.8578
> val_acc: 0.8296, val_f1: 0.8257
>> saved: /media/b115/Backup/NLP/bert/mams/acc_0.8296_f1_0.8257_230828-0639.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2249, acc: 0.9250
loss: 0.2005, acc: 0.9437
loss: 0.2238, acc: 0.9292
loss: 0.2181, acc: 0.9297
loss: 0.2075, acc: 0.9325
loss: 0.2091, acc: 0.9281
loss: 0.2129, acc: 0.9268
loss: 0.2215, acc: 0.9234
loss: 0.2258, acc: 0.9181
loss: 0.2331, acc: 0.9150
loss: 0.2328, acc: 0.9153
loss: 0.2267, acc: 0.9198
loss: 0.2227, acc: 0.9221
loss: 0.2170, acc: 0.9246
loss: 0.2108, acc: 0.9279
loss: 0.2073, acc: 0.9293
loss: 0.2076, acc: 0.9298
loss: 0.2171, acc: 0.9281
loss: 0.2186, acc: 0.9273
loss: 0.2212, acc: 0.9269
loss: 0.2186, acc: 0.9283
loss: 0.2184, acc: 0.9270
loss: 0.2181, acc: 0.9264
loss: 0.2185, acc: 0.9266
loss: 0.2173, acc: 0.9267
loss: 0.2177, acc: 0.9264
loss: 0.2176, acc: 0.9264
loss: 0.2164, acc: 0.9272
loss: 0.2158, acc: 0.9272
loss: 0.2146, acc: 0.9279
loss: 0.2147, acc: 0.9276
loss: 0.2164, acc: 0.9273
loss: 0.2157, acc: 0.9275
loss: 0.2142, acc: 0.9283
loss: 0.2125, acc: 0.9287
loss: 0.2135, acc: 0.9286
loss: 0.2135, acc: 0.9279
loss: 0.2143, acc: 0.9271
loss: 0.2136, acc: 0.9272
loss: 0.2124, acc: 0.9272
loss: 0.2119, acc: 0.9268
loss: 0.2134, acc: 0.9260
loss: 0.2130, acc: 0.9259
loss: 0.2154, acc: 0.9254
loss: 0.2157, acc: 0.9253
loss: 0.2156, acc: 0.9253
loss: 0.2168, acc: 0.9242
loss: 0.2158, acc: 0.9243
loss: 0.2158, acc: 0.9237
loss: 0.2157, acc: 0.9237
loss: 0.2156, acc: 0.9237
loss: 0.2158, acc: 0.9237
loss: 0.2144, acc: 0.9244
loss: 0.2141, acc: 0.9245
loss: 0.2157, acc: 0.9239
loss: 0.2155, acc: 0.9241
loss: 0.2166, acc: 0.9236
loss: 0.2180, acc: 0.9230
loss: 0.2190, acc: 0.9227
loss: 0.2186, acc: 0.9225
loss: 0.2187, acc: 0.9226
loss: 0.2195, acc: 0.9221
loss: 0.2210, acc: 0.9212
loss: 0.2217, acc: 0.9209
loss: 0.2231, acc: 0.9205
loss: 0.2241, acc: 0.9200
loss: 0.2239, acc: 0.9198
loss: 0.2233, acc: 0.9200
loss: 0.2228, acc: 0.9202
loss: 0.2222, acc: 0.9205
> val_acc: 0.8296, val_f1: 0.8221
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.0887, acc: 0.9812
loss: 0.1004, acc: 0.9688
loss: 0.1010, acc: 0.9667
loss: 0.0995, acc: 0.9672
loss: 0.0892, acc: 0.9700
loss: 0.1083, acc: 0.9656
loss: 0.1092, acc: 0.9670
loss: 0.1120, acc: 0.9656
loss: 0.1107, acc: 0.9653
loss: 0.1150, acc: 0.9637
loss: 0.1160, acc: 0.9631
loss: 0.1144, acc: 0.9630
loss: 0.1114, acc: 0.9644
loss: 0.1138, acc: 0.9643
loss: 0.1143, acc: 0.9637
loss: 0.1185, acc: 0.9637
loss: 0.1197, acc: 0.9629
loss: 0.1211, acc: 0.9628
loss: 0.1185, acc: 0.9635
loss: 0.1179, acc: 0.9628
loss: 0.1143, acc: 0.9640
loss: 0.1140, acc: 0.9645
loss: 0.1130, acc: 0.9644
loss: 0.1141, acc: 0.9641
loss: 0.1178, acc: 0.9630
loss: 0.1178, acc: 0.9625
loss: 0.1186, acc: 0.9616
loss: 0.1206, acc: 0.9609
loss: 0.1200, acc: 0.9610
loss: 0.1188, acc: 0.9610
loss: 0.1203, acc: 0.9607
loss: 0.1216, acc: 0.9600
loss: 0.1210, acc: 0.9602
loss: 0.1204, acc: 0.9607
loss: 0.1212, acc: 0.9602
loss: 0.1208, acc: 0.9601
loss: 0.1236, acc: 0.9588
loss: 0.1229, acc: 0.9592
loss: 0.1234, acc: 0.9591
loss: 0.1234, acc: 0.9594
loss: 0.1244, acc: 0.9595
loss: 0.1240, acc: 0.9600
loss: 0.1238, acc: 0.9599
loss: 0.1237, acc: 0.9598
loss: 0.1235, acc: 0.9600
loss: 0.1229, acc: 0.9602
loss: 0.1236, acc: 0.9597
loss: 0.1238, acc: 0.9595
loss: 0.1223, acc: 0.9599
loss: 0.1213, acc: 0.9604
loss: 0.1217, acc: 0.9605
loss: 0.1226, acc: 0.9601
loss: 0.1242, acc: 0.9596
loss: 0.1247, acc: 0.9595
loss: 0.1252, acc: 0.9593
loss: 0.1244, acc: 0.9596
loss: 0.1249, acc: 0.9595
loss: 0.1261, acc: 0.9594
loss: 0.1255, acc: 0.9596
loss: 0.1256, acc: 0.9596
loss: 0.1258, acc: 0.9597
loss: 0.1267, acc: 0.9597
loss: 0.1272, acc: 0.9592
loss: 0.1269, acc: 0.9592
loss: 0.1263, acc: 0.9592
loss: 0.1263, acc: 0.9595
loss: 0.1270, acc: 0.9592
loss: 0.1261, acc: 0.9595
loss: 0.1261, acc: 0.9592
loss: 0.1263, acc: 0.9593
> val_acc: 0.8266, val_f1: 0.8211
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1166, acc: 0.9625
loss: 0.0959, acc: 0.9656
loss: 0.0839, acc: 0.9708
loss: 0.0745, acc: 0.9750
loss: 0.0703, acc: 0.9775
loss: 0.0629, acc: 0.9792
loss: 0.0718, acc: 0.9777
loss: 0.0673, acc: 0.9797
loss: 0.0645, acc: 0.9812
loss: 0.0617, acc: 0.9819
loss: 0.0614, acc: 0.9812
loss: 0.0668, acc: 0.9786
loss: 0.0669, acc: 0.9788
loss: 0.0675, acc: 0.9777
loss: 0.0679, acc: 0.9771
loss: 0.0667, acc: 0.9777
loss: 0.0640, acc: 0.9787
loss: 0.0648, acc: 0.9785
loss: 0.0660, acc: 0.9773
loss: 0.0634, acc: 0.9784
loss: 0.0655, acc: 0.9780
loss: 0.0694, acc: 0.9770
loss: 0.0715, acc: 0.9769
loss: 0.0749, acc: 0.9766
loss: 0.0761, acc: 0.9765
loss: 0.0761, acc: 0.9764
loss: 0.0746, acc: 0.9769
loss: 0.0731, acc: 0.9775
loss: 0.0766, acc: 0.9767
loss: 0.0765, acc: 0.9771
loss: 0.0755, acc: 0.9772
loss: 0.0776, acc: 0.9768
loss: 0.0765, acc: 0.9771
loss: 0.0778, acc: 0.9767
loss: 0.0778, acc: 0.9766
loss: 0.0781, acc: 0.9766
loss: 0.0789, acc: 0.9764
loss: 0.0780, acc: 0.9766
loss: 0.0777, acc: 0.9769
loss: 0.0774, acc: 0.9770
loss: 0.0784, acc: 0.9770
loss: 0.0778, acc: 0.9772
loss: 0.0787, acc: 0.9770
loss: 0.0779, acc: 0.9774
loss: 0.0791, acc: 0.9767
loss: 0.0790, acc: 0.9768
loss: 0.0789, acc: 0.9769
loss: 0.0782, acc: 0.9771
loss: 0.0772, acc: 0.9774
loss: 0.0777, acc: 0.9774
loss: 0.0771, acc: 0.9776
loss: 0.0774, acc: 0.9775
loss: 0.0788, acc: 0.9769
loss: 0.0792, acc: 0.9769
loss: 0.0786, acc: 0.9769
loss: 0.0801, acc: 0.9766
loss: 0.0795, acc: 0.9766
loss: 0.0789, acc: 0.9768
loss: 0.0794, acc: 0.9767
loss: 0.0793, acc: 0.9768
loss: 0.0797, acc: 0.9766
loss: 0.0806, acc: 0.9764
loss: 0.0802, acc: 0.9765
loss: 0.0807, acc: 0.9764
loss: 0.0814, acc: 0.9762
loss: 0.0818, acc: 0.9759
loss: 0.0820, acc: 0.9759
loss: 0.0819, acc: 0.9760
loss: 0.0817, acc: 0.9762
loss: 0.0818, acc: 0.9761
> val_acc: 0.8251, val_f1: 0.8196
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0676, acc: 0.9812
loss: 0.0473, acc: 0.9844
loss: 0.0523, acc: 0.9792
loss: 0.0542, acc: 0.9812
loss: 0.0584, acc: 0.9812
loss: 0.0614, acc: 0.9802
loss: 0.0545, acc: 0.9830
loss: 0.0517, acc: 0.9844
loss: 0.0486, acc: 0.9854
loss: 0.0525, acc: 0.9844
loss: 0.0495, acc: 0.9852
loss: 0.0492, acc: 0.9849
loss: 0.0465, acc: 0.9861
loss: 0.0444, acc: 0.9866
loss: 0.0432, acc: 0.9871
loss: 0.0432, acc: 0.9863
loss: 0.0456, acc: 0.9853
loss: 0.0451, acc: 0.9854
loss: 0.0442, acc: 0.9852
loss: 0.0432, acc: 0.9856
loss: 0.0434, acc: 0.9854
loss: 0.0434, acc: 0.9858
loss: 0.0428, acc: 0.9859
loss: 0.0427, acc: 0.9862
loss: 0.0444, acc: 0.9855
loss: 0.0440, acc: 0.9853
loss: 0.0435, acc: 0.9854
loss: 0.0434, acc: 0.9850
loss: 0.0434, acc: 0.9849
loss: 0.0446, acc: 0.9844
loss: 0.0447, acc: 0.9847
loss: 0.0456, acc: 0.9846
loss: 0.0462, acc: 0.9847
loss: 0.0463, acc: 0.9844
loss: 0.0463, acc: 0.9843
loss: 0.0475, acc: 0.9842
loss: 0.0489, acc: 0.9836
loss: 0.0493, acc: 0.9836
loss: 0.0512, acc: 0.9832
loss: 0.0508, acc: 0.9833
loss: 0.0503, acc: 0.9832
loss: 0.0510, acc: 0.9830
loss: 0.0514, acc: 0.9831
loss: 0.0517, acc: 0.9832
loss: 0.0522, acc: 0.9829
loss: 0.0532, acc: 0.9825
loss: 0.0547, acc: 0.9822
loss: 0.0552, acc: 0.9818
loss: 0.0565, acc: 0.9812
loss: 0.0582, acc: 0.9809
loss: 0.0600, acc: 0.9803
loss: 0.0606, acc: 0.9803
loss: 0.0605, acc: 0.9804
loss: 0.0607, acc: 0.9803
loss: 0.0610, acc: 0.9803
loss: 0.0603, acc: 0.9806
loss: 0.0613, acc: 0.9804
loss: 0.0616, acc: 0.9803
loss: 0.0621, acc: 0.9799
loss: 0.0625, acc: 0.9796
loss: 0.0633, acc: 0.9792
loss: 0.0631, acc: 0.9792
loss: 0.0630, acc: 0.9793
loss: 0.0633, acc: 0.9791
loss: 0.0641, acc: 0.9788
loss: 0.0638, acc: 0.9790
loss: 0.0639, acc: 0.9790
loss: 0.0639, acc: 0.9791
loss: 0.0646, acc: 0.9791
loss: 0.0647, acc: 0.9791
> val_acc: 0.8146, val_f1: 0.8078
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0977, acc: 0.9750
loss: 0.0593, acc: 0.9875
loss: 0.0438, acc: 0.9896
loss: 0.0376, acc: 0.9906
loss: 0.0390, acc: 0.9912
loss: 0.0387, acc: 0.9917
loss: 0.0346, acc: 0.9929
loss: 0.0335, acc: 0.9930
loss: 0.0342, acc: 0.9917
loss: 0.0407, acc: 0.9906
loss: 0.0436, acc: 0.9903
loss: 0.0419, acc: 0.9906
loss: 0.0402, acc: 0.9909
loss: 0.0476, acc: 0.9897
loss: 0.0476, acc: 0.9896
loss: 0.0484, acc: 0.9895
loss: 0.0474, acc: 0.9893
loss: 0.0454, acc: 0.9899
loss: 0.0442, acc: 0.9905
loss: 0.0428, acc: 0.9906
loss: 0.0441, acc: 0.9905
loss: 0.0448, acc: 0.9895
loss: 0.0435, acc: 0.9899
loss: 0.0428, acc: 0.9898
loss: 0.0430, acc: 0.9898
loss: 0.0420, acc: 0.9899
loss: 0.0407, acc: 0.9903
loss: 0.0400, acc: 0.9902
loss: 0.0394, acc: 0.9903
loss: 0.0392, acc: 0.9900
loss: 0.0392, acc: 0.9895
loss: 0.0398, acc: 0.9896
loss: 0.0403, acc: 0.9894
loss: 0.0404, acc: 0.9893
loss: 0.0416, acc: 0.9888
loss: 0.0424, acc: 0.9884
loss: 0.0419, acc: 0.9885
loss: 0.0419, acc: 0.9885
loss: 0.0416, acc: 0.9886
loss: 0.0422, acc: 0.9883
loss: 0.0436, acc: 0.9877
loss: 0.0439, acc: 0.9876
loss: 0.0436, acc: 0.9878
loss: 0.0431, acc: 0.9878
loss: 0.0432, acc: 0.9876
loss: 0.0435, acc: 0.9876
loss: 0.0438, acc: 0.9875
loss: 0.0432, acc: 0.9878
loss: 0.0443, acc: 0.9875
loss: 0.0441, acc: 0.9875
loss: 0.0447, acc: 0.9874
loss: 0.0447, acc: 0.9874
loss: 0.0448, acc: 0.9871
loss: 0.0448, acc: 0.9872
loss: 0.0446, acc: 0.9870
loss: 0.0440, acc: 0.9872
loss: 0.0464, acc: 0.9865
loss: 0.0466, acc: 0.9863
loss: 0.0464, acc: 0.9862
loss: 0.0467, acc: 0.9860
loss: 0.0474, acc: 0.9859
loss: 0.0471, acc: 0.9859
loss: 0.0470, acc: 0.9859
loss: 0.0470, acc: 0.9858
loss: 0.0467, acc: 0.9858
loss: 0.0471, acc: 0.9857
loss: 0.0472, acc: 0.9855
loss: 0.0473, acc: 0.9856
loss: 0.0474, acc: 0.9856
loss: 0.0474, acc: 0.9856
> val_acc: 0.8161, val_f1: 0.8091
>> early stop.
>> test_acc: 0.8361, test_f1: 0.8320
>> test_acc: 0.8331, test_f1: 0.8286
>> test_acc: 0.8286, test_f1: 0.8218
>> test_acc: 0.8361, test_f1: 0.8320

>> avg_test_acc: 0.8326, avg_test_f1: 0.8275
>> max_test_acc: 0.8361, max_test_f1: 0.8320
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 499894784
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0984, acc: 0.3875
loss: 1.0959, acc: 0.3781
loss: 1.0809, acc: 0.4250
loss: 1.0647, acc: 0.4453
loss: 1.0543, acc: 0.4425
loss: 1.0458, acc: 0.4479
loss: 1.0372, acc: 0.4562
loss: 1.0282, acc: 0.4695
loss: 1.0162, acc: 0.4813
loss: 0.9980, acc: 0.4938
loss: 0.9833, acc: 0.5080
loss: 0.9675, acc: 0.5219
loss: 0.9476, acc: 0.5337
loss: 0.9353, acc: 0.5393
loss: 0.9225, acc: 0.5479
loss: 0.9103, acc: 0.5578
loss: 0.8987, acc: 0.5680
loss: 0.8842, acc: 0.5788
loss: 0.8661, acc: 0.5885
loss: 0.8607, acc: 0.5928
loss: 0.8499, acc: 0.6006
loss: 0.8454, acc: 0.6051
loss: 0.8377, acc: 0.6095
loss: 0.8291, acc: 0.6148
loss: 0.8203, acc: 0.6198
loss: 0.8119, acc: 0.6243
loss: 0.8074, acc: 0.6266
loss: 0.7987, acc: 0.6321
loss: 0.7890, acc: 0.6377
loss: 0.7810, acc: 0.6412
loss: 0.7766, acc: 0.6448
loss: 0.7747, acc: 0.6467
loss: 0.7697, acc: 0.6502
loss: 0.7598, acc: 0.6548
loss: 0.7524, acc: 0.6589
loss: 0.7471, acc: 0.6615
loss: 0.7428, acc: 0.6639
loss: 0.7358, acc: 0.6674
loss: 0.7336, acc: 0.6678
loss: 0.7296, acc: 0.6709
loss: 0.7258, acc: 0.6735
loss: 0.7187, acc: 0.6775
loss: 0.7163, acc: 0.6789
loss: 0.7121, acc: 0.6813
loss: 0.7112, acc: 0.6829
loss: 0.7073, acc: 0.6853
loss: 0.7028, acc: 0.6883
loss: 0.6987, acc: 0.6909
loss: 0.6966, acc: 0.6920
loss: 0.6934, acc: 0.6941
loss: 0.6889, acc: 0.6968
loss: 0.6839, acc: 0.7000
loss: 0.6826, acc: 0.7012
loss: 0.6787, acc: 0.7030
loss: 0.6742, acc: 0.7053
loss: 0.6704, acc: 0.7079
loss: 0.6657, acc: 0.7107
loss: 0.6616, acc: 0.7125
loss: 0.6578, acc: 0.7144
loss: 0.6557, acc: 0.7158
loss: 0.6530, acc: 0.7173
loss: 0.6507, acc: 0.7183
loss: 0.6469, acc: 0.7210
loss: 0.6444, acc: 0.7226
loss: 0.6417, acc: 0.7238
loss: 0.6396, acc: 0.7246
loss: 0.6376, acc: 0.7258
loss: 0.6364, acc: 0.7267
loss: 0.6337, acc: 0.7279
loss: 0.6318, acc: 0.7287
> val_acc: 0.7965, val_f1: 0.7875
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.7965_f1_0.7875_230828-0652.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4079, acc: 0.8625
loss: 0.3700, acc: 0.8719
loss: 0.3695, acc: 0.8667
loss: 0.3811, acc: 0.8688
loss: 0.3756, acc: 0.8675
loss: 0.4012, acc: 0.8625
loss: 0.4051, acc: 0.8589
loss: 0.4078, acc: 0.8562
loss: 0.4025, acc: 0.8562
loss: 0.4030, acc: 0.8544
loss: 0.4188, acc: 0.8489
loss: 0.4110, acc: 0.8495
loss: 0.4084, acc: 0.8514
loss: 0.4097, acc: 0.8482
loss: 0.4099, acc: 0.8475
loss: 0.4027, acc: 0.8512
loss: 0.4011, acc: 0.8507
loss: 0.3978, acc: 0.8517
loss: 0.3977, acc: 0.8510
loss: 0.3991, acc: 0.8509
loss: 0.3998, acc: 0.8503
loss: 0.4007, acc: 0.8511
loss: 0.4002, acc: 0.8516
loss: 0.3992, acc: 0.8523
loss: 0.3948, acc: 0.8535
loss: 0.3944, acc: 0.8543
loss: 0.3929, acc: 0.8537
loss: 0.3914, acc: 0.8545
loss: 0.3912, acc: 0.8545
loss: 0.3896, acc: 0.8550
loss: 0.3892, acc: 0.8544
loss: 0.3922, acc: 0.8537
loss: 0.3931, acc: 0.8538
loss: 0.3922, acc: 0.8531
loss: 0.3931, acc: 0.8534
loss: 0.3936, acc: 0.8519
loss: 0.3928, acc: 0.8524
loss: 0.3913, acc: 0.8526
loss: 0.3924, acc: 0.8519
loss: 0.3906, acc: 0.8527
loss: 0.3915, acc: 0.8515
loss: 0.3905, acc: 0.8518
loss: 0.3904, acc: 0.8519
loss: 0.3874, acc: 0.8531
loss: 0.3865, acc: 0.8531
loss: 0.3852, acc: 0.8538
loss: 0.3848, acc: 0.8533
loss: 0.3863, acc: 0.8533
loss: 0.3869, acc: 0.8531
loss: 0.3861, acc: 0.8534
loss: 0.3861, acc: 0.8533
loss: 0.3862, acc: 0.8531
loss: 0.3856, acc: 0.8537
loss: 0.3844, acc: 0.8544
loss: 0.3849, acc: 0.8545
loss: 0.3877, acc: 0.8539
loss: 0.3868, acc: 0.8543
loss: 0.3873, acc: 0.8537
loss: 0.3881, acc: 0.8534
loss: 0.3882, acc: 0.8532
loss: 0.3891, acc: 0.8530
loss: 0.3903, acc: 0.8525
loss: 0.3906, acc: 0.8522
loss: 0.3904, acc: 0.8522
loss: 0.3899, acc: 0.8526
loss: 0.3898, acc: 0.8522
loss: 0.3885, acc: 0.8533
loss: 0.3890, acc: 0.8531
loss: 0.3885, acc: 0.8536
loss: 0.3878, acc: 0.8537
> val_acc: 0.8348, val_f1: 0.8293
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8348_f1_0.8293_230828-0654.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2353, acc: 0.9250
loss: 0.2504, acc: 0.9156
loss: 0.2559, acc: 0.9146
loss: 0.2649, acc: 0.9094
loss: 0.2535, acc: 0.9163
loss: 0.2561, acc: 0.9156
loss: 0.2511, acc: 0.9187
loss: 0.2526, acc: 0.9172
loss: 0.2569, acc: 0.9139
loss: 0.2552, acc: 0.9131
loss: 0.2593, acc: 0.9119
loss: 0.2547, acc: 0.9130
loss: 0.2563, acc: 0.9115
loss: 0.2575, acc: 0.9121
loss: 0.2588, acc: 0.9096
loss: 0.2579, acc: 0.9090
loss: 0.2557, acc: 0.9096
loss: 0.2551, acc: 0.9090
loss: 0.2598, acc: 0.9076
loss: 0.2594, acc: 0.9075
loss: 0.2590, acc: 0.9080
loss: 0.2634, acc: 0.9057
loss: 0.2625, acc: 0.9068
loss: 0.2615, acc: 0.9070
loss: 0.2647, acc: 0.9052
loss: 0.2627, acc: 0.9065
loss: 0.2651, acc: 0.9056
loss: 0.2668, acc: 0.9042
loss: 0.2668, acc: 0.9039
loss: 0.2653, acc: 0.9050
loss: 0.2646, acc: 0.9054
loss: 0.2662, acc: 0.9049
loss: 0.2652, acc: 0.9057
loss: 0.2641, acc: 0.9061
loss: 0.2670, acc: 0.9054
loss: 0.2684, acc: 0.9045
loss: 0.2661, acc: 0.9054
loss: 0.2637, acc: 0.9059
loss: 0.2689, acc: 0.9051
loss: 0.2707, acc: 0.9048
loss: 0.2701, acc: 0.9049
loss: 0.2700, acc: 0.9049
loss: 0.2704, acc: 0.9049
loss: 0.2705, acc: 0.9047
loss: 0.2708, acc: 0.9050
loss: 0.2702, acc: 0.9052
loss: 0.2696, acc: 0.9059
loss: 0.2704, acc: 0.9056
loss: 0.2699, acc: 0.9057
loss: 0.2694, acc: 0.9056
loss: 0.2697, acc: 0.9059
loss: 0.2709, acc: 0.9050
loss: 0.2715, acc: 0.9047
loss: 0.2700, acc: 0.9052
loss: 0.2714, acc: 0.9049
loss: 0.2712, acc: 0.9051
loss: 0.2707, acc: 0.9054
loss: 0.2707, acc: 0.9055
loss: 0.2722, acc: 0.9047
loss: 0.2717, acc: 0.9048
loss: 0.2718, acc: 0.9049
loss: 0.2705, acc: 0.9055
loss: 0.2710, acc: 0.9056
loss: 0.2735, acc: 0.9046
loss: 0.2742, acc: 0.9042
loss: 0.2741, acc: 0.9042
loss: 0.2752, acc: 0.9036
loss: 0.2740, acc: 0.9042
loss: 0.2741, acc: 0.9038
loss: 0.2733, acc: 0.9035
> val_acc: 0.8446, val_f1: 0.8385
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8446_f1_0.8385_230828-0656.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1382, acc: 0.9375
loss: 0.1330, acc: 0.9469
loss: 0.1563, acc: 0.9333
loss: 0.1560, acc: 0.9391
loss: 0.1613, acc: 0.9375
loss: 0.1608, acc: 0.9385
loss: 0.1596, acc: 0.9402
loss: 0.1527, acc: 0.9437
loss: 0.1679, acc: 0.9396
loss: 0.1648, acc: 0.9413
loss: 0.1710, acc: 0.9403
loss: 0.1755, acc: 0.9380
loss: 0.1784, acc: 0.9370
loss: 0.1763, acc: 0.9379
loss: 0.1837, acc: 0.9354
loss: 0.1824, acc: 0.9348
loss: 0.1849, acc: 0.9342
loss: 0.1818, acc: 0.9361
loss: 0.1811, acc: 0.9355
loss: 0.1840, acc: 0.9341
loss: 0.1817, acc: 0.9354
loss: 0.1795, acc: 0.9366
loss: 0.1811, acc: 0.9361
loss: 0.1825, acc: 0.9362
loss: 0.1853, acc: 0.9350
loss: 0.1850, acc: 0.9358
loss: 0.1870, acc: 0.9359
loss: 0.1847, acc: 0.9364
loss: 0.1850, acc: 0.9364
loss: 0.1852, acc: 0.9365
loss: 0.1839, acc: 0.9367
loss: 0.1808, acc: 0.9381
loss: 0.1802, acc: 0.9386
loss: 0.1790, acc: 0.9392
loss: 0.1777, acc: 0.9396
loss: 0.1779, acc: 0.9398
loss: 0.1768, acc: 0.9400
loss: 0.1782, acc: 0.9391
loss: 0.1790, acc: 0.9391
loss: 0.1785, acc: 0.9392
loss: 0.1796, acc: 0.9387
loss: 0.1786, acc: 0.9390
loss: 0.1775, acc: 0.9397
loss: 0.1792, acc: 0.9395
loss: 0.1797, acc: 0.9396
loss: 0.1792, acc: 0.9397
loss: 0.1790, acc: 0.9394
loss: 0.1786, acc: 0.9391
loss: 0.1795, acc: 0.9386
loss: 0.1817, acc: 0.9376
loss: 0.1830, acc: 0.9373
loss: 0.1840, acc: 0.9367
loss: 0.1839, acc: 0.9369
loss: 0.1840, acc: 0.9369
loss: 0.1855, acc: 0.9365
loss: 0.1861, acc: 0.9362
loss: 0.1848, acc: 0.9367
loss: 0.1842, acc: 0.9371
loss: 0.1836, acc: 0.9373
loss: 0.1840, acc: 0.9375
loss: 0.1838, acc: 0.9373
loss: 0.1856, acc: 0.9369
loss: 0.1866, acc: 0.9363
loss: 0.1868, acc: 0.9358
loss: 0.1871, acc: 0.9359
loss: 0.1870, acc: 0.9357
loss: 0.1868, acc: 0.9358
loss: 0.1863, acc: 0.9361
loss: 0.1870, acc: 0.9361
loss: 0.1881, acc: 0.9357
> val_acc: 0.8288, val_f1: 0.8256
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1800, acc: 0.9313
loss: 0.2018, acc: 0.9219
loss: 0.1708, acc: 0.9354
loss: 0.1512, acc: 0.9437
loss: 0.1455, acc: 0.9463
loss: 0.1483, acc: 0.9458
loss: 0.1393, acc: 0.9491
loss: 0.1360, acc: 0.9492
loss: 0.1398, acc: 0.9479
loss: 0.1350, acc: 0.9500
loss: 0.1357, acc: 0.9511
loss: 0.1354, acc: 0.9516
loss: 0.1334, acc: 0.9524
loss: 0.1345, acc: 0.9527
loss: 0.1308, acc: 0.9533
loss: 0.1313, acc: 0.9527
loss: 0.1338, acc: 0.9526
loss: 0.1346, acc: 0.9521
loss: 0.1323, acc: 0.9530
loss: 0.1328, acc: 0.9525
loss: 0.1318, acc: 0.9530
loss: 0.1319, acc: 0.9528
loss: 0.1344, acc: 0.9524
loss: 0.1317, acc: 0.9539
loss: 0.1298, acc: 0.9543
loss: 0.1298, acc: 0.9550
loss: 0.1315, acc: 0.9542
loss: 0.1365, acc: 0.9525
loss: 0.1358, acc: 0.9530
loss: 0.1358, acc: 0.9531
loss: 0.1354, acc: 0.9534
loss: 0.1346, acc: 0.9541
loss: 0.1357, acc: 0.9542
loss: 0.1351, acc: 0.9539
loss: 0.1343, acc: 0.9541
loss: 0.1347, acc: 0.9536
loss: 0.1339, acc: 0.9537
loss: 0.1333, acc: 0.9538
loss: 0.1349, acc: 0.9535
loss: 0.1343, acc: 0.9541
loss: 0.1330, acc: 0.9544
loss: 0.1328, acc: 0.9546
loss: 0.1324, acc: 0.9547
loss: 0.1321, acc: 0.9550
loss: 0.1302, acc: 0.9557
loss: 0.1303, acc: 0.9557
loss: 0.1320, acc: 0.9551
loss: 0.1327, acc: 0.9552
loss: 0.1314, acc: 0.9555
loss: 0.1308, acc: 0.9554
loss: 0.1305, acc: 0.9556
loss: 0.1300, acc: 0.9554
loss: 0.1292, acc: 0.9558
loss: 0.1280, acc: 0.9559
loss: 0.1268, acc: 0.9563
loss: 0.1267, acc: 0.9565
loss: 0.1295, acc: 0.9556
loss: 0.1313, acc: 0.9545
loss: 0.1326, acc: 0.9541
loss: 0.1333, acc: 0.9539
loss: 0.1331, acc: 0.9537
loss: 0.1342, acc: 0.9534
loss: 0.1342, acc: 0.9533
loss: 0.1345, acc: 0.9531
loss: 0.1343, acc: 0.9530
loss: 0.1354, acc: 0.9527
loss: 0.1359, acc: 0.9526
loss: 0.1370, acc: 0.9522
loss: 0.1371, acc: 0.9523
loss: 0.1369, acc: 0.9523
> val_acc: 0.8386, val_f1: 0.8315
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.1161, acc: 0.9375
loss: 0.0755, acc: 0.9656
loss: 0.0620, acc: 0.9729
loss: 0.0730, acc: 0.9703
loss: 0.0875, acc: 0.9688
loss: 0.0774, acc: 0.9719
loss: 0.0785, acc: 0.9732
loss: 0.0773, acc: 0.9742
loss: 0.0849, acc: 0.9729
loss: 0.0858, acc: 0.9738
loss: 0.0851, acc: 0.9744
loss: 0.0881, acc: 0.9724
loss: 0.0850, acc: 0.9736
loss: 0.0869, acc: 0.9714
loss: 0.0866, acc: 0.9721
loss: 0.0854, acc: 0.9723
loss: 0.0858, acc: 0.9721
loss: 0.0848, acc: 0.9729
loss: 0.0861, acc: 0.9727
loss: 0.0888, acc: 0.9716
loss: 0.0917, acc: 0.9702
loss: 0.0926, acc: 0.9696
loss: 0.0908, acc: 0.9704
loss: 0.0919, acc: 0.9695
loss: 0.0896, acc: 0.9705
loss: 0.0892, acc: 0.9707
loss: 0.0894, acc: 0.9708
loss: 0.0901, acc: 0.9699
loss: 0.0919, acc: 0.9694
loss: 0.0938, acc: 0.9688
loss: 0.0945, acc: 0.9681
loss: 0.0941, acc: 0.9682
loss: 0.0944, acc: 0.9682
loss: 0.0936, acc: 0.9684
loss: 0.0926, acc: 0.9686
loss: 0.0933, acc: 0.9682
loss: 0.0923, acc: 0.9684
loss: 0.0920, acc: 0.9686
loss: 0.0915, acc: 0.9688
loss: 0.0920, acc: 0.9689
loss: 0.0926, acc: 0.9688
loss: 0.0950, acc: 0.9677
loss: 0.0962, acc: 0.9674
loss: 0.0962, acc: 0.9673
loss: 0.0960, acc: 0.9672
loss: 0.0954, acc: 0.9673
loss: 0.0972, acc: 0.9665
loss: 0.0979, acc: 0.9661
loss: 0.0973, acc: 0.9663
loss: 0.0977, acc: 0.9659
loss: 0.0986, acc: 0.9657
loss: 0.0991, acc: 0.9654
loss: 0.0983, acc: 0.9656
loss: 0.0978, acc: 0.9659
loss: 0.0971, acc: 0.9660
loss: 0.0977, acc: 0.9658
loss: 0.0984, acc: 0.9657
loss: 0.0981, acc: 0.9658
loss: 0.0988, acc: 0.9656
loss: 0.0988, acc: 0.9657
loss: 0.0984, acc: 0.9659
loss: 0.0991, acc: 0.9654
loss: 0.0984, acc: 0.9658
loss: 0.0984, acc: 0.9657
loss: 0.0985, acc: 0.9659
loss: 0.0983, acc: 0.9658
loss: 0.0989, acc: 0.9659
loss: 0.0996, acc: 0.9655
loss: 0.0997, acc: 0.9655
loss: 0.0995, acc: 0.9654
> val_acc: 0.8281, val_f1: 0.8247
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0893, acc: 0.9750
loss: 0.0633, acc: 0.9781
loss: 0.0485, acc: 0.9854
loss: 0.0587, acc: 0.9828
loss: 0.0533, acc: 0.9850
loss: 0.0621, acc: 0.9823
loss: 0.0568, acc: 0.9839
loss: 0.0526, acc: 0.9852
loss: 0.0495, acc: 0.9861
loss: 0.0488, acc: 0.9856
loss: 0.0480, acc: 0.9847
loss: 0.0484, acc: 0.9854
loss: 0.0473, acc: 0.9856
loss: 0.0507, acc: 0.9848
loss: 0.0525, acc: 0.9838
loss: 0.0520, acc: 0.9840
loss: 0.0539, acc: 0.9824
loss: 0.0521, acc: 0.9833
loss: 0.0522, acc: 0.9832
loss: 0.0545, acc: 0.9825
loss: 0.0558, acc: 0.9824
loss: 0.0572, acc: 0.9818
loss: 0.0566, acc: 0.9818
loss: 0.0575, acc: 0.9815
loss: 0.0609, acc: 0.9810
loss: 0.0618, acc: 0.9808
loss: 0.0627, acc: 0.9801
loss: 0.0637, acc: 0.9795
loss: 0.0651, acc: 0.9782
loss: 0.0656, acc: 0.9775
loss: 0.0659, acc: 0.9774
loss: 0.0663, acc: 0.9773
loss: 0.0660, acc: 0.9773
loss: 0.0670, acc: 0.9772
loss: 0.0684, acc: 0.9768
loss: 0.0689, acc: 0.9762
loss: 0.0698, acc: 0.9758
loss: 0.0709, acc: 0.9757
loss: 0.0728, acc: 0.9753
loss: 0.0743, acc: 0.9745
loss: 0.0756, acc: 0.9741
loss: 0.0783, acc: 0.9737
loss: 0.0775, acc: 0.9741
loss: 0.0781, acc: 0.9734
loss: 0.0784, acc: 0.9731
loss: 0.0786, acc: 0.9731
loss: 0.0781, acc: 0.9734
loss: 0.0787, acc: 0.9732
loss: 0.0778, acc: 0.9735
loss: 0.0779, acc: 0.9735
loss: 0.0789, acc: 0.9727
loss: 0.0787, acc: 0.9727
loss: 0.0782, acc: 0.9728
loss: 0.0786, acc: 0.9728
loss: 0.0801, acc: 0.9725
loss: 0.0815, acc: 0.9721
loss: 0.0822, acc: 0.9720
loss: 0.0829, acc: 0.9719
loss: 0.0831, acc: 0.9718
loss: 0.0825, acc: 0.9721
loss: 0.0827, acc: 0.9720
loss: 0.0836, acc: 0.9715
loss: 0.0846, acc: 0.9712
loss: 0.0843, acc: 0.9710
loss: 0.0839, acc: 0.9711
loss: 0.0840, acc: 0.9711
loss: 0.0840, acc: 0.9712
loss: 0.0842, acc: 0.9711
loss: 0.0837, acc: 0.9713
loss: 0.0828, acc: 0.9717
> val_acc: 0.8401, val_f1: 0.8344
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0257, acc: 0.9938
loss: 0.0316, acc: 0.9875
loss: 0.0360, acc: 0.9833
loss: 0.0349, acc: 0.9844
loss: 0.0497, acc: 0.9838
loss: 0.0566, acc: 0.9833
loss: 0.0558, acc: 0.9821
loss: 0.0529, acc: 0.9836
loss: 0.0524, acc: 0.9826
loss: 0.0551, acc: 0.9806
loss: 0.0549, acc: 0.9807
loss: 0.0515, acc: 0.9823
loss: 0.0521, acc: 0.9822
loss: 0.0511, acc: 0.9826
loss: 0.0521, acc: 0.9821
loss: 0.0533, acc: 0.9812
loss: 0.0539, acc: 0.9805
loss: 0.0540, acc: 0.9806
loss: 0.0572, acc: 0.9796
loss: 0.0559, acc: 0.9806
loss: 0.0553, acc: 0.9810
loss: 0.0573, acc: 0.9801
loss: 0.0588, acc: 0.9796
loss: 0.0618, acc: 0.9792
loss: 0.0649, acc: 0.9790
loss: 0.0678, acc: 0.9776
loss: 0.0714, acc: 0.9764
loss: 0.0715, acc: 0.9768
loss: 0.0716, acc: 0.9765
loss: 0.0714, acc: 0.9767
loss: 0.0714, acc: 0.9764
loss: 0.0713, acc: 0.9764
loss: 0.0707, acc: 0.9765
loss: 0.0704, acc: 0.9761
loss: 0.0716, acc: 0.9761
loss: 0.0726, acc: 0.9752
loss: 0.0734, acc: 0.9747
loss: 0.0757, acc: 0.9745
loss: 0.0758, acc: 0.9744
loss: 0.0783, acc: 0.9738
loss: 0.0788, acc: 0.9735
loss: 0.0799, acc: 0.9735
loss: 0.0793, acc: 0.9735
loss: 0.0789, acc: 0.9739
loss: 0.0783, acc: 0.9742
loss: 0.0790, acc: 0.9738
loss: 0.0801, acc: 0.9733
loss: 0.0797, acc: 0.9733
loss: 0.0796, acc: 0.9735
loss: 0.0796, acc: 0.9736
loss: 0.0801, acc: 0.9733
loss: 0.0792, acc: 0.9737
loss: 0.0793, acc: 0.9733
loss: 0.0796, acc: 0.9730
loss: 0.0794, acc: 0.9731
loss: 0.0791, acc: 0.9732
loss: 0.0787, acc: 0.9735
loss: 0.0783, acc: 0.9735
loss: 0.0785, acc: 0.9736
loss: 0.0776, acc: 0.9739
loss: 0.0770, acc: 0.9739
loss: 0.0771, acc: 0.9739
loss: 0.0767, acc: 0.9741
loss: 0.0766, acc: 0.9740
loss: 0.0761, acc: 0.9742
loss: 0.0762, acc: 0.9741
loss: 0.0760, acc: 0.9740
loss: 0.0761, acc: 0.9739
loss: 0.0754, acc: 0.9741
loss: 0.0751, acc: 0.9743
> val_acc: 0.8341, val_f1: 0.8300
>> early stop.
>> test_acc: 0.8331, test_f1: 0.8264
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2541904384
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0943, acc: 0.3563
loss: 1.0726, acc: 0.4188
loss: 1.0766, acc: 0.4250
loss: 1.0755, acc: 0.4234
loss: 1.0731, acc: 0.4338
loss: 1.0692, acc: 0.4365
loss: 1.0677, acc: 0.4321
loss: 1.0570, acc: 0.4367
loss: 1.0378, acc: 0.4472
loss: 1.0222, acc: 0.4644
loss: 1.0017, acc: 0.4818
loss: 0.9807, acc: 0.4974
loss: 0.9631, acc: 0.5106
loss: 0.9445, acc: 0.5254
loss: 0.9344, acc: 0.5337
loss: 0.9190, acc: 0.5449
loss: 0.8976, acc: 0.5585
loss: 0.8803, acc: 0.5708
loss: 0.8630, acc: 0.5813
loss: 0.8555, acc: 0.5887
loss: 0.8485, acc: 0.5949
loss: 0.8397, acc: 0.6017
loss: 0.8290, acc: 0.6076
loss: 0.8175, acc: 0.6148
loss: 0.8118, acc: 0.6200
loss: 0.8050, acc: 0.6240
loss: 0.7970, acc: 0.6294
loss: 0.7875, acc: 0.6350
loss: 0.7797, acc: 0.6399
loss: 0.7729, acc: 0.6435
loss: 0.7615, acc: 0.6506
loss: 0.7531, acc: 0.6555
loss: 0.7465, acc: 0.6593
loss: 0.7380, acc: 0.6642
loss: 0.7348, acc: 0.6666
loss: 0.7305, acc: 0.6694
loss: 0.7244, acc: 0.6725
loss: 0.7189, acc: 0.6765
loss: 0.7145, acc: 0.6795
loss: 0.7089, acc: 0.6823
loss: 0.7035, acc: 0.6860
loss: 0.6999, acc: 0.6872
loss: 0.6949, acc: 0.6903
loss: 0.6878, acc: 0.6937
loss: 0.6843, acc: 0.6960
loss: 0.6808, acc: 0.6973
loss: 0.6776, acc: 0.6999
loss: 0.6734, acc: 0.7020
loss: 0.6699, acc: 0.7046
loss: 0.6659, acc: 0.7069
loss: 0.6632, acc: 0.7081
loss: 0.6605, acc: 0.7105
loss: 0.6585, acc: 0.7120
loss: 0.6553, acc: 0.7132
loss: 0.6515, acc: 0.7157
loss: 0.6521, acc: 0.7160
loss: 0.6500, acc: 0.7173
loss: 0.6461, acc: 0.7193
loss: 0.6414, acc: 0.7219
loss: 0.6402, acc: 0.7231
loss: 0.6372, acc: 0.7252
loss: 0.6360, acc: 0.7262
loss: 0.6324, acc: 0.7283
loss: 0.6297, acc: 0.7294
loss: 0.6285, acc: 0.7302
loss: 0.6262, acc: 0.7315
loss: 0.6248, acc: 0.7327
loss: 0.6229, acc: 0.7337
loss: 0.6213, acc: 0.7341
loss: 0.6198, acc: 0.7350
> val_acc: 0.8183, val_f1: 0.8130
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8183_f1_0.813_230828-0709.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.3047, acc: 0.8938
loss: 0.3336, acc: 0.8781
loss: 0.3372, acc: 0.8771
loss: 0.3502, acc: 0.8766
loss: 0.3689, acc: 0.8638
loss: 0.3737, acc: 0.8646
loss: 0.3734, acc: 0.8634
loss: 0.3682, acc: 0.8656
loss: 0.3734, acc: 0.8646
loss: 0.3717, acc: 0.8612
loss: 0.3734, acc: 0.8619
loss: 0.3739, acc: 0.8609
loss: 0.3749, acc: 0.8582
loss: 0.3723, acc: 0.8589
loss: 0.3728, acc: 0.8600
loss: 0.3699, acc: 0.8621
loss: 0.3729, acc: 0.8607
loss: 0.3730, acc: 0.8622
loss: 0.3755, acc: 0.8615
loss: 0.3750, acc: 0.8622
loss: 0.3772, acc: 0.8619
loss: 0.3799, acc: 0.8608
loss: 0.3792, acc: 0.8595
loss: 0.3820, acc: 0.8589
loss: 0.3825, acc: 0.8578
loss: 0.3818, acc: 0.8584
loss: 0.3791, acc: 0.8600
loss: 0.3788, acc: 0.8598
loss: 0.3788, acc: 0.8597
loss: 0.3759, acc: 0.8612
loss: 0.3740, acc: 0.8615
loss: 0.3730, acc: 0.8615
loss: 0.3725, acc: 0.8614
loss: 0.3730, acc: 0.8608
loss: 0.3749, acc: 0.8609
loss: 0.3742, acc: 0.8609
loss: 0.3769, acc: 0.8596
loss: 0.3733, acc: 0.8610
loss: 0.3767, acc: 0.8599
loss: 0.3771, acc: 0.8606
loss: 0.3777, acc: 0.8604
loss: 0.3769, acc: 0.8606
loss: 0.3764, acc: 0.8605
loss: 0.3777, acc: 0.8597
loss: 0.3804, acc: 0.8578
loss: 0.3803, acc: 0.8575
loss: 0.3817, acc: 0.8569
loss: 0.3808, acc: 0.8570
loss: 0.3816, acc: 0.8565
loss: 0.3803, acc: 0.8571
loss: 0.3802, acc: 0.8574
loss: 0.3787, acc: 0.8584
loss: 0.3793, acc: 0.8584
loss: 0.3810, acc: 0.8573
loss: 0.3811, acc: 0.8575
loss: 0.3795, acc: 0.8583
loss: 0.3781, acc: 0.8591
loss: 0.3775, acc: 0.8594
loss: 0.3781, acc: 0.8590
loss: 0.3772, acc: 0.8593
loss: 0.3780, acc: 0.8592
loss: 0.3792, acc: 0.8588
loss: 0.3792, acc: 0.8590
loss: 0.3793, acc: 0.8587
loss: 0.3792, acc: 0.8586
loss: 0.3799, acc: 0.8586
loss: 0.3807, acc: 0.8585
loss: 0.3814, acc: 0.8582
loss: 0.3831, acc: 0.8570
loss: 0.3833, acc: 0.8571
> val_acc: 0.8228, val_f1: 0.8194
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8228_f1_0.8194_230828-0711.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2148, acc: 0.9500
loss: 0.2419, acc: 0.9344
loss: 0.2549, acc: 0.9167
loss: 0.2407, acc: 0.9203
loss: 0.2269, acc: 0.9263
loss: 0.2310, acc: 0.9198
loss: 0.2279, acc: 0.9223
loss: 0.2329, acc: 0.9203
loss: 0.2227, acc: 0.9236
loss: 0.2386, acc: 0.9163
loss: 0.2443, acc: 0.9153
loss: 0.2543, acc: 0.9130
loss: 0.2483, acc: 0.9154
loss: 0.2561, acc: 0.9112
loss: 0.2570, acc: 0.9108
loss: 0.2511, acc: 0.9129
loss: 0.2540, acc: 0.9107
loss: 0.2558, acc: 0.9090
loss: 0.2563, acc: 0.9095
loss: 0.2571, acc: 0.9081
loss: 0.2592, acc: 0.9071
loss: 0.2635, acc: 0.9054
loss: 0.2662, acc: 0.9043
loss: 0.2678, acc: 0.9036
loss: 0.2650, acc: 0.9052
loss: 0.2668, acc: 0.9034
loss: 0.2651, acc: 0.9039
loss: 0.2666, acc: 0.9033
loss: 0.2667, acc: 0.9037
loss: 0.2665, acc: 0.9042
loss: 0.2688, acc: 0.9038
loss: 0.2664, acc: 0.9037
loss: 0.2673, acc: 0.9038
loss: 0.2693, acc: 0.9028
loss: 0.2700, acc: 0.9021
loss: 0.2692, acc: 0.9028
loss: 0.2681, acc: 0.9035
loss: 0.2714, acc: 0.9023
loss: 0.2717, acc: 0.9027
loss: 0.2710, acc: 0.9031
loss: 0.2687, acc: 0.9044
loss: 0.2704, acc: 0.9037
loss: 0.2694, acc: 0.9045
loss: 0.2694, acc: 0.9044
loss: 0.2705, acc: 0.9035
loss: 0.2697, acc: 0.9034
loss: 0.2705, acc: 0.9029
loss: 0.2708, acc: 0.9029
loss: 0.2704, acc: 0.9028
loss: 0.2677, acc: 0.9039
loss: 0.2689, acc: 0.9033
loss: 0.2680, acc: 0.9035
loss: 0.2681, acc: 0.9032
loss: 0.2697, acc: 0.9027
loss: 0.2697, acc: 0.9026
loss: 0.2702, acc: 0.9021
loss: 0.2706, acc: 0.9025
loss: 0.2720, acc: 0.9020
loss: 0.2720, acc: 0.9021
loss: 0.2719, acc: 0.9024
loss: 0.2707, acc: 0.9028
loss: 0.2710, acc: 0.9024
loss: 0.2703, acc: 0.9030
loss: 0.2702, acc: 0.9032
loss: 0.2703, acc: 0.9033
loss: 0.2698, acc: 0.9036
loss: 0.2710, acc: 0.9033
loss: 0.2706, acc: 0.9031
loss: 0.2711, acc: 0.9028
loss: 0.2708, acc: 0.9031
> val_acc: 0.8311, val_f1: 0.8246
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8311_f1_0.8246_230828-0713.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.1494, acc: 0.9500
loss: 0.1922, acc: 0.9313
loss: 0.1775, acc: 0.9375
loss: 0.1822, acc: 0.9359
loss: 0.1803, acc: 0.9363
loss: 0.1864, acc: 0.9323
loss: 0.1842, acc: 0.9295
loss: 0.1839, acc: 0.9305
loss: 0.1845, acc: 0.9319
loss: 0.1998, acc: 0.9275
loss: 0.2041, acc: 0.9244
loss: 0.1987, acc: 0.9255
loss: 0.2006, acc: 0.9255
loss: 0.1989, acc: 0.9272
loss: 0.1968, acc: 0.9271
loss: 0.1977, acc: 0.9266
loss: 0.1967, acc: 0.9276
loss: 0.1986, acc: 0.9274
loss: 0.1981, acc: 0.9283
loss: 0.1941, acc: 0.9294
loss: 0.1960, acc: 0.9295
loss: 0.1910, acc: 0.9310
loss: 0.1864, acc: 0.9329
loss: 0.1848, acc: 0.9323
loss: 0.1862, acc: 0.9327
loss: 0.1877, acc: 0.9327
loss: 0.1922, acc: 0.9301
loss: 0.1899, acc: 0.9310
loss: 0.1907, acc: 0.9315
loss: 0.1902, acc: 0.9319
loss: 0.1886, acc: 0.9329
loss: 0.1904, acc: 0.9330
loss: 0.1894, acc: 0.9335
loss: 0.1898, acc: 0.9331
loss: 0.1880, acc: 0.9337
loss: 0.1880, acc: 0.9342
loss: 0.1898, acc: 0.9340
loss: 0.1883, acc: 0.9342
loss: 0.1866, acc: 0.9345
loss: 0.1873, acc: 0.9336
loss: 0.1883, acc: 0.9328
loss: 0.1891, acc: 0.9329
loss: 0.1904, acc: 0.9328
loss: 0.1912, acc: 0.9324
loss: 0.1927, acc: 0.9315
loss: 0.1943, acc: 0.9306
loss: 0.1957, acc: 0.9301
loss: 0.1965, acc: 0.9298
loss: 0.1947, acc: 0.9306
loss: 0.1969, acc: 0.9301
loss: 0.1962, acc: 0.9304
loss: 0.1960, acc: 0.9304
loss: 0.1962, acc: 0.9302
loss: 0.1972, acc: 0.9301
loss: 0.1965, acc: 0.9303
loss: 0.1967, acc: 0.9304
loss: 0.1996, acc: 0.9296
loss: 0.1991, acc: 0.9296
loss: 0.1982, acc: 0.9299
loss: 0.1979, acc: 0.9299
loss: 0.1968, acc: 0.9302
loss: 0.1966, acc: 0.9305
loss: 0.1957, acc: 0.9308
loss: 0.1957, acc: 0.9307
loss: 0.1958, acc: 0.9308
loss: 0.1966, acc: 0.9303
loss: 0.1965, acc: 0.9306
loss: 0.1960, acc: 0.9308
loss: 0.1960, acc: 0.9308
loss: 0.1956, acc: 0.9312
> val_acc: 0.8183, val_f1: 0.8168
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.1309, acc: 0.9563
loss: 0.1133, acc: 0.9625
loss: 0.1165, acc: 0.9625
loss: 0.1112, acc: 0.9625
loss: 0.0985, acc: 0.9688
loss: 0.1083, acc: 0.9635
loss: 0.1144, acc: 0.9643
loss: 0.1179, acc: 0.9617
loss: 0.1164, acc: 0.9604
loss: 0.1197, acc: 0.9594
loss: 0.1178, acc: 0.9597
loss: 0.1192, acc: 0.9583
loss: 0.1147, acc: 0.9601
loss: 0.1109, acc: 0.9621
loss: 0.1065, acc: 0.9637
loss: 0.1076, acc: 0.9629
loss: 0.1070, acc: 0.9632
loss: 0.1098, acc: 0.9622
loss: 0.1125, acc: 0.9615
loss: 0.1150, acc: 0.9600
loss: 0.1185, acc: 0.9589
loss: 0.1217, acc: 0.9574
loss: 0.1235, acc: 0.9573
loss: 0.1253, acc: 0.9565
loss: 0.1273, acc: 0.9555
loss: 0.1263, acc: 0.9560
loss: 0.1256, acc: 0.9563
loss: 0.1283, acc: 0.9556
loss: 0.1278, acc: 0.9558
loss: 0.1312, acc: 0.9550
loss: 0.1327, acc: 0.9544
loss: 0.1353, acc: 0.9535
loss: 0.1353, acc: 0.9536
loss: 0.1362, acc: 0.9533
loss: 0.1379, acc: 0.9525
loss: 0.1368, acc: 0.9531
loss: 0.1385, acc: 0.9529
loss: 0.1385, acc: 0.9526
loss: 0.1378, acc: 0.9529
loss: 0.1397, acc: 0.9525
loss: 0.1412, acc: 0.9520
loss: 0.1406, acc: 0.9519
loss: 0.1394, acc: 0.9526
loss: 0.1387, acc: 0.9530
loss: 0.1377, acc: 0.9535
loss: 0.1366, acc: 0.9541
loss: 0.1374, acc: 0.9537
loss: 0.1391, acc: 0.9533
loss: 0.1392, acc: 0.9532
loss: 0.1382, acc: 0.9535
loss: 0.1377, acc: 0.9534
loss: 0.1383, acc: 0.9534
loss: 0.1388, acc: 0.9532
loss: 0.1383, acc: 0.9534
loss: 0.1384, acc: 0.9532
loss: 0.1388, acc: 0.9532
loss: 0.1392, acc: 0.9532
loss: 0.1391, acc: 0.9536
loss: 0.1390, acc: 0.9539
loss: 0.1391, acc: 0.9534
loss: 0.1382, acc: 0.9536
loss: 0.1381, acc: 0.9538
loss: 0.1377, acc: 0.9540
loss: 0.1375, acc: 0.9541
loss: 0.1381, acc: 0.9541
loss: 0.1379, acc: 0.9540
loss: 0.1373, acc: 0.9542
loss: 0.1378, acc: 0.9539
loss: 0.1382, acc: 0.9536
loss: 0.1371, acc: 0.9540
> val_acc: 0.8341, val_f1: 0.8275
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8341_f1_0.8275_230828-0718.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.0902, acc: 0.9625
loss: 0.0856, acc: 0.9688
loss: 0.0708, acc: 0.9771
loss: 0.0654, acc: 0.9781
loss: 0.0745, acc: 0.9775
loss: 0.0754, acc: 0.9781
loss: 0.0704, acc: 0.9804
loss: 0.0682, acc: 0.9805
loss: 0.0713, acc: 0.9778
loss: 0.0722, acc: 0.9775
loss: 0.0757, acc: 0.9750
loss: 0.0729, acc: 0.9755
loss: 0.0707, acc: 0.9769
loss: 0.0683, acc: 0.9777
loss: 0.0718, acc: 0.9767
loss: 0.0755, acc: 0.9766
loss: 0.0732, acc: 0.9772
loss: 0.0719, acc: 0.9778
loss: 0.0706, acc: 0.9776
loss: 0.0685, acc: 0.9788
loss: 0.0688, acc: 0.9789
loss: 0.0692, acc: 0.9790
loss: 0.0731, acc: 0.9774
loss: 0.0747, acc: 0.9776
loss: 0.0730, acc: 0.9782
loss: 0.0773, acc: 0.9764
loss: 0.0814, acc: 0.9757
loss: 0.0810, acc: 0.9761
loss: 0.0802, acc: 0.9759
loss: 0.0814, acc: 0.9754
loss: 0.0828, acc: 0.9748
loss: 0.0825, acc: 0.9752
loss: 0.0825, acc: 0.9756
loss: 0.0827, acc: 0.9756
loss: 0.0851, acc: 0.9752
loss: 0.0844, acc: 0.9753
loss: 0.0850, acc: 0.9753
loss: 0.0858, acc: 0.9752
loss: 0.0863, acc: 0.9748
loss: 0.0879, acc: 0.9741
loss: 0.0876, acc: 0.9741
loss: 0.0881, acc: 0.9738
loss: 0.0896, acc: 0.9734
loss: 0.0898, acc: 0.9736
loss: 0.0919, acc: 0.9729
loss: 0.0916, acc: 0.9731
loss: 0.0906, acc: 0.9733
loss: 0.0911, acc: 0.9727
loss: 0.0923, acc: 0.9721
loss: 0.0930, acc: 0.9715
loss: 0.0919, acc: 0.9719
loss: 0.0917, acc: 0.9719
loss: 0.0927, acc: 0.9713
loss: 0.0926, acc: 0.9712
loss: 0.0930, acc: 0.9709
loss: 0.0923, acc: 0.9712
loss: 0.0928, acc: 0.9711
loss: 0.0924, acc: 0.9711
loss: 0.0935, acc: 0.9708
loss: 0.0951, acc: 0.9701
loss: 0.0952, acc: 0.9700
loss: 0.0959, acc: 0.9696
loss: 0.0955, acc: 0.9697
loss: 0.0945, acc: 0.9701
loss: 0.0935, acc: 0.9705
loss: 0.0930, acc: 0.9707
loss: 0.0930, acc: 0.9707
loss: 0.0944, acc: 0.9702
loss: 0.0952, acc: 0.9698
loss: 0.0958, acc: 0.9699
> val_acc: 0.8176, val_f1: 0.8166
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0576, acc: 0.9812
loss: 0.0564, acc: 0.9844
loss: 0.0762, acc: 0.9771
loss: 0.0784, acc: 0.9766
loss: 0.0696, acc: 0.9800
loss: 0.0699, acc: 0.9792
loss: 0.0657, acc: 0.9812
loss: 0.0639, acc: 0.9812
loss: 0.0637, acc: 0.9812
loss: 0.0639, acc: 0.9819
loss: 0.0593, acc: 0.9835
loss: 0.0585, acc: 0.9833
loss: 0.0616, acc: 0.9822
loss: 0.0632, acc: 0.9812
loss: 0.0667, acc: 0.9800
loss: 0.0646, acc: 0.9809
loss: 0.0649, acc: 0.9805
loss: 0.0638, acc: 0.9809
loss: 0.0683, acc: 0.9803
loss: 0.0690, acc: 0.9803
loss: 0.0680, acc: 0.9807
loss: 0.0687, acc: 0.9801
loss: 0.0733, acc: 0.9785
loss: 0.0727, acc: 0.9786
loss: 0.0748, acc: 0.9780
loss: 0.0750, acc: 0.9776
loss: 0.0741, acc: 0.9780
loss: 0.0754, acc: 0.9777
loss: 0.0744, acc: 0.9778
loss: 0.0753, acc: 0.9773
loss: 0.0750, acc: 0.9772
loss: 0.0740, acc: 0.9773
loss: 0.0749, acc: 0.9771
loss: 0.0751, acc: 0.9772
loss: 0.0747, acc: 0.9773
loss: 0.0747, acc: 0.9774
loss: 0.0756, acc: 0.9770
loss: 0.0750, acc: 0.9773
loss: 0.0748, acc: 0.9771
loss: 0.0750, acc: 0.9770
loss: 0.0746, acc: 0.9773
loss: 0.0753, acc: 0.9771
loss: 0.0758, acc: 0.9769
loss: 0.0758, acc: 0.9768
loss: 0.0752, acc: 0.9772
loss: 0.0745, acc: 0.9774
loss: 0.0754, acc: 0.9771
loss: 0.0762, acc: 0.9766
loss: 0.0759, acc: 0.9768
loss: 0.0754, acc: 0.9770
loss: 0.0753, acc: 0.9770
loss: 0.0762, acc: 0.9766
loss: 0.0779, acc: 0.9761
loss: 0.0784, acc: 0.9760
loss: 0.0806, acc: 0.9756
loss: 0.0803, acc: 0.9756
loss: 0.0808, acc: 0.9754
loss: 0.0810, acc: 0.9752
loss: 0.0817, acc: 0.9750
loss: 0.0814, acc: 0.9751
loss: 0.0816, acc: 0.9750
loss: 0.0825, acc: 0.9747
loss: 0.0825, acc: 0.9746
loss: 0.0827, acc: 0.9743
loss: 0.0828, acc: 0.9743
loss: 0.0827, acc: 0.9743
loss: 0.0831, acc: 0.9740
loss: 0.0834, acc: 0.9740
loss: 0.0835, acc: 0.9739
loss: 0.0842, acc: 0.9738
> val_acc: 0.8318, val_f1: 0.8273
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0373, acc: 0.9812
loss: 0.0443, acc: 0.9812
loss: 0.0790, acc: 0.9729
loss: 0.0631, acc: 0.9797
loss: 0.0587, acc: 0.9812
loss: 0.0722, acc: 0.9760
loss: 0.0705, acc: 0.9777
loss: 0.0719, acc: 0.9766
loss: 0.0681, acc: 0.9785
loss: 0.0675, acc: 0.9781
loss: 0.0682, acc: 0.9778
loss: 0.0685, acc: 0.9781
loss: 0.0669, acc: 0.9788
loss: 0.0634, acc: 0.9804
loss: 0.0630, acc: 0.9804
loss: 0.0649, acc: 0.9789
loss: 0.0659, acc: 0.9787
loss: 0.0701, acc: 0.9774
loss: 0.0696, acc: 0.9773
loss: 0.0687, acc: 0.9775
loss: 0.0681, acc: 0.9777
loss: 0.0681, acc: 0.9776
loss: 0.0670, acc: 0.9780
loss: 0.0656, acc: 0.9784
loss: 0.0653, acc: 0.9788
loss: 0.0646, acc: 0.9788
loss: 0.0657, acc: 0.9787
loss: 0.0664, acc: 0.9786
loss: 0.0657, acc: 0.9787
loss: 0.0646, acc: 0.9790
loss: 0.0640, acc: 0.9790
loss: 0.0649, acc: 0.9787
loss: 0.0642, acc: 0.9790
loss: 0.0636, acc: 0.9792
loss: 0.0622, acc: 0.9796
loss: 0.0628, acc: 0.9795
loss: 0.0623, acc: 0.9799
loss: 0.0641, acc: 0.9796
loss: 0.0634, acc: 0.9800
loss: 0.0645, acc: 0.9794
loss: 0.0638, acc: 0.9796
loss: 0.0641, acc: 0.9798
loss: 0.0631, acc: 0.9801
loss: 0.0629, acc: 0.9801
loss: 0.0627, acc: 0.9800
loss: 0.0643, acc: 0.9795
loss: 0.0639, acc: 0.9795
loss: 0.0645, acc: 0.9792
loss: 0.0648, acc: 0.9788
loss: 0.0648, acc: 0.9786
loss: 0.0647, acc: 0.9784
loss: 0.0645, acc: 0.9785
loss: 0.0649, acc: 0.9782
loss: 0.0653, acc: 0.9780
loss: 0.0649, acc: 0.9781
loss: 0.0643, acc: 0.9781
loss: 0.0638, acc: 0.9784
loss: 0.0633, acc: 0.9787
loss: 0.0630, acc: 0.9787
loss: 0.0645, acc: 0.9784
loss: 0.0643, acc: 0.9786
loss: 0.0645, acc: 0.9786
loss: 0.0652, acc: 0.9783
loss: 0.0653, acc: 0.9781
loss: 0.0657, acc: 0.9779
loss: 0.0670, acc: 0.9777
loss: 0.0673, acc: 0.9778
loss: 0.0671, acc: 0.9779
loss: 0.0674, acc: 0.9779
loss: 0.0675, acc: 0.9779
> val_acc: 0.8303, val_f1: 0.8281
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0190, acc: 0.9938
loss: 0.0421, acc: 0.9906
loss: 0.0550, acc: 0.9812
loss: 0.0527, acc: 0.9828
loss: 0.0579, acc: 0.9838
loss: 0.0625, acc: 0.9812
loss: 0.0572, acc: 0.9830
loss: 0.0528, acc: 0.9844
loss: 0.0515, acc: 0.9840
loss: 0.0489, acc: 0.9850
loss: 0.0450, acc: 0.9864
loss: 0.0475, acc: 0.9859
loss: 0.0526, acc: 0.9846
loss: 0.0552, acc: 0.9844
loss: 0.0534, acc: 0.9846
loss: 0.0548, acc: 0.9848
loss: 0.0530, acc: 0.9849
loss: 0.0526, acc: 0.9851
loss: 0.0552, acc: 0.9839
loss: 0.0567, acc: 0.9831
loss: 0.0584, acc: 0.9821
loss: 0.0583, acc: 0.9818
loss: 0.0569, acc: 0.9821
loss: 0.0555, acc: 0.9823
loss: 0.0547, acc: 0.9825
loss: 0.0533, acc: 0.9832
loss: 0.0528, acc: 0.9829
loss: 0.0561, acc: 0.9812
loss: 0.0566, acc: 0.9810
loss: 0.0564, acc: 0.9810
loss: 0.0557, acc: 0.9815
loss: 0.0556, acc: 0.9814
loss: 0.0551, acc: 0.9812
loss: 0.0564, acc: 0.9811
loss: 0.0570, acc: 0.9807
loss: 0.0581, acc: 0.9804
loss: 0.0583, acc: 0.9806
loss: 0.0576, acc: 0.9809
loss: 0.0585, acc: 0.9806
loss: 0.0584, acc: 0.9806
loss: 0.0595, acc: 0.9802
loss: 0.0587, acc: 0.9805
loss: 0.0579, acc: 0.9808
loss: 0.0579, acc: 0.9807
loss: 0.0575, acc: 0.9810
loss: 0.0574, acc: 0.9810
loss: 0.0569, acc: 0.9811
loss: 0.0561, acc: 0.9814
loss: 0.0554, acc: 0.9815
loss: 0.0549, acc: 0.9818
loss: 0.0554, acc: 0.9819
loss: 0.0567, acc: 0.9819
loss: 0.0582, acc: 0.9815
loss: 0.0581, acc: 0.9817
loss: 0.0598, acc: 0.9809
loss: 0.0600, acc: 0.9808
loss: 0.0597, acc: 0.9809
loss: 0.0595, acc: 0.9811
loss: 0.0588, acc: 0.9814
loss: 0.0594, acc: 0.9811
loss: 0.0595, acc: 0.9811
loss: 0.0603, acc: 0.9810
loss: 0.0599, acc: 0.9812
loss: 0.0599, acc: 0.9813
loss: 0.0595, acc: 0.9814
loss: 0.0592, acc: 0.9816
loss: 0.0588, acc: 0.9817
loss: 0.0588, acc: 0.9818
loss: 0.0586, acc: 0.9819
loss: 0.0581, acc: 0.9821
> val_acc: 0.8341, val_f1: 0.8276
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0766, acc: 0.9625
loss: 0.0441, acc: 0.9812
loss: 0.0450, acc: 0.9854
loss: 0.0412, acc: 0.9859
loss: 0.0423, acc: 0.9862
loss: 0.0443, acc: 0.9854
loss: 0.0482, acc: 0.9848
loss: 0.0534, acc: 0.9844
loss: 0.0515, acc: 0.9840
loss: 0.0544, acc: 0.9838
loss: 0.0585, acc: 0.9824
loss: 0.0555, acc: 0.9839
loss: 0.0572, acc: 0.9832
loss: 0.0597, acc: 0.9821
loss: 0.0580, acc: 0.9825
loss: 0.0619, acc: 0.9816
loss: 0.0595, acc: 0.9820
loss: 0.0586, acc: 0.9819
loss: 0.0582, acc: 0.9816
loss: 0.0583, acc: 0.9812
loss: 0.0584, acc: 0.9815
loss: 0.0563, acc: 0.9824
loss: 0.0545, acc: 0.9829
loss: 0.0533, acc: 0.9833
loss: 0.0527, acc: 0.9835
loss: 0.0512, acc: 0.9839
loss: 0.0512, acc: 0.9836
loss: 0.0504, acc: 0.9839
loss: 0.0503, acc: 0.9841
loss: 0.0513, acc: 0.9835
loss: 0.0517, acc: 0.9835
loss: 0.0503, acc: 0.9840
loss: 0.0514, acc: 0.9839
loss: 0.0509, acc: 0.9838
loss: 0.0526, acc: 0.9834
loss: 0.0524, acc: 0.9832
loss: 0.0516, acc: 0.9834
loss: 0.0510, acc: 0.9837
loss: 0.0517, acc: 0.9833
loss: 0.0512, acc: 0.9833
loss: 0.0513, acc: 0.9835
loss: 0.0511, acc: 0.9835
loss: 0.0527, acc: 0.9833
loss: 0.0545, acc: 0.9827
loss: 0.0536, acc: 0.9831
loss: 0.0537, acc: 0.9830
loss: 0.0538, acc: 0.9828
loss: 0.0529, acc: 0.9832
loss: 0.0528, acc: 0.9833
loss: 0.0532, acc: 0.9831
loss: 0.0523, acc: 0.9835
loss: 0.0529, acc: 0.9833
loss: 0.0527, acc: 0.9834
loss: 0.0532, acc: 0.9832
loss: 0.0535, acc: 0.9832
loss: 0.0538, acc: 0.9830
loss: 0.0540, acc: 0.9829
loss: 0.0538, acc: 0.9828
loss: 0.0538, acc: 0.9827
loss: 0.0541, acc: 0.9827
loss: 0.0544, acc: 0.9827
loss: 0.0550, acc: 0.9823
loss: 0.0551, acc: 0.9819
loss: 0.0562, acc: 0.9816
loss: 0.0571, acc: 0.9814
loss: 0.0568, acc: 0.9815
loss: 0.0566, acc: 0.9815
loss: 0.0572, acc: 0.9813
loss: 0.0573, acc: 0.9813
loss: 0.0572, acc: 0.9814
> val_acc: 0.8326, val_f1: 0.8276
>> early stop.
>> test_acc: 0.8211, test_f1: 0.8130
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
cuda memory allocated: 2547147264
> n_trainable_params: 124647939, n_nontrainable_params: 0
> training arguments:
>>> model_name: roberta
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 2e-05
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0852, acc: 0.4313
loss: 1.0954, acc: 0.4281
loss: 1.0873, acc: 0.4458
loss: 1.0814, acc: 0.4500
loss: 1.0761, acc: 0.4475
loss: 1.0673, acc: 0.4583
loss: 1.0550, acc: 0.4562
loss: 1.0455, acc: 0.4562
loss: 1.0356, acc: 0.4583
loss: 1.0295, acc: 0.4631
loss: 1.0228, acc: 0.4682
loss: 1.0173, acc: 0.4703
loss: 1.0082, acc: 0.4716
loss: 1.0023, acc: 0.4777
loss: 0.9960, acc: 0.4800
loss: 0.9871, acc: 0.4867
loss: 0.9835, acc: 0.4908
loss: 0.9717, acc: 0.4997
loss: 0.9624, acc: 0.5069
loss: 0.9501, acc: 0.5153
loss: 0.9366, acc: 0.5241
loss: 0.9249, acc: 0.5330
loss: 0.9137, acc: 0.5405
loss: 0.8967, acc: 0.5516
loss: 0.8848, acc: 0.5597
loss: 0.8734, acc: 0.5671
loss: 0.8627, acc: 0.5748
loss: 0.8539, acc: 0.5819
loss: 0.8395, acc: 0.5905
loss: 0.8297, acc: 0.5973
loss: 0.8227, acc: 0.6020
loss: 0.8163, acc: 0.6068
loss: 0.8130, acc: 0.6097
loss: 0.8058, acc: 0.6147
loss: 0.7979, acc: 0.6200
loss: 0.7903, acc: 0.6255
loss: 0.7863, acc: 0.6275
loss: 0.7795, acc: 0.6316
loss: 0.7741, acc: 0.6349
loss: 0.7678, acc: 0.6391
loss: 0.7620, acc: 0.6422
loss: 0.7580, acc: 0.6458
loss: 0.7554, acc: 0.6478
loss: 0.7514, acc: 0.6501
loss: 0.7453, acc: 0.6544
loss: 0.7410, acc: 0.6575
loss: 0.7354, acc: 0.6604
loss: 0.7303, acc: 0.6633
loss: 0.7266, acc: 0.6658
loss: 0.7234, acc: 0.6674
loss: 0.7211, acc: 0.6691
loss: 0.7164, acc: 0.6726
loss: 0.7113, acc: 0.6757
loss: 0.7066, acc: 0.6785
loss: 0.7062, acc: 0.6789
loss: 0.7023, acc: 0.6808
loss: 0.6998, acc: 0.6828
loss: 0.6953, acc: 0.6855
loss: 0.6923, acc: 0.6872
loss: 0.6890, acc: 0.6889
loss: 0.6854, acc: 0.6912
loss: 0.6830, acc: 0.6925
loss: 0.6795, acc: 0.6949
loss: 0.6766, acc: 0.6967
loss: 0.6729, acc: 0.6987
loss: 0.6694, acc: 0.7003
loss: 0.6675, acc: 0.7017
loss: 0.6648, acc: 0.7031
loss: 0.6615, acc: 0.7049
loss: 0.6588, acc: 0.7062
> val_acc: 0.8093, val_f1: 0.8056
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8093_f1_0.8056_230828-0731.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.3372, acc: 0.8625
loss: 0.3850, acc: 0.8594
loss: 0.4129, acc: 0.8562
loss: 0.4065, acc: 0.8562
loss: 0.4042, acc: 0.8525
loss: 0.4053, acc: 0.8490
loss: 0.3970, acc: 0.8536
loss: 0.3990, acc: 0.8516
loss: 0.3989, acc: 0.8528
loss: 0.3993, acc: 0.8519
loss: 0.3993, acc: 0.8517
loss: 0.3989, acc: 0.8531
loss: 0.3960, acc: 0.8534
loss: 0.3931, acc: 0.8545
loss: 0.4002, acc: 0.8504
loss: 0.4009, acc: 0.8492
loss: 0.3976, acc: 0.8518
loss: 0.3952, acc: 0.8517
loss: 0.3925, acc: 0.8523
loss: 0.3912, acc: 0.8531
loss: 0.3906, acc: 0.8530
loss: 0.3903, acc: 0.8528
loss: 0.3943, acc: 0.8508
loss: 0.3939, acc: 0.8510
loss: 0.3921, acc: 0.8525
loss: 0.3942, acc: 0.8524
loss: 0.3935, acc: 0.8532
loss: 0.3936, acc: 0.8531
loss: 0.3943, acc: 0.8526
loss: 0.3925, acc: 0.8535
loss: 0.3928, acc: 0.8534
loss: 0.3913, acc: 0.8539
loss: 0.3926, acc: 0.8528
loss: 0.3915, acc: 0.8535
loss: 0.3914, acc: 0.8539
loss: 0.3908, acc: 0.8543
loss: 0.3897, acc: 0.8549
loss: 0.3891, acc: 0.8546
loss: 0.3905, acc: 0.8545
loss: 0.3884, acc: 0.8550
loss: 0.3886, acc: 0.8544
loss: 0.3899, acc: 0.8542
loss: 0.3903, acc: 0.8536
loss: 0.3913, acc: 0.8531
loss: 0.3910, acc: 0.8535
loss: 0.3912, acc: 0.8535
loss: 0.3896, acc: 0.8541
loss: 0.3919, acc: 0.8534
loss: 0.3912, acc: 0.8536
loss: 0.3909, acc: 0.8539
loss: 0.3908, acc: 0.8538
loss: 0.3903, acc: 0.8537
loss: 0.3884, acc: 0.8547
loss: 0.3879, acc: 0.8547
loss: 0.3875, acc: 0.8545
loss: 0.3869, acc: 0.8548
loss: 0.3882, acc: 0.8542
loss: 0.3879, acc: 0.8546
loss: 0.3877, acc: 0.8548
loss: 0.3868, acc: 0.8549
loss: 0.3865, acc: 0.8552
loss: 0.3851, acc: 0.8559
loss: 0.3843, acc: 0.8558
loss: 0.3834, acc: 0.8560
loss: 0.3831, acc: 0.8562
loss: 0.3830, acc: 0.8562
loss: 0.3818, acc: 0.8568
loss: 0.3826, acc: 0.8564
loss: 0.3837, acc: 0.8559
loss: 0.3851, acc: 0.8552
> val_acc: 0.8228, val_f1: 0.8189
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8228_f1_0.8189_230828-0733.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.2797, acc: 0.9250
loss: 0.2716, acc: 0.9125
loss: 0.2439, acc: 0.9271
loss: 0.2586, acc: 0.9187
loss: 0.2798, acc: 0.9137
loss: 0.3130, acc: 0.9000
loss: 0.3088, acc: 0.9009
loss: 0.3046, acc: 0.9008
loss: 0.2950, acc: 0.9021
loss: 0.2911, acc: 0.9006
loss: 0.2834, acc: 0.9040
loss: 0.2833, acc: 0.9042
loss: 0.2809, acc: 0.9024
loss: 0.2770, acc: 0.9027
loss: 0.2763, acc: 0.9042
loss: 0.2764, acc: 0.9043
loss: 0.2722, acc: 0.9059
loss: 0.2685, acc: 0.9069
loss: 0.2679, acc: 0.9066
loss: 0.2711, acc: 0.9034
loss: 0.2739, acc: 0.9024
loss: 0.2761, acc: 0.9017
loss: 0.2728, acc: 0.9030
loss: 0.2735, acc: 0.9021
loss: 0.2703, acc: 0.9035
loss: 0.2727, acc: 0.9024
loss: 0.2728, acc: 0.9019
loss: 0.2718, acc: 0.9025
loss: 0.2740, acc: 0.9015
loss: 0.2737, acc: 0.9019
loss: 0.2750, acc: 0.9014
loss: 0.2731, acc: 0.9023
loss: 0.2741, acc: 0.9021
loss: 0.2758, acc: 0.9017
loss: 0.2770, acc: 0.9002
loss: 0.2761, acc: 0.9002
loss: 0.2734, acc: 0.9007
loss: 0.2742, acc: 0.9008
loss: 0.2747, acc: 0.9005
loss: 0.2726, acc: 0.9011
loss: 0.2729, acc: 0.9008
loss: 0.2758, acc: 0.8994
loss: 0.2758, acc: 0.8990
loss: 0.2742, acc: 0.8993
loss: 0.2740, acc: 0.8999
loss: 0.2734, acc: 0.9008
loss: 0.2744, acc: 0.9007
loss: 0.2785, acc: 0.8999
loss: 0.2789, acc: 0.8996
loss: 0.2808, acc: 0.8989
loss: 0.2812, acc: 0.8984
loss: 0.2799, acc: 0.8984
loss: 0.2810, acc: 0.8980
loss: 0.2823, acc: 0.8976
loss: 0.2824, acc: 0.8975
loss: 0.2844, acc: 0.8967
loss: 0.2854, acc: 0.8961
loss: 0.2866, acc: 0.8957
loss: 0.2856, acc: 0.8960
loss: 0.2849, acc: 0.8962
loss: 0.2836, acc: 0.8965
loss: 0.2825, acc: 0.8970
loss: 0.2831, acc: 0.8966
loss: 0.2840, acc: 0.8961
loss: 0.2841, acc: 0.8957
loss: 0.2856, acc: 0.8953
loss: 0.2871, acc: 0.8944
loss: 0.2887, acc: 0.8938
loss: 0.2897, acc: 0.8930
loss: 0.2910, acc: 0.8925
> val_acc: 0.8131, val_f1: 0.8103
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.2309, acc: 0.9125
loss: 0.2107, acc: 0.9187
loss: 0.1907, acc: 0.9292
loss: 0.1818, acc: 0.9344
loss: 0.1619, acc: 0.9437
loss: 0.1578, acc: 0.9427
loss: 0.1617, acc: 0.9420
loss: 0.1713, acc: 0.9398
loss: 0.1767, acc: 0.9375
loss: 0.1810, acc: 0.9363
loss: 0.1826, acc: 0.9347
loss: 0.1826, acc: 0.9349
loss: 0.1813, acc: 0.9356
loss: 0.1799, acc: 0.9348
loss: 0.1828, acc: 0.9342
loss: 0.1809, acc: 0.9344
loss: 0.1823, acc: 0.9357
loss: 0.1809, acc: 0.9361
loss: 0.1808, acc: 0.9365
loss: 0.1792, acc: 0.9375
loss: 0.1809, acc: 0.9354
loss: 0.1818, acc: 0.9344
loss: 0.1778, acc: 0.9359
loss: 0.1769, acc: 0.9367
loss: 0.1768, acc: 0.9373
loss: 0.1775, acc: 0.9368
loss: 0.1792, acc: 0.9363
loss: 0.1796, acc: 0.9362
loss: 0.1793, acc: 0.9360
loss: 0.1828, acc: 0.9346
loss: 0.1828, acc: 0.9351
loss: 0.1839, acc: 0.9355
loss: 0.1836, acc: 0.9356
loss: 0.1844, acc: 0.9347
loss: 0.1859, acc: 0.9345
loss: 0.1887, acc: 0.9339
loss: 0.1894, acc: 0.9334
loss: 0.1890, acc: 0.9336
loss: 0.1910, acc: 0.9330
loss: 0.1929, acc: 0.9322
loss: 0.1922, acc: 0.9325
loss: 0.1925, acc: 0.9323
loss: 0.1933, acc: 0.9320
loss: 0.1929, acc: 0.9321
loss: 0.1932, acc: 0.9318
loss: 0.1938, acc: 0.9315
loss: 0.1935, acc: 0.9315
loss: 0.1943, acc: 0.9306
loss: 0.1936, acc: 0.9307
loss: 0.1932, acc: 0.9307
loss: 0.1938, acc: 0.9304
loss: 0.1934, acc: 0.9306
loss: 0.1947, acc: 0.9303
loss: 0.1949, acc: 0.9302
loss: 0.1964, acc: 0.9297
loss: 0.1967, acc: 0.9299
loss: 0.1983, acc: 0.9291
loss: 0.1984, acc: 0.9288
loss: 0.1986, acc: 0.9288
loss: 0.1985, acc: 0.9286
loss: 0.1993, acc: 0.9286
loss: 0.1993, acc: 0.9285
loss: 0.1992, acc: 0.9286
loss: 0.2002, acc: 0.9282
loss: 0.2006, acc: 0.9279
loss: 0.2004, acc: 0.9281
loss: 0.1998, acc: 0.9283
loss: 0.2005, acc: 0.9280
loss: 0.2010, acc: 0.9280
loss: 0.2019, acc: 0.9276
> val_acc: 0.8273, val_f1: 0.8241
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8273_f1_0.8241_230828-0737.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.2424, acc: 0.9375
loss: 0.1892, acc: 0.9500
loss: 0.1915, acc: 0.9479
loss: 0.1787, acc: 0.9453
loss: 0.1695, acc: 0.9450
loss: 0.1632, acc: 0.9479
loss: 0.1518, acc: 0.9518
loss: 0.1450, acc: 0.9531
loss: 0.1440, acc: 0.9528
loss: 0.1436, acc: 0.9500
loss: 0.1442, acc: 0.9500
loss: 0.1496, acc: 0.9469
loss: 0.1478, acc: 0.9476
loss: 0.1476, acc: 0.9473
loss: 0.1501, acc: 0.9467
loss: 0.1513, acc: 0.9461
loss: 0.1503, acc: 0.9471
loss: 0.1475, acc: 0.9476
loss: 0.1466, acc: 0.9480
loss: 0.1425, acc: 0.9494
loss: 0.1419, acc: 0.9500
loss: 0.1429, acc: 0.9497
loss: 0.1422, acc: 0.9500
loss: 0.1398, acc: 0.9505
loss: 0.1397, acc: 0.9507
loss: 0.1386, acc: 0.9517
loss: 0.1400, acc: 0.9514
loss: 0.1395, acc: 0.9516
loss: 0.1383, acc: 0.9519
loss: 0.1375, acc: 0.9525
loss: 0.1360, acc: 0.9528
loss: 0.1386, acc: 0.9523
loss: 0.1403, acc: 0.9513
loss: 0.1395, acc: 0.9518
loss: 0.1413, acc: 0.9513
loss: 0.1419, acc: 0.9512
loss: 0.1433, acc: 0.9512
loss: 0.1438, acc: 0.9515
loss: 0.1441, acc: 0.9508
loss: 0.1458, acc: 0.9502
loss: 0.1460, acc: 0.9498
loss: 0.1458, acc: 0.9493
loss: 0.1457, acc: 0.9494
loss: 0.1455, acc: 0.9491
loss: 0.1442, acc: 0.9497
loss: 0.1446, acc: 0.9493
loss: 0.1464, acc: 0.9485
loss: 0.1452, acc: 0.9490
loss: 0.1454, acc: 0.9487
loss: 0.1446, acc: 0.9487
loss: 0.1430, acc: 0.9494
loss: 0.1450, acc: 0.9494
loss: 0.1444, acc: 0.9492
loss: 0.1449, acc: 0.9492
loss: 0.1451, acc: 0.9490
loss: 0.1462, acc: 0.9490
loss: 0.1465, acc: 0.9487
loss: 0.1465, acc: 0.9487
loss: 0.1460, acc: 0.9489
loss: 0.1465, acc: 0.9487
loss: 0.1464, acc: 0.9488
loss: 0.1458, acc: 0.9489
loss: 0.1459, acc: 0.9489
loss: 0.1458, acc: 0.9489
loss: 0.1455, acc: 0.9488
loss: 0.1449, acc: 0.9489
loss: 0.1447, acc: 0.9488
loss: 0.1461, acc: 0.9481
loss: 0.1463, acc: 0.9480
loss: 0.1456, acc: 0.9480
> val_acc: 0.8416, val_f1: 0.8372
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8416_f1_0.8372_230828-0739.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.1530, acc: 0.9563
loss: 0.1188, acc: 0.9563
loss: 0.1248, acc: 0.9500
loss: 0.1246, acc: 0.9547
loss: 0.1186, acc: 0.9563
loss: 0.1133, acc: 0.9573
loss: 0.1079, acc: 0.9589
loss: 0.1043, acc: 0.9609
loss: 0.1059, acc: 0.9618
loss: 0.1095, acc: 0.9613
loss: 0.1039, acc: 0.9631
loss: 0.1039, acc: 0.9635
loss: 0.0994, acc: 0.9659
loss: 0.0989, acc: 0.9661
loss: 0.0985, acc: 0.9667
loss: 0.0980, acc: 0.9660
loss: 0.0988, acc: 0.9654
loss: 0.1004, acc: 0.9649
loss: 0.1010, acc: 0.9641
loss: 0.0992, acc: 0.9647
loss: 0.0997, acc: 0.9643
loss: 0.0991, acc: 0.9645
loss: 0.0996, acc: 0.9641
loss: 0.1001, acc: 0.9638
loss: 0.1013, acc: 0.9635
loss: 0.0991, acc: 0.9642
loss: 0.0994, acc: 0.9639
loss: 0.0997, acc: 0.9632
loss: 0.1016, acc: 0.9623
loss: 0.1006, acc: 0.9627
loss: 0.1003, acc: 0.9623
loss: 0.1023, acc: 0.9615
loss: 0.1017, acc: 0.9619
loss: 0.1024, acc: 0.9619
loss: 0.1016, acc: 0.9623
loss: 0.1034, acc: 0.9613
loss: 0.1034, acc: 0.9615
loss: 0.1037, acc: 0.9612
loss: 0.1036, acc: 0.9612
loss: 0.1029, acc: 0.9619
loss: 0.1029, acc: 0.9620
loss: 0.1030, acc: 0.9619
loss: 0.1027, acc: 0.9621
loss: 0.1033, acc: 0.9621
loss: 0.1026, acc: 0.9624
loss: 0.1028, acc: 0.9624
loss: 0.1017, acc: 0.9628
loss: 0.1014, acc: 0.9628
loss: 0.1015, acc: 0.9629
loss: 0.1014, acc: 0.9630
loss: 0.1005, acc: 0.9632
loss: 0.1002, acc: 0.9635
loss: 0.1007, acc: 0.9632
loss: 0.1012, acc: 0.9628
loss: 0.1016, acc: 0.9628
loss: 0.1029, acc: 0.9628
loss: 0.1029, acc: 0.9625
loss: 0.1029, acc: 0.9627
loss: 0.1029, acc: 0.9627
loss: 0.1021, acc: 0.9627
loss: 0.1012, acc: 0.9632
loss: 0.1013, acc: 0.9633
loss: 0.1014, acc: 0.9634
loss: 0.1029, acc: 0.9629
loss: 0.1042, acc: 0.9626
loss: 0.1049, acc: 0.9624
loss: 0.1050, acc: 0.9626
loss: 0.1053, acc: 0.9625
loss: 0.1052, acc: 0.9627
loss: 0.1054, acc: 0.9624
> val_acc: 0.8453, val_f1: 0.8396
>> saved: /media/b115/Backup/NLP/roberta/mams/acc_0.8453_f1_0.8396_230828-0741.model
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.0501, acc: 0.9875
loss: 0.0458, acc: 0.9906
loss: 0.0492, acc: 0.9833
loss: 0.0552, acc: 0.9781
loss: 0.0751, acc: 0.9700
loss: 0.0751, acc: 0.9719
loss: 0.0752, acc: 0.9732
loss: 0.0708, acc: 0.9750
loss: 0.0734, acc: 0.9729
loss: 0.0694, acc: 0.9756
loss: 0.0684, acc: 0.9761
loss: 0.0660, acc: 0.9771
loss: 0.0703, acc: 0.9745
loss: 0.0701, acc: 0.9746
loss: 0.0732, acc: 0.9738
loss: 0.0742, acc: 0.9734
loss: 0.0778, acc: 0.9724
loss: 0.0810, acc: 0.9715
loss: 0.0838, acc: 0.9694
loss: 0.0852, acc: 0.9684
loss: 0.0849, acc: 0.9688
loss: 0.0832, acc: 0.9693
loss: 0.0815, acc: 0.9704
loss: 0.0812, acc: 0.9711
loss: 0.0797, acc: 0.9718
loss: 0.0774, acc: 0.9726
loss: 0.0754, acc: 0.9731
loss: 0.0754, acc: 0.9737
loss: 0.0761, acc: 0.9737
loss: 0.0767, acc: 0.9735
loss: 0.0758, acc: 0.9740
loss: 0.0749, acc: 0.9744
loss: 0.0743, acc: 0.9748
loss: 0.0750, acc: 0.9748
loss: 0.0746, acc: 0.9746
loss: 0.0759, acc: 0.9745
loss: 0.0751, acc: 0.9747
loss: 0.0756, acc: 0.9747
loss: 0.0765, acc: 0.9745
loss: 0.0774, acc: 0.9742
loss: 0.0788, acc: 0.9741
loss: 0.0784, acc: 0.9741
loss: 0.0788, acc: 0.9740
loss: 0.0796, acc: 0.9736
loss: 0.0801, acc: 0.9736
loss: 0.0795, acc: 0.9738
loss: 0.0784, acc: 0.9743
loss: 0.0780, acc: 0.9745
loss: 0.0780, acc: 0.9745
loss: 0.0787, acc: 0.9742
loss: 0.0811, acc: 0.9740
loss: 0.0815, acc: 0.9738
loss: 0.0814, acc: 0.9739
loss: 0.0814, acc: 0.9738
loss: 0.0816, acc: 0.9735
loss: 0.0826, acc: 0.9732
loss: 0.0829, acc: 0.9730
loss: 0.0827, acc: 0.9730
loss: 0.0829, acc: 0.9730
loss: 0.0828, acc: 0.9729
loss: 0.0829, acc: 0.9728
loss: 0.0832, acc: 0.9728
loss: 0.0833, acc: 0.9726
loss: 0.0830, acc: 0.9728
loss: 0.0828, acc: 0.9730
loss: 0.0824, acc: 0.9730
loss: 0.0828, acc: 0.9729
loss: 0.0824, acc: 0.9730
loss: 0.0815, acc: 0.9734
loss: 0.0814, acc: 0.9733
> val_acc: 0.8348, val_f1: 0.8305
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.0500, acc: 0.9750
loss: 0.0574, acc: 0.9781
loss: 0.0743, acc: 0.9750
loss: 0.0622, acc: 0.9797
loss: 0.0737, acc: 0.9775
loss: 0.0739, acc: 0.9771
loss: 0.0685, acc: 0.9786
loss: 0.0700, acc: 0.9781
loss: 0.0714, acc: 0.9778
loss: 0.0674, acc: 0.9788
loss: 0.0659, acc: 0.9790
loss: 0.0654, acc: 0.9797
loss: 0.0699, acc: 0.9793
loss: 0.0668, acc: 0.9804
loss: 0.0678, acc: 0.9792
loss: 0.0710, acc: 0.9781
loss: 0.0697, acc: 0.9779
loss: 0.0681, acc: 0.9785
loss: 0.0663, acc: 0.9789
loss: 0.0645, acc: 0.9794
loss: 0.0656, acc: 0.9789
loss: 0.0677, acc: 0.9784
loss: 0.0674, acc: 0.9783
loss: 0.0694, acc: 0.9771
loss: 0.0688, acc: 0.9772
loss: 0.0688, acc: 0.9774
loss: 0.0724, acc: 0.9769
loss: 0.0730, acc: 0.9766
loss: 0.0727, acc: 0.9769
loss: 0.0729, acc: 0.9765
loss: 0.0726, acc: 0.9764
loss: 0.0722, acc: 0.9764
loss: 0.0718, acc: 0.9765
loss: 0.0721, acc: 0.9757
loss: 0.0727, acc: 0.9754
loss: 0.0719, acc: 0.9753
loss: 0.0715, acc: 0.9755
loss: 0.0724, acc: 0.9755
loss: 0.0720, acc: 0.9758
loss: 0.0709, acc: 0.9762
loss: 0.0708, acc: 0.9762
loss: 0.0707, acc: 0.9763
loss: 0.0706, acc: 0.9763
loss: 0.0702, acc: 0.9764
loss: 0.0703, acc: 0.9764
loss: 0.0697, acc: 0.9768
loss: 0.0695, acc: 0.9770
loss: 0.0687, acc: 0.9772
loss: 0.0687, acc: 0.9772
loss: 0.0684, acc: 0.9774
loss: 0.0681, acc: 0.9775
loss: 0.0677, acc: 0.9775
loss: 0.0678, acc: 0.9776
loss: 0.0678, acc: 0.9778
loss: 0.0671, acc: 0.9780
loss: 0.0674, acc: 0.9779
loss: 0.0667, acc: 0.9781
loss: 0.0673, acc: 0.9779
loss: 0.0671, acc: 0.9781
loss: 0.0675, acc: 0.9776
loss: 0.0676, acc: 0.9777
loss: 0.0677, acc: 0.9777
loss: 0.0678, acc: 0.9777
loss: 0.0679, acc: 0.9776
loss: 0.0685, acc: 0.9776
loss: 0.0680, acc: 0.9777
loss: 0.0691, acc: 0.9776
loss: 0.0686, acc: 0.9778
loss: 0.0681, acc: 0.9780
loss: 0.0678, acc: 0.9781
> val_acc: 0.8423, val_f1: 0.8361
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.0951, acc: 0.9688
loss: 0.0674, acc: 0.9781
loss: 0.0604, acc: 0.9771
loss: 0.0681, acc: 0.9766
loss: 0.0621, acc: 0.9788
loss: 0.0646, acc: 0.9760
loss: 0.0722, acc: 0.9759
loss: 0.0716, acc: 0.9758
loss: 0.0688, acc: 0.9764
loss: 0.0640, acc: 0.9781
loss: 0.0653, acc: 0.9778
loss: 0.0619, acc: 0.9792
loss: 0.0593, acc: 0.9798
loss: 0.0594, acc: 0.9790
loss: 0.0589, acc: 0.9796
loss: 0.0592, acc: 0.9801
loss: 0.0565, acc: 0.9812
loss: 0.0555, acc: 0.9812
loss: 0.0605, acc: 0.9803
loss: 0.0637, acc: 0.9800
loss: 0.0622, acc: 0.9801
loss: 0.0649, acc: 0.9795
loss: 0.0651, acc: 0.9793
loss: 0.0658, acc: 0.9789
loss: 0.0659, acc: 0.9790
loss: 0.0656, acc: 0.9793
loss: 0.0655, acc: 0.9792
loss: 0.0645, acc: 0.9797
loss: 0.0660, acc: 0.9791
loss: 0.0660, acc: 0.9794
loss: 0.0668, acc: 0.9794
loss: 0.0669, acc: 0.9793
loss: 0.0651, acc: 0.9799
loss: 0.0638, acc: 0.9803
loss: 0.0633, acc: 0.9805
loss: 0.0623, acc: 0.9809
loss: 0.0616, acc: 0.9811
loss: 0.0603, acc: 0.9816
loss: 0.0602, acc: 0.9814
loss: 0.0598, acc: 0.9811
loss: 0.0599, acc: 0.9811
loss: 0.0606, acc: 0.9811
loss: 0.0628, acc: 0.9801
loss: 0.0640, acc: 0.9798
loss: 0.0632, acc: 0.9800
loss: 0.0630, acc: 0.9799
loss: 0.0625, acc: 0.9801
loss: 0.0622, acc: 0.9801
loss: 0.0635, acc: 0.9795
loss: 0.0647, acc: 0.9792
loss: 0.0645, acc: 0.9793
loss: 0.0643, acc: 0.9793
loss: 0.0637, acc: 0.9795
loss: 0.0633, acc: 0.9795
loss: 0.0643, acc: 0.9794
loss: 0.0647, acc: 0.9792
loss: 0.0644, acc: 0.9794
loss: 0.0660, acc: 0.9787
loss: 0.0653, acc: 0.9789
loss: 0.0659, acc: 0.9785
loss: 0.0662, acc: 0.9785
loss: 0.0663, acc: 0.9784
loss: 0.0668, acc: 0.9781
loss: 0.0663, acc: 0.9783
loss: 0.0676, acc: 0.9781
loss: 0.0674, acc: 0.9782
loss: 0.0677, acc: 0.9780
loss: 0.0676, acc: 0.9780
loss: 0.0673, acc: 0.9781
loss: 0.0669, acc: 0.9782
> val_acc: 0.8131, val_f1: 0.8107
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.0378, acc: 0.9812
loss: 0.0413, acc: 0.9844
loss: 0.0538, acc: 0.9792
loss: 0.0506, acc: 0.9812
loss: 0.0612, acc: 0.9775
loss: 0.0678, acc: 0.9781
loss: 0.0668, acc: 0.9795
loss: 0.0620, acc: 0.9820
loss: 0.0590, acc: 0.9819
loss: 0.0550, acc: 0.9831
loss: 0.0527, acc: 0.9841
loss: 0.0509, acc: 0.9844
loss: 0.0521, acc: 0.9846
loss: 0.0535, acc: 0.9839
loss: 0.0511, acc: 0.9846
loss: 0.0493, acc: 0.9848
loss: 0.0509, acc: 0.9835
loss: 0.0506, acc: 0.9837
loss: 0.0516, acc: 0.9832
loss: 0.0494, acc: 0.9841
loss: 0.0504, acc: 0.9836
loss: 0.0506, acc: 0.9838
loss: 0.0523, acc: 0.9837
loss: 0.0509, acc: 0.9844
loss: 0.0512, acc: 0.9840
loss: 0.0504, acc: 0.9841
loss: 0.0509, acc: 0.9843
loss: 0.0514, acc: 0.9839
loss: 0.0517, acc: 0.9843
loss: 0.0526, acc: 0.9840
loss: 0.0517, acc: 0.9845
loss: 0.0518, acc: 0.9844
loss: 0.0514, acc: 0.9841
loss: 0.0505, acc: 0.9846
loss: 0.0499, acc: 0.9848
loss: 0.0494, acc: 0.9851
loss: 0.0497, acc: 0.9851
loss: 0.0509, acc: 0.9849
loss: 0.0515, acc: 0.9848
loss: 0.0529, acc: 0.9845
loss: 0.0554, acc: 0.9837
loss: 0.0555, acc: 0.9835
loss: 0.0551, acc: 0.9834
loss: 0.0549, acc: 0.9835
loss: 0.0551, acc: 0.9836
loss: 0.0549, acc: 0.9836
loss: 0.0572, acc: 0.9830
loss: 0.0574, acc: 0.9829
loss: 0.0572, acc: 0.9829
loss: 0.0571, acc: 0.9828
loss: 0.0578, acc: 0.9825
loss: 0.0582, acc: 0.9825
loss: 0.0579, acc: 0.9825
loss: 0.0576, acc: 0.9826
loss: 0.0581, acc: 0.9822
loss: 0.0577, acc: 0.9823
loss: 0.0581, acc: 0.9821
loss: 0.0606, acc: 0.9815
loss: 0.0605, acc: 0.9814
loss: 0.0608, acc: 0.9812
loss: 0.0616, acc: 0.9811
loss: 0.0613, acc: 0.9811
loss: 0.0612, acc: 0.9811
loss: 0.0612, acc: 0.9808
loss: 0.0610, acc: 0.9807
loss: 0.0610, acc: 0.9807
loss: 0.0607, acc: 0.9808
loss: 0.0603, acc: 0.9809
loss: 0.0604, acc: 0.9809
loss: 0.0606, acc: 0.9808
> val_acc: 0.8348, val_f1: 0.8290
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.0777, acc: 0.9812
loss: 0.0557, acc: 0.9844
loss: 0.0423, acc: 0.9896
loss: 0.0415, acc: 0.9875
loss: 0.0391, acc: 0.9875
loss: 0.0350, acc: 0.9885
loss: 0.0322, acc: 0.9902
loss: 0.0334, acc: 0.9906
loss: 0.0365, acc: 0.9910
loss: 0.0339, acc: 0.9912
loss: 0.0361, acc: 0.9903
loss: 0.0421, acc: 0.9891
loss: 0.0426, acc: 0.9880
loss: 0.0416, acc: 0.9879
loss: 0.0421, acc: 0.9871
loss: 0.0455, acc: 0.9871
loss: 0.0434, acc: 0.9879
loss: 0.0448, acc: 0.9875
loss: 0.0436, acc: 0.9878
loss: 0.0475, acc: 0.9869
loss: 0.0467, acc: 0.9869
loss: 0.0479, acc: 0.9866
loss: 0.0478, acc: 0.9864
loss: 0.0463, acc: 0.9867
loss: 0.0476, acc: 0.9865
loss: 0.0475, acc: 0.9865
loss: 0.0466, acc: 0.9868
loss: 0.0470, acc: 0.9864
loss: 0.0460, acc: 0.9866
loss: 0.0449, acc: 0.9871
loss: 0.0438, acc: 0.9873
loss: 0.0442, acc: 0.9873
loss: 0.0434, acc: 0.9873
loss: 0.0426, acc: 0.9875
loss: 0.0416, acc: 0.9879
loss: 0.0406, acc: 0.9882
loss: 0.0412, acc: 0.9880
loss: 0.0410, acc: 0.9882
loss: 0.0401, acc: 0.9885
loss: 0.0402, acc: 0.9881
loss: 0.0410, acc: 0.9880
loss: 0.0412, acc: 0.9878
loss: 0.0417, acc: 0.9874
loss: 0.0416, acc: 0.9875
loss: 0.0412, acc: 0.9875
loss: 0.0410, acc: 0.9875
loss: 0.0407, acc: 0.9876
loss: 0.0404, acc: 0.9878
loss: 0.0400, acc: 0.9879
loss: 0.0394, acc: 0.9881
loss: 0.0389, acc: 0.9884
loss: 0.0391, acc: 0.9881
loss: 0.0388, acc: 0.9881
loss: 0.0390, acc: 0.9880
loss: 0.0391, acc: 0.9880
loss: 0.0394, acc: 0.9878
loss: 0.0408, acc: 0.9876
loss: 0.0411, acc: 0.9875
loss: 0.0411, acc: 0.9874
loss: 0.0418, acc: 0.9870
loss: 0.0424, acc: 0.9869
loss: 0.0425, acc: 0.9868
loss: 0.0423, acc: 0.9867
loss: 0.0431, acc: 0.9864
loss: 0.0431, acc: 0.9864
loss: 0.0431, acc: 0.9864
loss: 0.0429, acc: 0.9864
loss: 0.0428, acc: 0.9865
loss: 0.0426, acc: 0.9866
loss: 0.0425, acc: 0.9867
> val_acc: 0.8326, val_f1: 0.8281
>> early stop.
>> test_acc: 0.8406, test_f1: 0.8344
>> test_acc: 0.8331, test_f1: 0.8264
>> test_acc: 0.8211, test_f1: 0.8130
>> test_acc: 0.8406, test_f1: 0.8344

>> avg_test_acc: 0.8316, avg_test_f1: 0.8246
>> max_test_acc: 0.8406, max_test_f1: 0.8344
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 441448448
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0859, acc: 0.4250
loss: 1.0843, acc: 0.4094
loss: 1.0877, acc: 0.3979
loss: 1.0742, acc: 0.4266
loss: 1.0604, acc: 0.4313
loss: 1.0653, acc: 0.4188
loss: 1.0569, acc: 0.4295
loss: 1.0542, acc: 0.4422
loss: 1.0468, acc: 0.4500
loss: 1.0444, acc: 0.4500
loss: 1.0330, acc: 0.4585
loss: 1.0271, acc: 0.4641
loss: 1.0165, acc: 0.4726
loss: 1.0117, acc: 0.4777
loss: 1.0087, acc: 0.4813
loss: 1.0037, acc: 0.4883
loss: 0.9981, acc: 0.4919
loss: 0.9845, acc: 0.5031
loss: 0.9787, acc: 0.5105
loss: 0.9679, acc: 0.5181
loss: 0.9616, acc: 0.5217
loss: 0.9567, acc: 0.5241
loss: 0.9525, acc: 0.5255
loss: 0.9419, acc: 0.5326
loss: 0.9335, acc: 0.5395
loss: 0.9313, acc: 0.5411
loss: 0.9304, acc: 0.5410
loss: 0.9233, acc: 0.5469
loss: 0.9132, acc: 0.5539
loss: 0.9109, acc: 0.5573
loss: 0.9038, acc: 0.5623
loss: 0.8965, acc: 0.5668
loss: 0.8883, acc: 0.5722
loss: 0.8819, acc: 0.5767
loss: 0.8789, acc: 0.5786
loss: 0.8705, acc: 0.5830
loss: 0.8659, acc: 0.5850
loss: 0.8605, acc: 0.5882
loss: 0.8565, acc: 0.5893
loss: 0.8512, acc: 0.5927
loss: 0.8445, acc: 0.5976
loss: 0.8401, acc: 0.6006
loss: 0.8358, acc: 0.6033
loss: 0.8324, acc: 0.6058
loss: 0.8280, acc: 0.6086
loss: 0.8218, acc: 0.6129
loss: 0.8164, acc: 0.6173
loss: 0.8121, acc: 0.6207
loss: 0.8078, acc: 0.6236
loss: 0.8027, acc: 0.6271
loss: 0.7993, acc: 0.6297
loss: 0.7927, acc: 0.6337
loss: 0.7890, acc: 0.6364
loss: 0.7867, acc: 0.6387
loss: 0.7851, acc: 0.6399
loss: 0.7815, acc: 0.6419
loss: 0.7785, acc: 0.6429
loss: 0.7751, acc: 0.6449
loss: 0.7700, acc: 0.6479
loss: 0.7644, acc: 0.6508
loss: 0.7611, acc: 0.6529
loss: 0.7575, acc: 0.6546
loss: 0.7552, acc: 0.6558
loss: 0.7531, acc: 0.6574
loss: 0.7509, acc: 0.6587
loss: 0.7490, acc: 0.6601
loss: 0.7473, acc: 0.6608
loss: 0.7455, acc: 0.6619
loss: 0.7431, acc: 0.6631
loss: 0.7405, acc: 0.6649
> val_acc: 0.7695, val_f1: 0.7502
>> saved: peft/bert_lora/mams//acc_0.7695_f1_0.7502_230828-0754
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.4318, acc: 0.8438
loss: 0.4876, acc: 0.8125
loss: 0.4691, acc: 0.8167
loss: 0.4720, acc: 0.8172
loss: 0.4781, acc: 0.8100
loss: 0.4997, acc: 0.7969
loss: 0.5117, acc: 0.7920
loss: 0.5167, acc: 0.7914
loss: 0.5232, acc: 0.7868
loss: 0.5244, acc: 0.7850
loss: 0.5252, acc: 0.7852
loss: 0.5198, acc: 0.7880
loss: 0.5208, acc: 0.7880
loss: 0.5183, acc: 0.7866
loss: 0.5192, acc: 0.7858
loss: 0.5270, acc: 0.7832
loss: 0.5213, acc: 0.7846
loss: 0.5218, acc: 0.7837
loss: 0.5230, acc: 0.7832
loss: 0.5207, acc: 0.7847
loss: 0.5213, acc: 0.7845
loss: 0.5239, acc: 0.7858
loss: 0.5240, acc: 0.7861
loss: 0.5245, acc: 0.7862
loss: 0.5223, acc: 0.7883
loss: 0.5223, acc: 0.7889
loss: 0.5192, acc: 0.7900
loss: 0.5157, acc: 0.7935
loss: 0.5139, acc: 0.7953
loss: 0.5147, acc: 0.7944
loss: 0.5180, acc: 0.7923
loss: 0.5159, acc: 0.7934
loss: 0.5169, acc: 0.7932
loss: 0.5151, acc: 0.7937
loss: 0.5147, acc: 0.7946
loss: 0.5150, acc: 0.7943
loss: 0.5150, acc: 0.7944
loss: 0.5150, acc: 0.7949
loss: 0.5152, acc: 0.7947
loss: 0.5149, acc: 0.7952
loss: 0.5133, acc: 0.7962
loss: 0.5109, acc: 0.7972
loss: 0.5124, acc: 0.7967
loss: 0.5092, acc: 0.7986
loss: 0.5101, acc: 0.7983
loss: 0.5108, acc: 0.7981
loss: 0.5107, acc: 0.7983
loss: 0.5126, acc: 0.7969
loss: 0.5103, acc: 0.7977
loss: 0.5091, acc: 0.7986
loss: 0.5081, acc: 0.7987
loss: 0.5051, acc: 0.8002
loss: 0.5035, acc: 0.8009
loss: 0.5040, acc: 0.8000
loss: 0.5039, acc: 0.8002
loss: 0.5033, acc: 0.8008
loss: 0.5025, acc: 0.8008
loss: 0.5033, acc: 0.8003
loss: 0.5019, acc: 0.8010
loss: 0.5018, acc: 0.8010
loss: 0.5024, acc: 0.8008
loss: 0.5017, acc: 0.8010
loss: 0.5016, acc: 0.8011
loss: 0.4998, acc: 0.8020
loss: 0.4989, acc: 0.8022
loss: 0.4973, acc: 0.8032
loss: 0.4971, acc: 0.8035
loss: 0.4968, acc: 0.8036
loss: 0.4971, acc: 0.8035
loss: 0.4974, acc: 0.8030
> val_acc: 0.7778, val_f1: 0.7736
>> saved: peft/bert_lora/mams//acc_0.7778_f1_0.7736_230828-0755
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3825, acc: 0.8438
loss: 0.3849, acc: 0.8438
loss: 0.3772, acc: 0.8521
loss: 0.4250, acc: 0.8344
loss: 0.4666, acc: 0.8137
loss: 0.4653, acc: 0.8198
loss: 0.4590, acc: 0.8205
loss: 0.4411, acc: 0.8305
loss: 0.4325, acc: 0.8361
loss: 0.4388, acc: 0.8363
loss: 0.4368, acc: 0.8369
loss: 0.4373, acc: 0.8344
loss: 0.4366, acc: 0.8332
loss: 0.4409, acc: 0.8321
loss: 0.4402, acc: 0.8350
loss: 0.4385, acc: 0.8355
loss: 0.4395, acc: 0.8357
loss: 0.4360, acc: 0.8365
loss: 0.4414, acc: 0.8349
loss: 0.4383, acc: 0.8366
loss: 0.4336, acc: 0.8378
loss: 0.4321, acc: 0.8392
loss: 0.4347, acc: 0.8383
loss: 0.4308, acc: 0.8398
loss: 0.4283, acc: 0.8410
loss: 0.4305, acc: 0.8404
loss: 0.4261, acc: 0.8424
loss: 0.4277, acc: 0.8406
loss: 0.4276, acc: 0.8397
loss: 0.4249, acc: 0.8408
loss: 0.4266, acc: 0.8401
loss: 0.4304, acc: 0.8389
loss: 0.4323, acc: 0.8375
loss: 0.4324, acc: 0.8371
loss: 0.4323, acc: 0.8377
loss: 0.4332, acc: 0.8368
loss: 0.4333, acc: 0.8367
loss: 0.4338, acc: 0.8362
loss: 0.4330, acc: 0.8365
loss: 0.4341, acc: 0.8347
loss: 0.4331, acc: 0.8355
loss: 0.4313, acc: 0.8368
loss: 0.4328, acc: 0.8368
loss: 0.4326, acc: 0.8371
loss: 0.4324, acc: 0.8374
loss: 0.4336, acc: 0.8364
loss: 0.4347, acc: 0.8359
loss: 0.4350, acc: 0.8359
loss: 0.4355, acc: 0.8358
loss: 0.4367, acc: 0.8347
loss: 0.4377, acc: 0.8338
loss: 0.4370, acc: 0.8341
loss: 0.4377, acc: 0.8340
loss: 0.4365, acc: 0.8340
loss: 0.4381, acc: 0.8337
loss: 0.4384, acc: 0.8334
loss: 0.4369, acc: 0.8340
loss: 0.4369, acc: 0.8335
loss: 0.4385, acc: 0.8329
loss: 0.4391, acc: 0.8328
loss: 0.4397, acc: 0.8327
loss: 0.4389, acc: 0.8333
loss: 0.4401, acc: 0.8326
loss: 0.4397, acc: 0.8331
loss: 0.4391, acc: 0.8336
loss: 0.4392, acc: 0.8334
loss: 0.4398, acc: 0.8332
loss: 0.4393, acc: 0.8334
loss: 0.4393, acc: 0.8334
loss: 0.4387, acc: 0.8337
> val_acc: 0.8033, val_f1: 0.7965
>> saved: peft/bert_lora/mams//acc_0.8033_f1_0.7965_230828-0757
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.3326, acc: 0.8688
loss: 0.3496, acc: 0.8500
loss: 0.3525, acc: 0.8604
loss: 0.3611, acc: 0.8625
loss: 0.3413, acc: 0.8675
loss: 0.3488, acc: 0.8615
loss: 0.3439, acc: 0.8652
loss: 0.3607, acc: 0.8555
loss: 0.3631, acc: 0.8528
loss: 0.3603, acc: 0.8538
loss: 0.3667, acc: 0.8528
loss: 0.3742, acc: 0.8495
loss: 0.3767, acc: 0.8500
loss: 0.3680, acc: 0.8536
loss: 0.3698, acc: 0.8546
loss: 0.3671, acc: 0.8570
loss: 0.3663, acc: 0.8577
loss: 0.3643, acc: 0.8576
loss: 0.3618, acc: 0.8579
loss: 0.3586, acc: 0.8600
loss: 0.3562, acc: 0.8604
loss: 0.3560, acc: 0.8622
loss: 0.3579, acc: 0.8611
loss: 0.3628, acc: 0.8596
loss: 0.3619, acc: 0.8592
loss: 0.3583, acc: 0.8620
loss: 0.3612, acc: 0.8606
loss: 0.3613, acc: 0.8594
loss: 0.3650, acc: 0.8588
loss: 0.3675, acc: 0.8577
loss: 0.3709, acc: 0.8558
loss: 0.3707, acc: 0.8551
loss: 0.3710, acc: 0.8555
loss: 0.3729, acc: 0.8546
loss: 0.3713, acc: 0.8559
loss: 0.3729, acc: 0.8557
loss: 0.3753, acc: 0.8546
loss: 0.3753, acc: 0.8535
loss: 0.3764, acc: 0.8530
loss: 0.3766, acc: 0.8536
loss: 0.3776, acc: 0.8530
loss: 0.3775, acc: 0.8530
loss: 0.3769, acc: 0.8531
loss: 0.3786, acc: 0.8537
loss: 0.3800, acc: 0.8529
loss: 0.3829, acc: 0.8515
loss: 0.3815, acc: 0.8524
loss: 0.3824, acc: 0.8518
loss: 0.3833, acc: 0.8517
loss: 0.3833, acc: 0.8519
loss: 0.3817, acc: 0.8526
loss: 0.3806, acc: 0.8532
loss: 0.3795, acc: 0.8535
loss: 0.3808, acc: 0.8537
loss: 0.3813, acc: 0.8532
loss: 0.3819, acc: 0.8530
loss: 0.3828, acc: 0.8529
loss: 0.3841, acc: 0.8522
loss: 0.3855, acc: 0.8512
loss: 0.3854, acc: 0.8512
loss: 0.3848, acc: 0.8513
loss: 0.3849, acc: 0.8513
loss: 0.3842, acc: 0.8516
loss: 0.3835, acc: 0.8519
loss: 0.3838, acc: 0.8519
loss: 0.3841, acc: 0.8514
loss: 0.3857, acc: 0.8509
loss: 0.3865, acc: 0.8506
loss: 0.3859, acc: 0.8507
loss: 0.3855, acc: 0.8507
> val_acc: 0.8071, val_f1: 0.8036
>> saved: peft/bert_lora/mams//acc_0.8071_f1_0.8036_230828-0758
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3706, acc: 0.8625
loss: 0.3142, acc: 0.8719
loss: 0.3103, acc: 0.8750
loss: 0.3181, acc: 0.8656
loss: 0.3414, acc: 0.8588
loss: 0.3329, acc: 0.8615
loss: 0.3617, acc: 0.8536
loss: 0.3648, acc: 0.8555
loss: 0.3765, acc: 0.8486
loss: 0.3729, acc: 0.8525
loss: 0.3722, acc: 0.8551
loss: 0.3746, acc: 0.8557
loss: 0.3691, acc: 0.8587
loss: 0.3665, acc: 0.8589
loss: 0.3598, acc: 0.8608
loss: 0.3568, acc: 0.8625
loss: 0.3580, acc: 0.8618
loss: 0.3599, acc: 0.8615
loss: 0.3561, acc: 0.8632
loss: 0.3569, acc: 0.8631
loss: 0.3536, acc: 0.8637
loss: 0.3569, acc: 0.8616
loss: 0.3606, acc: 0.8606
loss: 0.3580, acc: 0.8612
loss: 0.3583, acc: 0.8608
loss: 0.3612, acc: 0.8579
loss: 0.3601, acc: 0.8586
loss: 0.3584, acc: 0.8598
loss: 0.3568, acc: 0.8603
loss: 0.3564, acc: 0.8602
loss: 0.3535, acc: 0.8617
loss: 0.3541, acc: 0.8619
loss: 0.3527, acc: 0.8627
loss: 0.3507, acc: 0.8636
loss: 0.3518, acc: 0.8630
loss: 0.3525, acc: 0.8630
loss: 0.3528, acc: 0.8639
loss: 0.3541, acc: 0.8637
loss: 0.3524, acc: 0.8647
loss: 0.3546, acc: 0.8641
loss: 0.3534, acc: 0.8648
loss: 0.3549, acc: 0.8638
loss: 0.3539, acc: 0.8644
loss: 0.3535, acc: 0.8648
loss: 0.3534, acc: 0.8653
loss: 0.3554, acc: 0.8643
loss: 0.3563, acc: 0.8638
loss: 0.3583, acc: 0.8633
loss: 0.3596, acc: 0.8624
loss: 0.3608, acc: 0.8615
loss: 0.3582, acc: 0.8629
loss: 0.3577, acc: 0.8633
loss: 0.3589, acc: 0.8632
loss: 0.3589, acc: 0.8634
loss: 0.3600, acc: 0.8631
loss: 0.3615, acc: 0.8626
loss: 0.3613, acc: 0.8624
loss: 0.3608, acc: 0.8625
loss: 0.3615, acc: 0.8625
loss: 0.3597, acc: 0.8632
loss: 0.3597, acc: 0.8629
loss: 0.3609, acc: 0.8627
loss: 0.3604, acc: 0.8628
loss: 0.3599, acc: 0.8629
loss: 0.3583, acc: 0.8634
loss: 0.3572, acc: 0.8638
loss: 0.3599, acc: 0.8628
loss: 0.3618, acc: 0.8610
loss: 0.3631, acc: 0.8601
loss: 0.3643, acc: 0.8595
> val_acc: 0.8071, val_f1: 0.8023
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3551, acc: 0.8750
loss: 0.3392, acc: 0.8875
loss: 0.3587, acc: 0.8792
loss: 0.3518, acc: 0.8797
loss: 0.3415, acc: 0.8825
loss: 0.3437, acc: 0.8802
loss: 0.3485, acc: 0.8786
loss: 0.3531, acc: 0.8766
loss: 0.3615, acc: 0.8722
loss: 0.3589, acc: 0.8725
loss: 0.3540, acc: 0.8739
loss: 0.3622, acc: 0.8672
loss: 0.3602, acc: 0.8683
loss: 0.3518, acc: 0.8714
loss: 0.3468, acc: 0.8717
loss: 0.3433, acc: 0.8742
loss: 0.3412, acc: 0.8750
loss: 0.3450, acc: 0.8726
loss: 0.3410, acc: 0.8743
loss: 0.3385, acc: 0.8762
loss: 0.3365, acc: 0.8768
loss: 0.3430, acc: 0.8750
loss: 0.3447, acc: 0.8720
loss: 0.3399, acc: 0.8742
loss: 0.3419, acc: 0.8732
loss: 0.3396, acc: 0.8743
loss: 0.3350, acc: 0.8755
loss: 0.3348, acc: 0.8754
loss: 0.3353, acc: 0.8761
loss: 0.3320, acc: 0.8769
loss: 0.3319, acc: 0.8766
loss: 0.3353, acc: 0.8754
loss: 0.3376, acc: 0.8739
loss: 0.3377, acc: 0.8739
loss: 0.3374, acc: 0.8743
loss: 0.3367, acc: 0.8745
loss: 0.3366, acc: 0.8738
loss: 0.3400, acc: 0.8729
loss: 0.3399, acc: 0.8729
loss: 0.3419, acc: 0.8727
loss: 0.3434, acc: 0.8723
loss: 0.3443, acc: 0.8720
loss: 0.3438, acc: 0.8727
loss: 0.3439, acc: 0.8726
loss: 0.3428, acc: 0.8725
loss: 0.3425, acc: 0.8728
loss: 0.3434, acc: 0.8723
loss: 0.3443, acc: 0.8719
loss: 0.3430, acc: 0.8724
loss: 0.3410, acc: 0.8732
loss: 0.3402, acc: 0.8735
loss: 0.3391, acc: 0.8737
loss: 0.3404, acc: 0.8732
loss: 0.3402, acc: 0.8733
loss: 0.3408, acc: 0.8727
loss: 0.3414, acc: 0.8723
loss: 0.3389, acc: 0.8729
loss: 0.3395, acc: 0.8726
loss: 0.3394, acc: 0.8724
loss: 0.3386, acc: 0.8726
loss: 0.3389, acc: 0.8723
loss: 0.3395, acc: 0.8721
loss: 0.3398, acc: 0.8718
loss: 0.3407, acc: 0.8711
loss: 0.3418, acc: 0.8707
loss: 0.3423, acc: 0.8703
loss: 0.3430, acc: 0.8701
loss: 0.3429, acc: 0.8701
loss: 0.3434, acc: 0.8699
loss: 0.3438, acc: 0.8696
> val_acc: 0.7965, val_f1: 0.7940
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2818, acc: 0.8750
loss: 0.3273, acc: 0.8656
loss: 0.2892, acc: 0.8833
loss: 0.2916, acc: 0.8797
loss: 0.2769, acc: 0.8888
loss: 0.2926, acc: 0.8896
loss: 0.2864, acc: 0.8938
loss: 0.2875, acc: 0.8922
loss: 0.2936, acc: 0.8896
loss: 0.2913, acc: 0.8912
loss: 0.2921, acc: 0.8898
loss: 0.2893, acc: 0.8922
loss: 0.2917, acc: 0.8923
loss: 0.2900, acc: 0.8938
loss: 0.2846, acc: 0.8962
loss: 0.2855, acc: 0.8949
loss: 0.2852, acc: 0.8960
loss: 0.2831, acc: 0.8965
loss: 0.2846, acc: 0.8964
loss: 0.2832, acc: 0.8972
loss: 0.2856, acc: 0.8961
loss: 0.2863, acc: 0.8960
loss: 0.2872, acc: 0.8954
loss: 0.2913, acc: 0.8922
loss: 0.2959, acc: 0.8898
loss: 0.3010, acc: 0.8870
loss: 0.2979, acc: 0.8889
loss: 0.2992, acc: 0.8886
loss: 0.3014, acc: 0.8873
loss: 0.3033, acc: 0.8862
loss: 0.3051, acc: 0.8847
loss: 0.3082, acc: 0.8832
loss: 0.3109, acc: 0.8814
loss: 0.3105, acc: 0.8822
loss: 0.3114, acc: 0.8818
loss: 0.3108, acc: 0.8809
loss: 0.3108, acc: 0.8807
loss: 0.3122, acc: 0.8799
loss: 0.3126, acc: 0.8798
loss: 0.3116, acc: 0.8805
loss: 0.3101, acc: 0.8811
loss: 0.3124, acc: 0.8804
loss: 0.3121, acc: 0.8807
loss: 0.3126, acc: 0.8801
loss: 0.3134, acc: 0.8800
loss: 0.3140, acc: 0.8798
loss: 0.3132, acc: 0.8802
loss: 0.3126, acc: 0.8801
loss: 0.3117, acc: 0.8802
loss: 0.3097, acc: 0.8808
loss: 0.3102, acc: 0.8806
loss: 0.3090, acc: 0.8815
loss: 0.3088, acc: 0.8810
loss: 0.3068, acc: 0.8819
loss: 0.3062, acc: 0.8822
loss: 0.3040, acc: 0.8835
loss: 0.3044, acc: 0.8826
loss: 0.3057, acc: 0.8818
loss: 0.3069, acc: 0.8816
loss: 0.3068, acc: 0.8812
loss: 0.3080, acc: 0.8806
loss: 0.3080, acc: 0.8806
loss: 0.3084, acc: 0.8806
loss: 0.3091, acc: 0.8806
loss: 0.3083, acc: 0.8810
loss: 0.3087, acc: 0.8810
loss: 0.3090, acc: 0.8810
loss: 0.3095, acc: 0.8807
loss: 0.3081, acc: 0.8812
loss: 0.3081, acc: 0.8812
> val_acc: 0.7928, val_f1: 0.7887
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2083, acc: 0.9125
loss: 0.2082, acc: 0.9187
loss: 0.2218, acc: 0.9104
loss: 0.2134, acc: 0.9109
loss: 0.2123, acc: 0.9150
loss: 0.2148, acc: 0.9167
loss: 0.2239, acc: 0.9152
loss: 0.2289, acc: 0.9133
loss: 0.2374, acc: 0.9090
loss: 0.2426, acc: 0.9056
loss: 0.2444, acc: 0.9062
loss: 0.2423, acc: 0.9078
loss: 0.2554, acc: 0.9053
loss: 0.2572, acc: 0.9022
loss: 0.2570, acc: 0.9046
loss: 0.2566, acc: 0.9043
loss: 0.2573, acc: 0.9029
loss: 0.2629, acc: 0.9014
loss: 0.2653, acc: 0.8987
loss: 0.2671, acc: 0.8975
loss: 0.2673, acc: 0.8979
loss: 0.2649, acc: 0.8997
loss: 0.2654, acc: 0.9008
loss: 0.2664, acc: 0.9005
loss: 0.2649, acc: 0.9022
loss: 0.2651, acc: 0.9017
loss: 0.2645, acc: 0.9019
loss: 0.2655, acc: 0.9016
loss: 0.2687, acc: 0.9009
loss: 0.2666, acc: 0.9025
loss: 0.2667, acc: 0.9026
loss: 0.2689, acc: 0.9014
loss: 0.2710, acc: 0.9006
loss: 0.2714, acc: 0.9002
loss: 0.2724, acc: 0.8996
loss: 0.2731, acc: 0.8993
loss: 0.2756, acc: 0.8988
loss: 0.2746, acc: 0.8997
loss: 0.2733, acc: 0.9002
loss: 0.2737, acc: 0.8997
loss: 0.2745, acc: 0.8994
loss: 0.2752, acc: 0.8993
loss: 0.2749, acc: 0.8997
loss: 0.2764, acc: 0.8994
loss: 0.2760, acc: 0.8992
loss: 0.2765, acc: 0.8988
loss: 0.2756, acc: 0.8993
loss: 0.2744, acc: 0.8999
loss: 0.2749, acc: 0.8997
loss: 0.2732, acc: 0.9010
loss: 0.2731, acc: 0.9007
loss: 0.2750, acc: 0.8998
loss: 0.2741, acc: 0.9000
loss: 0.2751, acc: 0.8998
loss: 0.2755, acc: 0.8994
loss: 0.2764, acc: 0.8985
loss: 0.2775, acc: 0.8982
loss: 0.2776, acc: 0.8983
loss: 0.2775, acc: 0.8981
loss: 0.2775, acc: 0.8981
loss: 0.2779, acc: 0.8980
loss: 0.2780, acc: 0.8979
loss: 0.2785, acc: 0.8975
loss: 0.2787, acc: 0.8971
loss: 0.2795, acc: 0.8970
loss: 0.2812, acc: 0.8963
loss: 0.2814, acc: 0.8963
loss: 0.2830, acc: 0.8957
loss: 0.2837, acc: 0.8955
loss: 0.2846, acc: 0.8950
> val_acc: 0.8251, val_f1: 0.8181
>> saved: peft/bert_lora/mams//acc_0.8251_f1_0.8181_230828-0804
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2106, acc: 0.9125
loss: 0.2507, acc: 0.8906
loss: 0.2625, acc: 0.8958
loss: 0.2510, acc: 0.9031
loss: 0.2471, acc: 0.9038
loss: 0.2699, acc: 0.8958
loss: 0.2632, acc: 0.8946
loss: 0.2708, acc: 0.8875
loss: 0.2733, acc: 0.8889
loss: 0.2801, acc: 0.8894
loss: 0.2754, acc: 0.8938
loss: 0.2738, acc: 0.8943
loss: 0.2762, acc: 0.8933
loss: 0.2759, acc: 0.8929
loss: 0.2694, acc: 0.8950
loss: 0.2678, acc: 0.8961
loss: 0.2666, acc: 0.8971
loss: 0.2621, acc: 0.8990
loss: 0.2602, acc: 0.9000
loss: 0.2629, acc: 0.9003
loss: 0.2667, acc: 0.8982
loss: 0.2677, acc: 0.8977
loss: 0.2642, acc: 0.8995
loss: 0.2608, acc: 0.9010
loss: 0.2574, acc: 0.9020
loss: 0.2558, acc: 0.9024
loss: 0.2558, acc: 0.9025
loss: 0.2582, acc: 0.9020
loss: 0.2597, acc: 0.9019
loss: 0.2600, acc: 0.9017
loss: 0.2592, acc: 0.9020
loss: 0.2583, acc: 0.9021
loss: 0.2591, acc: 0.9015
loss: 0.2616, acc: 0.9002
loss: 0.2611, acc: 0.9007
loss: 0.2618, acc: 0.9007
loss: 0.2619, acc: 0.9000
loss: 0.2623, acc: 0.9000
loss: 0.2630, acc: 0.8998
loss: 0.2632, acc: 0.8992
loss: 0.2623, acc: 0.8994
loss: 0.2620, acc: 0.8990
loss: 0.2589, acc: 0.9003
loss: 0.2585, acc: 0.9009
loss: 0.2585, acc: 0.9010
loss: 0.2577, acc: 0.9008
loss: 0.2575, acc: 0.9012
loss: 0.2582, acc: 0.9004
loss: 0.2584, acc: 0.8996
loss: 0.2599, acc: 0.8996
loss: 0.2594, acc: 0.8999
loss: 0.2597, acc: 0.9000
loss: 0.2605, acc: 0.8998
loss: 0.2606, acc: 0.8999
loss: 0.2609, acc: 0.8998
loss: 0.2621, acc: 0.8993
loss: 0.2620, acc: 0.8992
loss: 0.2626, acc: 0.8988
loss: 0.2624, acc: 0.8987
loss: 0.2627, acc: 0.8983
loss: 0.2628, acc: 0.8978
loss: 0.2642, acc: 0.8974
loss: 0.2647, acc: 0.8971
loss: 0.2654, acc: 0.8975
loss: 0.2664, acc: 0.8972
loss: 0.2683, acc: 0.8969
loss: 0.2680, acc: 0.8973
loss: 0.2670, acc: 0.8973
loss: 0.2689, acc: 0.8972
loss: 0.2686, acc: 0.8974
> val_acc: 0.8273, val_f1: 0.8220
>> saved: peft/bert_lora/mams//acc_0.8273_f1_0.822_230828-0806
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.2650, acc: 0.9062
loss: 0.2275, acc: 0.9219
loss: 0.2105, acc: 0.9292
loss: 0.2263, acc: 0.9234
loss: 0.2279, acc: 0.9187
loss: 0.2219, acc: 0.9187
loss: 0.2119, acc: 0.9205
loss: 0.2114, acc: 0.9227
loss: 0.2132, acc: 0.9236
loss: 0.2184, acc: 0.9206
loss: 0.2210, acc: 0.9193
loss: 0.2209, acc: 0.9187
loss: 0.2173, acc: 0.9212
loss: 0.2233, acc: 0.9196
loss: 0.2233, acc: 0.9192
loss: 0.2214, acc: 0.9195
loss: 0.2216, acc: 0.9202
loss: 0.2207, acc: 0.9205
loss: 0.2220, acc: 0.9184
loss: 0.2213, acc: 0.9181
loss: 0.2234, acc: 0.9176
loss: 0.2223, acc: 0.9182
loss: 0.2236, acc: 0.9177
loss: 0.2244, acc: 0.9174
loss: 0.2263, acc: 0.9153
loss: 0.2267, acc: 0.9149
loss: 0.2241, acc: 0.9164
loss: 0.2256, acc: 0.9163
loss: 0.2303, acc: 0.9142
loss: 0.2311, acc: 0.9146
loss: 0.2326, acc: 0.9133
loss: 0.2319, acc: 0.9139
loss: 0.2321, acc: 0.9136
loss: 0.2332, acc: 0.9134
loss: 0.2345, acc: 0.9127
loss: 0.2352, acc: 0.9120
loss: 0.2363, acc: 0.9117
loss: 0.2366, acc: 0.9113
loss: 0.2352, acc: 0.9117
loss: 0.2365, acc: 0.9114
loss: 0.2374, acc: 0.9111
loss: 0.2353, acc: 0.9121
loss: 0.2367, acc: 0.9113
loss: 0.2371, acc: 0.9114
loss: 0.2388, acc: 0.9107
loss: 0.2380, acc: 0.9111
loss: 0.2391, acc: 0.9106
loss: 0.2410, acc: 0.9095
loss: 0.2414, acc: 0.9097
loss: 0.2408, acc: 0.9103
loss: 0.2420, acc: 0.9096
loss: 0.2432, acc: 0.9095
loss: 0.2424, acc: 0.9096
loss: 0.2436, acc: 0.9089
loss: 0.2427, acc: 0.9094
loss: 0.2455, acc: 0.9081
loss: 0.2477, acc: 0.9072
loss: 0.2492, acc: 0.9066
loss: 0.2499, acc: 0.9066
loss: 0.2502, acc: 0.9066
loss: 0.2509, acc: 0.9060
loss: 0.2520, acc: 0.9050
loss: 0.2519, acc: 0.9051
loss: 0.2523, acc: 0.9046
loss: 0.2522, acc: 0.9046
loss: 0.2537, acc: 0.9047
loss: 0.2535, acc: 0.9049
loss: 0.2527, acc: 0.9051
loss: 0.2528, acc: 0.9048
loss: 0.2541, acc: 0.9043
> val_acc: 0.8048, val_f1: 0.7935
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2138, acc: 0.9187
loss: 0.2003, acc: 0.9219
loss: 0.2285, acc: 0.9083
loss: 0.2338, acc: 0.9094
loss: 0.2286, acc: 0.9163
loss: 0.2215, acc: 0.9208
loss: 0.2143, acc: 0.9241
loss: 0.2165, acc: 0.9219
loss: 0.2092, acc: 0.9229
loss: 0.2092, acc: 0.9231
loss: 0.2078, acc: 0.9239
loss: 0.2078, acc: 0.9234
loss: 0.2104, acc: 0.9245
loss: 0.2114, acc: 0.9250
loss: 0.2124, acc: 0.9250
loss: 0.2148, acc: 0.9254
loss: 0.2198, acc: 0.9232
loss: 0.2171, acc: 0.9250
loss: 0.2225, acc: 0.9243
loss: 0.2212, acc: 0.9237
loss: 0.2212, acc: 0.9229
loss: 0.2199, acc: 0.9224
loss: 0.2226, acc: 0.9215
loss: 0.2243, acc: 0.9208
loss: 0.2233, acc: 0.9203
loss: 0.2227, acc: 0.9204
loss: 0.2222, acc: 0.9204
loss: 0.2227, acc: 0.9201
loss: 0.2230, acc: 0.9194
loss: 0.2236, acc: 0.9183
loss: 0.2278, acc: 0.9171
loss: 0.2298, acc: 0.9162
loss: 0.2259, acc: 0.9180
loss: 0.2254, acc: 0.9187
loss: 0.2251, acc: 0.9184
loss: 0.2232, acc: 0.9193
loss: 0.2219, acc: 0.9201
loss: 0.2231, acc: 0.9191
loss: 0.2244, acc: 0.9191
loss: 0.2234, acc: 0.9197
loss: 0.2234, acc: 0.9198
loss: 0.2242, acc: 0.9192
loss: 0.2254, acc: 0.9183
loss: 0.2266, acc: 0.9172
loss: 0.2283, acc: 0.9157
loss: 0.2283, acc: 0.9156
loss: 0.2292, acc: 0.9153
loss: 0.2299, acc: 0.9152
loss: 0.2303, acc: 0.9151
loss: 0.2340, acc: 0.9139
loss: 0.2341, acc: 0.9134
loss: 0.2348, acc: 0.9130
loss: 0.2346, acc: 0.9132
loss: 0.2333, acc: 0.9134
loss: 0.2329, acc: 0.9133
loss: 0.2352, acc: 0.9123
loss: 0.2367, acc: 0.9117
loss: 0.2385, acc: 0.9107
loss: 0.2390, acc: 0.9104
loss: 0.2397, acc: 0.9105
loss: 0.2404, acc: 0.9101
loss: 0.2401, acc: 0.9100
loss: 0.2394, acc: 0.9103
loss: 0.2393, acc: 0.9104
loss: 0.2404, acc: 0.9101
loss: 0.2404, acc: 0.9099
loss: 0.2415, acc: 0.9097
loss: 0.2424, acc: 0.9093
loss: 0.2439, acc: 0.9086
loss: 0.2441, acc: 0.9085
> val_acc: 0.8243, val_f1: 0.8192
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1857, acc: 0.9437
loss: 0.1983, acc: 0.9281
loss: 0.2260, acc: 0.9125
loss: 0.2113, acc: 0.9187
loss: 0.2203, acc: 0.9150
loss: 0.2304, acc: 0.9125
loss: 0.2375, acc: 0.9107
loss: 0.2381, acc: 0.9086
loss: 0.2344, acc: 0.9097
loss: 0.2341, acc: 0.9069
loss: 0.2343, acc: 0.9068
loss: 0.2326, acc: 0.9109
loss: 0.2349, acc: 0.9091
loss: 0.2337, acc: 0.9089
loss: 0.2271, acc: 0.9117
loss: 0.2282, acc: 0.9109
loss: 0.2315, acc: 0.9107
loss: 0.2316, acc: 0.9094
loss: 0.2325, acc: 0.9105
loss: 0.2342, acc: 0.9097
loss: 0.2369, acc: 0.9083
loss: 0.2362, acc: 0.9091
loss: 0.2338, acc: 0.9095
loss: 0.2337, acc: 0.9094
loss: 0.2362, acc: 0.9095
loss: 0.2368, acc: 0.9091
loss: 0.2430, acc: 0.9074
loss: 0.2496, acc: 0.9054
loss: 0.2520, acc: 0.9045
loss: 0.2492, acc: 0.9058
loss: 0.2513, acc: 0.9054
loss: 0.2499, acc: 0.9059
loss: 0.2521, acc: 0.9042
loss: 0.2518, acc: 0.9044
loss: 0.2527, acc: 0.9039
loss: 0.2528, acc: 0.9040
loss: 0.2547, acc: 0.9029
loss: 0.2574, acc: 0.9023
loss: 0.2579, acc: 0.9022
loss: 0.2566, acc: 0.9027
loss: 0.2562, acc: 0.9030
loss: 0.2590, acc: 0.9019
loss: 0.2588, acc: 0.9020
loss: 0.2591, acc: 0.9021
loss: 0.2581, acc: 0.9025
loss: 0.2580, acc: 0.9026
loss: 0.2578, acc: 0.9024
loss: 0.2561, acc: 0.9029
loss: 0.2572, acc: 0.9031
loss: 0.2561, acc: 0.9031
loss: 0.2565, acc: 0.9025
loss: 0.2570, acc: 0.9023
loss: 0.2571, acc: 0.9025
loss: 0.2577, acc: 0.9024
loss: 0.2580, acc: 0.9024
loss: 0.2569, acc: 0.9030
loss: 0.2565, acc: 0.9034
loss: 0.2578, acc: 0.9028
loss: 0.2571, acc: 0.9032
loss: 0.2577, acc: 0.9027
loss: 0.2600, acc: 0.9023
loss: 0.2606, acc: 0.9021
loss: 0.2597, acc: 0.9026
loss: 0.2594, acc: 0.9026
loss: 0.2586, acc: 0.9028
loss: 0.2579, acc: 0.9028
loss: 0.2575, acc: 0.9027
loss: 0.2562, acc: 0.9028
loss: 0.2545, acc: 0.9034
loss: 0.2538, acc: 0.9034
> val_acc: 0.8093, val_f1: 0.7992
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1463, acc: 0.9437
loss: 0.1868, acc: 0.9281
loss: 0.2032, acc: 0.9271
loss: 0.2111, acc: 0.9297
loss: 0.1997, acc: 0.9337
loss: 0.2073, acc: 0.9302
loss: 0.1966, acc: 0.9330
loss: 0.2039, acc: 0.9328
loss: 0.2037, acc: 0.9306
loss: 0.2139, acc: 0.9275
loss: 0.2180, acc: 0.9261
loss: 0.2194, acc: 0.9250
loss: 0.2206, acc: 0.9250
loss: 0.2233, acc: 0.9219
loss: 0.2203, acc: 0.9229
loss: 0.2169, acc: 0.9230
loss: 0.2165, acc: 0.9232
loss: 0.2159, acc: 0.9236
loss: 0.2179, acc: 0.9220
loss: 0.2178, acc: 0.9216
loss: 0.2181, acc: 0.9214
loss: 0.2161, acc: 0.9224
loss: 0.2166, acc: 0.9223
loss: 0.2201, acc: 0.9203
loss: 0.2204, acc: 0.9205
loss: 0.2169, acc: 0.9221
loss: 0.2207, acc: 0.9215
loss: 0.2225, acc: 0.9192
loss: 0.2208, acc: 0.9196
loss: 0.2224, acc: 0.9187
loss: 0.2217, acc: 0.9187
loss: 0.2210, acc: 0.9191
loss: 0.2256, acc: 0.9176
loss: 0.2264, acc: 0.9169
loss: 0.2267, acc: 0.9170
loss: 0.2278, acc: 0.9167
loss: 0.2286, acc: 0.9162
loss: 0.2280, acc: 0.9166
loss: 0.2266, acc: 0.9176
loss: 0.2288, acc: 0.9167
loss: 0.2287, acc: 0.9165
loss: 0.2296, acc: 0.9159
loss: 0.2304, acc: 0.9153
loss: 0.2313, acc: 0.9148
loss: 0.2318, acc: 0.9146
loss: 0.2312, acc: 0.9149
loss: 0.2323, acc: 0.9150
loss: 0.2334, acc: 0.9150
loss: 0.2329, acc: 0.9151
loss: 0.2338, acc: 0.9140
loss: 0.2361, acc: 0.9126
loss: 0.2365, acc: 0.9127
loss: 0.2374, acc: 0.9125
loss: 0.2392, acc: 0.9123
loss: 0.2382, acc: 0.9126
loss: 0.2390, acc: 0.9125
loss: 0.2385, acc: 0.9126
loss: 0.2385, acc: 0.9128
loss: 0.2377, acc: 0.9128
loss: 0.2381, acc: 0.9124
loss: 0.2376, acc: 0.9126
loss: 0.2371, acc: 0.9124
loss: 0.2382, acc: 0.9121
loss: 0.2379, acc: 0.9122
loss: 0.2383, acc: 0.9119
loss: 0.2392, acc: 0.9116
loss: 0.2386, acc: 0.9121
loss: 0.2388, acc: 0.9121
loss: 0.2380, acc: 0.9122
loss: 0.2380, acc: 0.9122
> val_acc: 0.8116, val_f1: 0.8077
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.1850, acc: 0.9375
loss: 0.2061, acc: 0.9187
loss: 0.2195, acc: 0.9146
loss: 0.2278, acc: 0.9047
loss: 0.2152, acc: 0.9100
loss: 0.2201, acc: 0.9073
loss: 0.2169, acc: 0.9098
loss: 0.2143, acc: 0.9125
loss: 0.2074, acc: 0.9153
loss: 0.2031, acc: 0.9187
loss: 0.2050, acc: 0.9182
loss: 0.2003, acc: 0.9219
loss: 0.2049, acc: 0.9187
loss: 0.2009, acc: 0.9205
loss: 0.1955, acc: 0.9229
loss: 0.1991, acc: 0.9230
loss: 0.2009, acc: 0.9228
loss: 0.2065, acc: 0.9212
loss: 0.2047, acc: 0.9217
loss: 0.2014, acc: 0.9228
loss: 0.2015, acc: 0.9238
loss: 0.2003, acc: 0.9256
loss: 0.2023, acc: 0.9255
loss: 0.2035, acc: 0.9250
loss: 0.2056, acc: 0.9237
loss: 0.2074, acc: 0.9231
loss: 0.2061, acc: 0.9241
loss: 0.2050, acc: 0.9248
loss: 0.2045, acc: 0.9254
loss: 0.2067, acc: 0.9242
loss: 0.2061, acc: 0.9242
loss: 0.2055, acc: 0.9242
loss: 0.2071, acc: 0.9239
loss: 0.2076, acc: 0.9241
loss: 0.2077, acc: 0.9230
loss: 0.2093, acc: 0.9229
loss: 0.2086, acc: 0.9231
loss: 0.2073, acc: 0.9237
loss: 0.2071, acc: 0.9240
loss: 0.2069, acc: 0.9241
loss: 0.2078, acc: 0.9241
loss: 0.2080, acc: 0.9240
loss: 0.2095, acc: 0.9234
loss: 0.2085, acc: 0.9239
loss: 0.2090, acc: 0.9233
loss: 0.2084, acc: 0.9236
loss: 0.2092, acc: 0.9230
loss: 0.2117, acc: 0.9217
loss: 0.2126, acc: 0.9213
loss: 0.2141, acc: 0.9205
loss: 0.2144, acc: 0.9206
loss: 0.2141, acc: 0.9203
loss: 0.2149, acc: 0.9202
loss: 0.2142, acc: 0.9206
loss: 0.2152, acc: 0.9207
loss: 0.2178, acc: 0.9200
loss: 0.2183, acc: 0.9201
loss: 0.2192, acc: 0.9194
loss: 0.2189, acc: 0.9193
loss: 0.2187, acc: 0.9192
loss: 0.2198, acc: 0.9187
loss: 0.2199, acc: 0.9182
loss: 0.2206, acc: 0.9179
loss: 0.2208, acc: 0.9182
loss: 0.2222, acc: 0.9178
loss: 0.2216, acc: 0.9181
loss: 0.2224, acc: 0.9179
loss: 0.2219, acc: 0.9180
loss: 0.2216, acc: 0.9185
loss: 0.2232, acc: 0.9181
> val_acc: 0.8258, val_f1: 0.8180
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8323, test_f1: 0.8258
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 484357632
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1527, acc: 0.3500
loss: 1.1343, acc: 0.3750
loss: 1.1304, acc: 0.3896
loss: 1.1219, acc: 0.3984
loss: 1.1147, acc: 0.3925
loss: 1.1064, acc: 0.4104
loss: 1.0963, acc: 0.4179
loss: 1.0896, acc: 0.4250
loss: 1.0796, acc: 0.4326
loss: 1.0705, acc: 0.4394
loss: 1.0673, acc: 0.4415
loss: 1.0562, acc: 0.4495
loss: 1.0500, acc: 0.4524
loss: 1.0402, acc: 0.4616
loss: 1.0268, acc: 0.4646
loss: 1.0147, acc: 0.4766
loss: 1.0026, acc: 0.4827
loss: 1.0000, acc: 0.4854
loss: 0.9868, acc: 0.4934
loss: 0.9793, acc: 0.4984
loss: 0.9691, acc: 0.5045
loss: 0.9688, acc: 0.5068
loss: 0.9643, acc: 0.5084
loss: 0.9583, acc: 0.5133
loss: 0.9519, acc: 0.5162
loss: 0.9448, acc: 0.5214
loss: 0.9385, acc: 0.5266
loss: 0.9333, acc: 0.5308
loss: 0.9281, acc: 0.5338
loss: 0.9235, acc: 0.5387
loss: 0.9170, acc: 0.5444
loss: 0.9131, acc: 0.5477
loss: 0.9099, acc: 0.5513
loss: 0.9041, acc: 0.5563
loss: 0.8952, acc: 0.5614
loss: 0.8916, acc: 0.5644
loss: 0.8849, acc: 0.5689
loss: 0.8759, acc: 0.5743
loss: 0.8696, acc: 0.5784
loss: 0.8615, acc: 0.5836
loss: 0.8560, acc: 0.5870
loss: 0.8518, acc: 0.5899
loss: 0.8484, acc: 0.5924
loss: 0.8440, acc: 0.5955
loss: 0.8393, acc: 0.5988
loss: 0.8326, acc: 0.6031
loss: 0.8277, acc: 0.6064
loss: 0.8224, acc: 0.6091
loss: 0.8180, acc: 0.6120
loss: 0.8125, acc: 0.6156
loss: 0.8078, acc: 0.6188
loss: 0.8037, acc: 0.6219
loss: 0.7994, acc: 0.6250
loss: 0.7949, acc: 0.6279
loss: 0.7905, acc: 0.6311
loss: 0.7861, acc: 0.6344
loss: 0.7807, acc: 0.6370
loss: 0.7773, acc: 0.6389
loss: 0.7750, acc: 0.6407
loss: 0.7720, acc: 0.6427
loss: 0.7678, acc: 0.6451
loss: 0.7662, acc: 0.6470
loss: 0.7619, acc: 0.6493
loss: 0.7577, acc: 0.6515
loss: 0.7543, acc: 0.6534
loss: 0.7500, acc: 0.6553
loss: 0.7458, acc: 0.6570
loss: 0.7438, acc: 0.6584
loss: 0.7408, acc: 0.6609
loss: 0.7383, acc: 0.6627
> val_acc: 0.7740, val_f1: 0.7623
>> saved: peft/bert_lora/mams//acc_0.774_f1_0.7623_230828-0815
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5927, acc: 0.7812
loss: 0.5983, acc: 0.7656
loss: 0.5453, acc: 0.7854
loss: 0.5355, acc: 0.7891
loss: 0.5389, acc: 0.7850
loss: 0.5396, acc: 0.7812
loss: 0.5249, acc: 0.7848
loss: 0.5290, acc: 0.7789
loss: 0.5218, acc: 0.7847
loss: 0.5157, acc: 0.7900
loss: 0.5173, acc: 0.7903
loss: 0.5158, acc: 0.7901
loss: 0.5263, acc: 0.7832
loss: 0.5323, acc: 0.7790
loss: 0.5318, acc: 0.7779
loss: 0.5328, acc: 0.7777
loss: 0.5305, acc: 0.7801
loss: 0.5298, acc: 0.7830
loss: 0.5287, acc: 0.7819
loss: 0.5249, acc: 0.7841
loss: 0.5229, acc: 0.7839
loss: 0.5205, acc: 0.7847
loss: 0.5119, acc: 0.7891
loss: 0.5063, acc: 0.7924
loss: 0.5120, acc: 0.7893
loss: 0.5103, acc: 0.7892
loss: 0.5094, acc: 0.7903
loss: 0.5081, acc: 0.7908
loss: 0.5068, acc: 0.7918
loss: 0.5041, acc: 0.7937
loss: 0.5044, acc: 0.7940
loss: 0.5050, acc: 0.7937
loss: 0.5043, acc: 0.7939
loss: 0.5037, acc: 0.7943
loss: 0.5061, acc: 0.7941
loss: 0.5066, acc: 0.7946
loss: 0.5056, acc: 0.7956
loss: 0.5039, acc: 0.7969
loss: 0.5059, acc: 0.7960
loss: 0.5075, acc: 0.7958
loss: 0.5079, acc: 0.7944
loss: 0.5088, acc: 0.7939
loss: 0.5082, acc: 0.7943
loss: 0.5100, acc: 0.7926
loss: 0.5089, acc: 0.7942
loss: 0.5082, acc: 0.7951
loss: 0.5058, acc: 0.7955
loss: 0.5040, acc: 0.7957
loss: 0.5038, acc: 0.7963
loss: 0.5028, acc: 0.7959
loss: 0.5045, acc: 0.7957
loss: 0.5047, acc: 0.7958
loss: 0.5024, acc: 0.7971
loss: 0.5022, acc: 0.7964
loss: 0.5009, acc: 0.7972
loss: 0.5013, acc: 0.7973
loss: 0.5004, acc: 0.7974
loss: 0.5014, acc: 0.7976
loss: 0.5005, acc: 0.7978
loss: 0.4997, acc: 0.7982
loss: 0.4989, acc: 0.7986
loss: 0.4981, acc: 0.7990
loss: 0.4979, acc: 0.7989
loss: 0.4967, acc: 0.7993
loss: 0.4953, acc: 0.8001
loss: 0.4951, acc: 0.8007
loss: 0.4956, acc: 0.8006
loss: 0.4953, acc: 0.8011
loss: 0.4955, acc: 0.8006
loss: 0.4949, acc: 0.8007
> val_acc: 0.8101, val_f1: 0.8038
>> saved: peft/bert_lora/mams//acc_0.8101_f1_0.8038_230828-0816
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5206, acc: 0.8000
loss: 0.4566, acc: 0.8219
loss: 0.4272, acc: 0.8375
loss: 0.4135, acc: 0.8422
loss: 0.4081, acc: 0.8400
loss: 0.4181, acc: 0.8344
loss: 0.4332, acc: 0.8241
loss: 0.4352, acc: 0.8180
loss: 0.4264, acc: 0.8229
loss: 0.4431, acc: 0.8187
loss: 0.4436, acc: 0.8193
loss: 0.4446, acc: 0.8187
loss: 0.4434, acc: 0.8216
loss: 0.4390, acc: 0.8246
loss: 0.4424, acc: 0.8250
loss: 0.4426, acc: 0.8250
loss: 0.4451, acc: 0.8235
loss: 0.4461, acc: 0.8240
loss: 0.4423, acc: 0.8276
loss: 0.4367, acc: 0.8291
loss: 0.4377, acc: 0.8286
loss: 0.4344, acc: 0.8281
loss: 0.4399, acc: 0.8269
loss: 0.4400, acc: 0.8268
loss: 0.4441, acc: 0.8247
loss: 0.4408, acc: 0.8267
loss: 0.4427, acc: 0.8271
loss: 0.4470, acc: 0.8259
loss: 0.4455, acc: 0.8269
loss: 0.4461, acc: 0.8273
loss: 0.4453, acc: 0.8266
loss: 0.4436, acc: 0.8270
loss: 0.4418, acc: 0.8269
loss: 0.4421, acc: 0.8268
loss: 0.4406, acc: 0.8279
loss: 0.4370, acc: 0.8292
loss: 0.4366, acc: 0.8302
loss: 0.4365, acc: 0.8304
loss: 0.4370, acc: 0.8300
loss: 0.4384, acc: 0.8295
loss: 0.4399, acc: 0.8293
loss: 0.4394, acc: 0.8293
loss: 0.4407, acc: 0.8285
loss: 0.4418, acc: 0.8281
loss: 0.4413, acc: 0.8281
loss: 0.4392, acc: 0.8287
loss: 0.4397, acc: 0.8286
loss: 0.4384, acc: 0.8289
loss: 0.4389, acc: 0.8287
loss: 0.4390, acc: 0.8289
loss: 0.4396, acc: 0.8290
loss: 0.4400, acc: 0.8292
loss: 0.4399, acc: 0.8295
loss: 0.4412, acc: 0.8287
loss: 0.4405, acc: 0.8290
loss: 0.4422, acc: 0.8278
loss: 0.4442, acc: 0.8269
loss: 0.4432, acc: 0.8270
loss: 0.4441, acc: 0.8262
loss: 0.4438, acc: 0.8260
loss: 0.4433, acc: 0.8262
loss: 0.4446, acc: 0.8252
loss: 0.4437, acc: 0.8259
loss: 0.4428, acc: 0.8266
loss: 0.4409, acc: 0.8270
loss: 0.4401, acc: 0.8273
loss: 0.4402, acc: 0.8274
loss: 0.4412, acc: 0.8267
loss: 0.4414, acc: 0.8266
loss: 0.4413, acc: 0.8264
> val_acc: 0.8093, val_f1: 0.8043
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4783, acc: 0.8313
loss: 0.5030, acc: 0.8063
loss: 0.5023, acc: 0.8167
loss: 0.4585, acc: 0.8266
loss: 0.4293, acc: 0.8337
loss: 0.4035, acc: 0.8427
loss: 0.3797, acc: 0.8527
loss: 0.3768, acc: 0.8539
loss: 0.3843, acc: 0.8514
loss: 0.3939, acc: 0.8488
loss: 0.3930, acc: 0.8494
loss: 0.3911, acc: 0.8495
loss: 0.3982, acc: 0.8447
loss: 0.3963, acc: 0.8460
loss: 0.3958, acc: 0.8471
loss: 0.3944, acc: 0.8473
loss: 0.3974, acc: 0.8463
loss: 0.3980, acc: 0.8462
loss: 0.3967, acc: 0.8454
loss: 0.3954, acc: 0.8453
loss: 0.3961, acc: 0.8455
loss: 0.3910, acc: 0.8477
loss: 0.3933, acc: 0.8478
loss: 0.3917, acc: 0.8464
loss: 0.3913, acc: 0.8468
loss: 0.3952, acc: 0.8454
loss: 0.3980, acc: 0.8447
loss: 0.3963, acc: 0.8464
loss: 0.3998, acc: 0.8453
loss: 0.3981, acc: 0.8454
loss: 0.4002, acc: 0.8442
loss: 0.3994, acc: 0.8443
loss: 0.3976, acc: 0.8441
loss: 0.3971, acc: 0.8445
loss: 0.3965, acc: 0.8455
loss: 0.3964, acc: 0.8462
loss: 0.3944, acc: 0.8468
loss: 0.3964, acc: 0.8464
loss: 0.3962, acc: 0.8471
loss: 0.3979, acc: 0.8469
loss: 0.4007, acc: 0.8451
loss: 0.4005, acc: 0.8448
loss: 0.3978, acc: 0.8458
loss: 0.3980, acc: 0.8455
loss: 0.3996, acc: 0.8442
loss: 0.3981, acc: 0.8443
loss: 0.3946, acc: 0.8456
loss: 0.3945, acc: 0.8457
loss: 0.3958, acc: 0.8449
loss: 0.3949, acc: 0.8459
loss: 0.3954, acc: 0.8458
loss: 0.3979, acc: 0.8452
loss: 0.3988, acc: 0.8454
loss: 0.3981, acc: 0.8456
loss: 0.3984, acc: 0.8452
loss: 0.3975, acc: 0.8454
loss: 0.3967, acc: 0.8454
loss: 0.3961, acc: 0.8458
loss: 0.3970, acc: 0.8447
loss: 0.3972, acc: 0.8446
loss: 0.3954, acc: 0.8455
loss: 0.3973, acc: 0.8452
loss: 0.3984, acc: 0.8447
loss: 0.3975, acc: 0.8451
loss: 0.3974, acc: 0.8449
loss: 0.3972, acc: 0.8447
loss: 0.3975, acc: 0.8443
loss: 0.3981, acc: 0.8445
loss: 0.3987, acc: 0.8442
loss: 0.3985, acc: 0.8445
> val_acc: 0.8168, val_f1: 0.8091
>> saved: peft/bert_lora/mams//acc_0.8168_f1_0.8091_230828-0819
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3317, acc: 0.9125
loss: 0.3242, acc: 0.8969
loss: 0.3317, acc: 0.8875
loss: 0.3256, acc: 0.8828
loss: 0.3206, acc: 0.8800
loss: 0.3194, acc: 0.8812
loss: 0.3459, acc: 0.8723
loss: 0.3400, acc: 0.8750
loss: 0.3447, acc: 0.8757
loss: 0.3460, acc: 0.8750
loss: 0.3386, acc: 0.8773
loss: 0.3412, acc: 0.8745
loss: 0.3367, acc: 0.8750
loss: 0.3410, acc: 0.8714
loss: 0.3412, acc: 0.8708
loss: 0.3461, acc: 0.8684
loss: 0.3514, acc: 0.8665
loss: 0.3478, acc: 0.8684
loss: 0.3491, acc: 0.8684
loss: 0.3470, acc: 0.8703
loss: 0.3511, acc: 0.8688
loss: 0.3518, acc: 0.8682
loss: 0.3526, acc: 0.8674
loss: 0.3527, acc: 0.8661
loss: 0.3518, acc: 0.8670
loss: 0.3561, acc: 0.8656
loss: 0.3561, acc: 0.8650
loss: 0.3561, acc: 0.8641
loss: 0.3572, acc: 0.8640
loss: 0.3582, acc: 0.8633
loss: 0.3572, acc: 0.8631
loss: 0.3593, acc: 0.8617
loss: 0.3577, acc: 0.8619
loss: 0.3600, acc: 0.8616
loss: 0.3586, acc: 0.8625
loss: 0.3573, acc: 0.8634
loss: 0.3581, acc: 0.8628
loss: 0.3562, acc: 0.8637
loss: 0.3588, acc: 0.8627
loss: 0.3574, acc: 0.8631
loss: 0.3561, acc: 0.8639
loss: 0.3571, acc: 0.8638
loss: 0.3553, acc: 0.8642
loss: 0.3589, acc: 0.8634
loss: 0.3559, acc: 0.8647
loss: 0.3545, acc: 0.8651
loss: 0.3559, acc: 0.8649
loss: 0.3574, acc: 0.8643
loss: 0.3580, acc: 0.8640
loss: 0.3597, acc: 0.8638
loss: 0.3600, acc: 0.8636
loss: 0.3602, acc: 0.8637
loss: 0.3593, acc: 0.8638
loss: 0.3597, acc: 0.8635
loss: 0.3583, acc: 0.8643
loss: 0.3578, acc: 0.8642
loss: 0.3586, acc: 0.8638
loss: 0.3583, acc: 0.8639
loss: 0.3577, acc: 0.8642
loss: 0.3593, acc: 0.8635
loss: 0.3584, acc: 0.8640
loss: 0.3589, acc: 0.8637
loss: 0.3599, acc: 0.8634
loss: 0.3593, acc: 0.8635
loss: 0.3587, acc: 0.8638
loss: 0.3567, acc: 0.8644
loss: 0.3572, acc: 0.8644
loss: 0.3572, acc: 0.8642
loss: 0.3575, acc: 0.8640
loss: 0.3586, acc: 0.8635
> val_acc: 0.8086, val_f1: 0.8013
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3140, acc: 0.8750
loss: 0.2653, acc: 0.8906
loss: 0.2990, acc: 0.8750
loss: 0.2889, acc: 0.8859
loss: 0.2773, acc: 0.8875
loss: 0.2833, acc: 0.8885
loss: 0.2766, acc: 0.8929
loss: 0.2836, acc: 0.8922
loss: 0.2866, acc: 0.8924
loss: 0.2935, acc: 0.8875
loss: 0.2993, acc: 0.8864
loss: 0.2980, acc: 0.8859
loss: 0.2954, acc: 0.8865
loss: 0.2989, acc: 0.8857
loss: 0.3009, acc: 0.8846
loss: 0.3031, acc: 0.8844
loss: 0.3071, acc: 0.8842
loss: 0.3082, acc: 0.8837
loss: 0.3051, acc: 0.8852
loss: 0.3097, acc: 0.8825
loss: 0.3077, acc: 0.8839
loss: 0.3047, acc: 0.8855
loss: 0.3110, acc: 0.8842
loss: 0.3121, acc: 0.8844
loss: 0.3116, acc: 0.8852
loss: 0.3097, acc: 0.8863
loss: 0.3103, acc: 0.8863
loss: 0.3095, acc: 0.8864
loss: 0.3081, acc: 0.8866
loss: 0.3109, acc: 0.8858
loss: 0.3144, acc: 0.8843
loss: 0.3140, acc: 0.8842
loss: 0.3146, acc: 0.8835
loss: 0.3157, acc: 0.8831
loss: 0.3148, acc: 0.8829
loss: 0.3163, acc: 0.8819
loss: 0.3163, acc: 0.8819
loss: 0.3185, acc: 0.8811
loss: 0.3177, acc: 0.8817
loss: 0.3174, acc: 0.8825
loss: 0.3178, acc: 0.8828
loss: 0.3179, acc: 0.8827
loss: 0.3175, acc: 0.8824
loss: 0.3209, acc: 0.8804
loss: 0.3224, acc: 0.8803
loss: 0.3221, acc: 0.8804
loss: 0.3215, acc: 0.8807
loss: 0.3201, acc: 0.8814
loss: 0.3186, acc: 0.8816
loss: 0.3178, acc: 0.8819
loss: 0.3185, acc: 0.8816
loss: 0.3172, acc: 0.8823
loss: 0.3189, acc: 0.8816
loss: 0.3180, acc: 0.8818
loss: 0.3192, acc: 0.8816
loss: 0.3199, acc: 0.8810
loss: 0.3200, acc: 0.8804
loss: 0.3212, acc: 0.8798
loss: 0.3230, acc: 0.8789
loss: 0.3240, acc: 0.8781
loss: 0.3248, acc: 0.8775
loss: 0.3251, acc: 0.8772
loss: 0.3250, acc: 0.8776
loss: 0.3246, acc: 0.8776
loss: 0.3255, acc: 0.8770
loss: 0.3261, acc: 0.8772
loss: 0.3263, acc: 0.8769
loss: 0.3258, acc: 0.8771
loss: 0.3267, acc: 0.8768
loss: 0.3269, acc: 0.8768
> val_acc: 0.8101, val_f1: 0.8087
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.3977, acc: 0.8438
loss: 0.3284, acc: 0.8781
loss: 0.2968, acc: 0.8896
loss: 0.2851, acc: 0.8891
loss: 0.2944, acc: 0.8862
loss: 0.2968, acc: 0.8875
loss: 0.3087, acc: 0.8875
loss: 0.3134, acc: 0.8844
loss: 0.3086, acc: 0.8826
loss: 0.3110, acc: 0.8825
loss: 0.3082, acc: 0.8852
loss: 0.3130, acc: 0.8839
loss: 0.3093, acc: 0.8837
loss: 0.3085, acc: 0.8844
loss: 0.3044, acc: 0.8867
loss: 0.3046, acc: 0.8855
loss: 0.3051, acc: 0.8860
loss: 0.3028, acc: 0.8861
loss: 0.3064, acc: 0.8845
loss: 0.3091, acc: 0.8838
loss: 0.3108, acc: 0.8830
loss: 0.3095, acc: 0.8827
loss: 0.3092, acc: 0.8818
loss: 0.3062, acc: 0.8833
loss: 0.3083, acc: 0.8830
loss: 0.3072, acc: 0.8832
loss: 0.3101, acc: 0.8826
loss: 0.3095, acc: 0.8835
loss: 0.3089, acc: 0.8843
loss: 0.3095, acc: 0.8840
loss: 0.3086, acc: 0.8849
loss: 0.3083, acc: 0.8848
loss: 0.3079, acc: 0.8850
loss: 0.3080, acc: 0.8846
loss: 0.3094, acc: 0.8843
loss: 0.3085, acc: 0.8851
loss: 0.3094, acc: 0.8843
loss: 0.3091, acc: 0.8840
loss: 0.3094, acc: 0.8830
loss: 0.3082, acc: 0.8831
loss: 0.3101, acc: 0.8828
loss: 0.3113, acc: 0.8830
loss: 0.3125, acc: 0.8827
loss: 0.3131, acc: 0.8820
loss: 0.3116, acc: 0.8826
loss: 0.3098, acc: 0.8837
loss: 0.3085, acc: 0.8840
loss: 0.3100, acc: 0.8840
loss: 0.3123, acc: 0.8835
loss: 0.3129, acc: 0.8831
loss: 0.3150, acc: 0.8827
loss: 0.3137, acc: 0.8831
loss: 0.3147, acc: 0.8830
loss: 0.3153, acc: 0.8823
loss: 0.3149, acc: 0.8819
loss: 0.3150, acc: 0.8818
loss: 0.3172, acc: 0.8810
loss: 0.3159, acc: 0.8814
loss: 0.3156, acc: 0.8815
loss: 0.3150, acc: 0.8818
loss: 0.3144, acc: 0.8819
loss: 0.3151, acc: 0.8820
loss: 0.3140, acc: 0.8821
loss: 0.3126, acc: 0.8826
loss: 0.3123, acc: 0.8826
loss: 0.3119, acc: 0.8828
loss: 0.3115, acc: 0.8831
loss: 0.3116, acc: 0.8832
loss: 0.3143, acc: 0.8824
loss: 0.3144, acc: 0.8821
> val_acc: 0.8168, val_f1: 0.8096
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2437, acc: 0.9187
loss: 0.2585, acc: 0.9062
loss: 0.2907, acc: 0.8938
loss: 0.2716, acc: 0.8984
loss: 0.2754, acc: 0.8975
loss: 0.2697, acc: 0.9000
loss: 0.2624, acc: 0.8991
loss: 0.2654, acc: 0.8906
loss: 0.2584, acc: 0.8924
loss: 0.2571, acc: 0.8938
loss: 0.2551, acc: 0.8949
loss: 0.2504, acc: 0.8974
loss: 0.2599, acc: 0.8942
loss: 0.2611, acc: 0.8960
loss: 0.2691, acc: 0.8917
loss: 0.2702, acc: 0.8902
loss: 0.2673, acc: 0.8926
loss: 0.2672, acc: 0.8941
loss: 0.2645, acc: 0.8951
loss: 0.2640, acc: 0.8953
loss: 0.2677, acc: 0.8952
loss: 0.2726, acc: 0.8938
loss: 0.2746, acc: 0.8935
loss: 0.2750, acc: 0.8935
loss: 0.2738, acc: 0.8950
loss: 0.2745, acc: 0.8940
loss: 0.2718, acc: 0.8954
loss: 0.2739, acc: 0.8958
loss: 0.2743, acc: 0.8963
loss: 0.2765, acc: 0.8962
loss: 0.2768, acc: 0.8956
loss: 0.2744, acc: 0.8967
loss: 0.2751, acc: 0.8960
loss: 0.2731, acc: 0.8969
loss: 0.2716, acc: 0.8975
loss: 0.2706, acc: 0.8977
loss: 0.2701, acc: 0.8980
loss: 0.2679, acc: 0.8988
loss: 0.2672, acc: 0.8992
loss: 0.2666, acc: 0.8988
loss: 0.2680, acc: 0.8980
loss: 0.2685, acc: 0.8981
loss: 0.2701, acc: 0.8981
loss: 0.2708, acc: 0.8982
loss: 0.2704, acc: 0.8985
loss: 0.2708, acc: 0.8982
loss: 0.2714, acc: 0.8980
loss: 0.2721, acc: 0.8975
loss: 0.2723, acc: 0.8976
loss: 0.2725, acc: 0.8969
loss: 0.2744, acc: 0.8963
loss: 0.2745, acc: 0.8962
loss: 0.2773, acc: 0.8950
loss: 0.2768, acc: 0.8950
loss: 0.2779, acc: 0.8947
loss: 0.2788, acc: 0.8943
loss: 0.2792, acc: 0.8942
loss: 0.2799, acc: 0.8943
loss: 0.2800, acc: 0.8944
loss: 0.2798, acc: 0.8949
loss: 0.2811, acc: 0.8948
loss: 0.2811, acc: 0.8947
loss: 0.2810, acc: 0.8948
loss: 0.2805, acc: 0.8952
loss: 0.2811, acc: 0.8950
loss: 0.2808, acc: 0.8951
loss: 0.2809, acc: 0.8951
loss: 0.2815, acc: 0.8948
loss: 0.2809, acc: 0.8950
loss: 0.2805, acc: 0.8951
> val_acc: 0.8153, val_f1: 0.8093
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3031, acc: 0.8625
loss: 0.2651, acc: 0.8844
loss: 0.2469, acc: 0.8979
loss: 0.2344, acc: 0.9078
loss: 0.2322, acc: 0.9137
loss: 0.2331, acc: 0.9135
loss: 0.2223, acc: 0.9187
loss: 0.2120, acc: 0.9234
loss: 0.2024, acc: 0.9278
loss: 0.2109, acc: 0.9231
loss: 0.2135, acc: 0.9210
loss: 0.2111, acc: 0.9208
loss: 0.2221, acc: 0.9168
loss: 0.2267, acc: 0.9134
loss: 0.2283, acc: 0.9121
loss: 0.2356, acc: 0.9094
loss: 0.2382, acc: 0.9074
loss: 0.2385, acc: 0.9066
loss: 0.2383, acc: 0.9076
loss: 0.2415, acc: 0.9059
loss: 0.2435, acc: 0.9057
loss: 0.2456, acc: 0.9043
loss: 0.2486, acc: 0.9041
loss: 0.2503, acc: 0.9029
loss: 0.2507, acc: 0.9022
loss: 0.2518, acc: 0.9012
loss: 0.2529, acc: 0.9014
loss: 0.2532, acc: 0.9007
loss: 0.2568, acc: 0.9000
loss: 0.2562, acc: 0.9000
loss: 0.2569, acc: 0.8992
loss: 0.2617, acc: 0.8971
loss: 0.2610, acc: 0.8972
loss: 0.2607, acc: 0.8980
loss: 0.2632, acc: 0.8973
loss: 0.2653, acc: 0.8958
loss: 0.2642, acc: 0.8968
loss: 0.2654, acc: 0.8964
loss: 0.2650, acc: 0.8968
loss: 0.2660, acc: 0.8961
loss: 0.2654, acc: 0.8962
loss: 0.2647, acc: 0.8963
loss: 0.2674, acc: 0.8955
loss: 0.2690, acc: 0.8956
loss: 0.2688, acc: 0.8958
loss: 0.2706, acc: 0.8952
loss: 0.2696, acc: 0.8957
loss: 0.2688, acc: 0.8962
loss: 0.2673, acc: 0.8969
loss: 0.2700, acc: 0.8965
loss: 0.2717, acc: 0.8963
loss: 0.2731, acc: 0.8960
loss: 0.2719, acc: 0.8961
loss: 0.2704, acc: 0.8964
loss: 0.2721, acc: 0.8959
loss: 0.2721, acc: 0.8964
loss: 0.2718, acc: 0.8966
loss: 0.2724, acc: 0.8960
loss: 0.2721, acc: 0.8962
loss: 0.2723, acc: 0.8965
loss: 0.2729, acc: 0.8961
loss: 0.2736, acc: 0.8956
loss: 0.2737, acc: 0.8955
loss: 0.2753, acc: 0.8951
loss: 0.2758, acc: 0.8953
loss: 0.2755, acc: 0.8953
loss: 0.2751, acc: 0.8953
loss: 0.2761, acc: 0.8952
loss: 0.2767, acc: 0.8954
loss: 0.2774, acc: 0.8950
> val_acc: 0.8198, val_f1: 0.8123
>> saved: peft/bert_lora/mams//acc_0.8198_f1_0.8123_230828-0827
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1777, acc: 0.9250
loss: 0.2017, acc: 0.9187
loss: 0.2072, acc: 0.9146
loss: 0.2014, acc: 0.9203
loss: 0.2133, acc: 0.9175
loss: 0.2124, acc: 0.9156
loss: 0.2186, acc: 0.9143
loss: 0.2170, acc: 0.9156
loss: 0.2116, acc: 0.9208
loss: 0.2171, acc: 0.9187
loss: 0.2203, acc: 0.9182
loss: 0.2220, acc: 0.9172
loss: 0.2236, acc: 0.9168
loss: 0.2317, acc: 0.9134
loss: 0.2330, acc: 0.9133
loss: 0.2372, acc: 0.9117
loss: 0.2340, acc: 0.9132
loss: 0.2337, acc: 0.9125
loss: 0.2372, acc: 0.9109
loss: 0.2357, acc: 0.9109
loss: 0.2336, acc: 0.9122
loss: 0.2356, acc: 0.9122
loss: 0.2343, acc: 0.9130
loss: 0.2305, acc: 0.9135
loss: 0.2310, acc: 0.9135
loss: 0.2301, acc: 0.9137
loss: 0.2279, acc: 0.9146
loss: 0.2285, acc: 0.9143
loss: 0.2288, acc: 0.9136
loss: 0.2292, acc: 0.9133
loss: 0.2293, acc: 0.9129
loss: 0.2306, acc: 0.9135
loss: 0.2339, acc: 0.9119
loss: 0.2369, acc: 0.9096
loss: 0.2366, acc: 0.9096
loss: 0.2360, acc: 0.9099
loss: 0.2369, acc: 0.9096
loss: 0.2354, acc: 0.9105
loss: 0.2366, acc: 0.9098
loss: 0.2378, acc: 0.9089
loss: 0.2382, acc: 0.9090
loss: 0.2379, acc: 0.9095
loss: 0.2386, acc: 0.9093
loss: 0.2382, acc: 0.9092
loss: 0.2379, acc: 0.9092
loss: 0.2375, acc: 0.9090
loss: 0.2386, acc: 0.9086
loss: 0.2390, acc: 0.9089
loss: 0.2402, acc: 0.9087
loss: 0.2413, acc: 0.9087
loss: 0.2421, acc: 0.9087
loss: 0.2419, acc: 0.9089
loss: 0.2440, acc: 0.9078
loss: 0.2448, acc: 0.9079
loss: 0.2456, acc: 0.9075
loss: 0.2467, acc: 0.9071
loss: 0.2466, acc: 0.9069
loss: 0.2476, acc: 0.9065
loss: 0.2478, acc: 0.9065
loss: 0.2483, acc: 0.9060
loss: 0.2506, acc: 0.9051
loss: 0.2507, acc: 0.9048
loss: 0.2540, acc: 0.9037
loss: 0.2548, acc: 0.9033
loss: 0.2562, acc: 0.9030
loss: 0.2557, acc: 0.9036
loss: 0.2561, acc: 0.9034
loss: 0.2566, acc: 0.9033
loss: 0.2568, acc: 0.9034
loss: 0.2575, acc: 0.9030
> val_acc: 0.8101, val_f1: 0.8023
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.1676, acc: 0.9250
loss: 0.1890, acc: 0.9125
loss: 0.2116, acc: 0.9167
loss: 0.2123, acc: 0.9172
loss: 0.2053, acc: 0.9200
loss: 0.2055, acc: 0.9229
loss: 0.2063, acc: 0.9241
loss: 0.1999, acc: 0.9281
loss: 0.2100, acc: 0.9229
loss: 0.2131, acc: 0.9213
loss: 0.2097, acc: 0.9222
loss: 0.2078, acc: 0.9219
loss: 0.2083, acc: 0.9216
loss: 0.2073, acc: 0.9232
loss: 0.2066, acc: 0.9242
loss: 0.2071, acc: 0.9250
loss: 0.2055, acc: 0.9257
loss: 0.2027, acc: 0.9264
loss: 0.2062, acc: 0.9240
loss: 0.2070, acc: 0.9241
loss: 0.2109, acc: 0.9214
loss: 0.2115, acc: 0.9210
loss: 0.2141, acc: 0.9212
loss: 0.2174, acc: 0.9201
loss: 0.2165, acc: 0.9203
loss: 0.2151, acc: 0.9197
loss: 0.2129, acc: 0.9204
loss: 0.2132, acc: 0.9208
loss: 0.2193, acc: 0.9194
loss: 0.2199, acc: 0.9187
loss: 0.2239, acc: 0.9167
loss: 0.2226, acc: 0.9176
loss: 0.2223, acc: 0.9176
loss: 0.2204, acc: 0.9186
loss: 0.2211, acc: 0.9184
loss: 0.2213, acc: 0.9181
loss: 0.2218, acc: 0.9186
loss: 0.2229, acc: 0.9183
loss: 0.2235, acc: 0.9179
loss: 0.2248, acc: 0.9177
loss: 0.2250, acc: 0.9172
loss: 0.2285, acc: 0.9164
loss: 0.2298, acc: 0.9161
loss: 0.2293, acc: 0.9162
loss: 0.2307, acc: 0.9150
loss: 0.2300, acc: 0.9148
loss: 0.2305, acc: 0.9148
loss: 0.2297, acc: 0.9152
loss: 0.2283, acc: 0.9157
loss: 0.2287, acc: 0.9154
loss: 0.2309, acc: 0.9140
loss: 0.2313, acc: 0.9135
loss: 0.2337, acc: 0.9129
loss: 0.2348, acc: 0.9126
loss: 0.2348, acc: 0.9124
loss: 0.2346, acc: 0.9127
loss: 0.2351, acc: 0.9126
loss: 0.2363, acc: 0.9125
loss: 0.2377, acc: 0.9120
loss: 0.2377, acc: 0.9120
loss: 0.2386, acc: 0.9120
loss: 0.2389, acc: 0.9117
loss: 0.2397, acc: 0.9112
loss: 0.2415, acc: 0.9104
loss: 0.2412, acc: 0.9108
loss: 0.2419, acc: 0.9101
loss: 0.2421, acc: 0.9097
loss: 0.2417, acc: 0.9101
loss: 0.2419, acc: 0.9104
loss: 0.2428, acc: 0.9099
> val_acc: 0.8221, val_f1: 0.8174
>> saved: peft/bert_lora/mams//acc_0.8221_f1_0.8174_230828-0830
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1834, acc: 0.9437
loss: 0.2440, acc: 0.9094
loss: 0.2251, acc: 0.9146
loss: 0.2394, acc: 0.9141
loss: 0.2317, acc: 0.9125
loss: 0.2341, acc: 0.9146
loss: 0.2374, acc: 0.9134
loss: 0.2343, acc: 0.9156
loss: 0.2335, acc: 0.9153
loss: 0.2334, acc: 0.9144
loss: 0.2317, acc: 0.9159
loss: 0.2315, acc: 0.9161
loss: 0.2285, acc: 0.9163
loss: 0.2258, acc: 0.9183
loss: 0.2282, acc: 0.9183
loss: 0.2301, acc: 0.9180
loss: 0.2268, acc: 0.9180
loss: 0.2259, acc: 0.9181
loss: 0.2274, acc: 0.9171
loss: 0.2289, acc: 0.9163
loss: 0.2259, acc: 0.9167
loss: 0.2267, acc: 0.9162
loss: 0.2241, acc: 0.9177
loss: 0.2263, acc: 0.9180
loss: 0.2273, acc: 0.9180
loss: 0.2281, acc: 0.9180
loss: 0.2261, acc: 0.9190
loss: 0.2260, acc: 0.9190
loss: 0.2250, acc: 0.9187
loss: 0.2252, acc: 0.9185
loss: 0.2258, acc: 0.9187
loss: 0.2269, acc: 0.9178
loss: 0.2272, acc: 0.9178
loss: 0.2265, acc: 0.9180
loss: 0.2276, acc: 0.9180
loss: 0.2279, acc: 0.9179
loss: 0.2262, acc: 0.9182
loss: 0.2255, acc: 0.9178
loss: 0.2243, acc: 0.9184
loss: 0.2241, acc: 0.9184
loss: 0.2232, acc: 0.9184
loss: 0.2251, acc: 0.9174
loss: 0.2243, acc: 0.9176
loss: 0.2243, acc: 0.9176
loss: 0.2248, acc: 0.9175
loss: 0.2255, acc: 0.9168
loss: 0.2251, acc: 0.9166
loss: 0.2250, acc: 0.9169
loss: 0.2246, acc: 0.9170
loss: 0.2246, acc: 0.9169
loss: 0.2257, acc: 0.9164
loss: 0.2253, acc: 0.9165
loss: 0.2267, acc: 0.9162
loss: 0.2258, acc: 0.9163
loss: 0.2280, acc: 0.9159
loss: 0.2275, acc: 0.9160
loss: 0.2261, acc: 0.9161
loss: 0.2266, acc: 0.9158
loss: 0.2256, acc: 0.9161
loss: 0.2267, acc: 0.9159
loss: 0.2278, acc: 0.9154
loss: 0.2285, acc: 0.9151
loss: 0.2282, acc: 0.9153
loss: 0.2285, acc: 0.9152
loss: 0.2296, acc: 0.9149
loss: 0.2293, acc: 0.9148
loss: 0.2313, acc: 0.9143
loss: 0.2325, acc: 0.9141
loss: 0.2334, acc: 0.9136
loss: 0.2345, acc: 0.9134
> val_acc: 0.8206, val_f1: 0.8152
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1604, acc: 0.9563
loss: 0.1506, acc: 0.9531
loss: 0.1808, acc: 0.9375
loss: 0.1701, acc: 0.9422
loss: 0.1765, acc: 0.9413
loss: 0.1873, acc: 0.9375
loss: 0.1911, acc: 0.9339
loss: 0.1912, acc: 0.9336
loss: 0.1948, acc: 0.9306
loss: 0.2014, acc: 0.9250
loss: 0.2071, acc: 0.9244
loss: 0.2055, acc: 0.9240
loss: 0.2030, acc: 0.9250
loss: 0.2019, acc: 0.9263
loss: 0.1968, acc: 0.9292
loss: 0.1980, acc: 0.9273
loss: 0.1997, acc: 0.9276
loss: 0.2022, acc: 0.9264
loss: 0.2002, acc: 0.9270
loss: 0.2079, acc: 0.9244
loss: 0.2161, acc: 0.9223
loss: 0.2174, acc: 0.9216
loss: 0.2190, acc: 0.9209
loss: 0.2210, acc: 0.9201
loss: 0.2228, acc: 0.9190
loss: 0.2244, acc: 0.9178
loss: 0.2297, acc: 0.9148
loss: 0.2271, acc: 0.9156
loss: 0.2275, acc: 0.9149
loss: 0.2260, acc: 0.9156
loss: 0.2262, acc: 0.9157
loss: 0.2288, acc: 0.9146
loss: 0.2294, acc: 0.9144
loss: 0.2274, acc: 0.9153
loss: 0.2264, acc: 0.9155
loss: 0.2282, acc: 0.9148
loss: 0.2267, acc: 0.9157
loss: 0.2270, acc: 0.9155
loss: 0.2260, acc: 0.9163
loss: 0.2244, acc: 0.9167
loss: 0.2232, acc: 0.9175
loss: 0.2212, acc: 0.9183
loss: 0.2231, acc: 0.9182
loss: 0.2227, acc: 0.9179
loss: 0.2261, acc: 0.9161
loss: 0.2272, acc: 0.9155
loss: 0.2278, acc: 0.9152
loss: 0.2281, acc: 0.9148
loss: 0.2264, acc: 0.9154
loss: 0.2253, acc: 0.9157
loss: 0.2256, acc: 0.9153
loss: 0.2244, acc: 0.9159
loss: 0.2237, acc: 0.9160
loss: 0.2231, acc: 0.9157
loss: 0.2255, acc: 0.9153
loss: 0.2252, acc: 0.9154
loss: 0.2262, acc: 0.9154
loss: 0.2268, acc: 0.9153
loss: 0.2269, acc: 0.9154
loss: 0.2271, acc: 0.9150
loss: 0.2267, acc: 0.9152
loss: 0.2272, acc: 0.9149
loss: 0.2274, acc: 0.9150
loss: 0.2269, acc: 0.9152
loss: 0.2288, acc: 0.9146
loss: 0.2291, acc: 0.9146
loss: 0.2306, acc: 0.9142
loss: 0.2311, acc: 0.9139
loss: 0.2309, acc: 0.9141
loss: 0.2315, acc: 0.9136
> val_acc: 0.8183, val_f1: 0.8130
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.1989, acc: 0.9313
loss: 0.2358, acc: 0.9156
loss: 0.2212, acc: 0.9187
loss: 0.2012, acc: 0.9250
loss: 0.1966, acc: 0.9300
loss: 0.1915, acc: 0.9323
loss: 0.1966, acc: 0.9304
loss: 0.1994, acc: 0.9289
loss: 0.1919, acc: 0.9319
loss: 0.1928, acc: 0.9325
loss: 0.1928, acc: 0.9324
loss: 0.2015, acc: 0.9297
loss: 0.2043, acc: 0.9293
loss: 0.2032, acc: 0.9295
loss: 0.2036, acc: 0.9287
loss: 0.1985, acc: 0.9313
loss: 0.1965, acc: 0.9305
loss: 0.1955, acc: 0.9306
loss: 0.1967, acc: 0.9306
loss: 0.1956, acc: 0.9313
loss: 0.1943, acc: 0.9315
loss: 0.1957, acc: 0.9315
loss: 0.1959, acc: 0.9315
loss: 0.1956, acc: 0.9318
loss: 0.1957, acc: 0.9313
loss: 0.2004, acc: 0.9293
loss: 0.2012, acc: 0.9287
loss: 0.2015, acc: 0.9283
loss: 0.2054, acc: 0.9274
loss: 0.2072, acc: 0.9269
loss: 0.2090, acc: 0.9258
loss: 0.2088, acc: 0.9262
loss: 0.2067, acc: 0.9269
loss: 0.2065, acc: 0.9274
loss: 0.2080, acc: 0.9264
loss: 0.2107, acc: 0.9255
loss: 0.2127, acc: 0.9247
loss: 0.2128, acc: 0.9245
loss: 0.2102, acc: 0.9256
loss: 0.2103, acc: 0.9253
loss: 0.2102, acc: 0.9256
loss: 0.2101, acc: 0.9254
loss: 0.2104, acc: 0.9251
loss: 0.2112, acc: 0.9251
loss: 0.2113, acc: 0.9247
loss: 0.2109, acc: 0.9251
loss: 0.2131, acc: 0.9245
loss: 0.2127, acc: 0.9251
loss: 0.2129, acc: 0.9246
loss: 0.2129, acc: 0.9245
loss: 0.2116, acc: 0.9248
loss: 0.2137, acc: 0.9239
loss: 0.2126, acc: 0.9243
loss: 0.2118, acc: 0.9247
loss: 0.2138, acc: 0.9241
loss: 0.2140, acc: 0.9239
loss: 0.2163, acc: 0.9229
loss: 0.2173, acc: 0.9223
loss: 0.2173, acc: 0.9224
loss: 0.2173, acc: 0.9223
loss: 0.2178, acc: 0.9219
loss: 0.2208, acc: 0.9210
loss: 0.2217, acc: 0.9206
loss: 0.2227, acc: 0.9202
loss: 0.2240, acc: 0.9196
loss: 0.2238, acc: 0.9198
loss: 0.2234, acc: 0.9198
loss: 0.2232, acc: 0.9197
loss: 0.2259, acc: 0.9189
loss: 0.2255, acc: 0.9191
> val_acc: 0.8228, val_f1: 0.8150
>> saved: peft/bert_lora/mams//acc_0.8228_f1_0.815_230828-0834
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 0.1931, acc: 0.9313
loss: 0.1521, acc: 0.9469
loss: 0.1500, acc: 0.9458
loss: 0.1535, acc: 0.9453
loss: 0.1637, acc: 0.9363
loss: 0.1697, acc: 0.9323
loss: 0.1836, acc: 0.9277
loss: 0.1836, acc: 0.9289
loss: 0.1805, acc: 0.9319
loss: 0.1860, acc: 0.9313
loss: 0.1900, acc: 0.9307
loss: 0.1893, acc: 0.9307
loss: 0.1911, acc: 0.9293
loss: 0.1896, acc: 0.9304
loss: 0.1922, acc: 0.9292
loss: 0.1899, acc: 0.9297
loss: 0.1898, acc: 0.9294
loss: 0.1890, acc: 0.9299
loss: 0.1851, acc: 0.9313
loss: 0.1833, acc: 0.9319
loss: 0.1871, acc: 0.9295
loss: 0.1895, acc: 0.9287
loss: 0.1922, acc: 0.9283
loss: 0.1918, acc: 0.9286
loss: 0.1935, acc: 0.9273
loss: 0.1912, acc: 0.9284
loss: 0.1886, acc: 0.9299
loss: 0.1869, acc: 0.9304
loss: 0.1851, acc: 0.9315
loss: 0.1878, acc: 0.9306
loss: 0.1902, acc: 0.9300
loss: 0.1889, acc: 0.9307
loss: 0.1886, acc: 0.9313
loss: 0.1886, acc: 0.9313
loss: 0.1877, acc: 0.9311
loss: 0.1869, acc: 0.9316
loss: 0.1880, acc: 0.9314
loss: 0.1892, acc: 0.9314
loss: 0.1896, acc: 0.9311
loss: 0.1895, acc: 0.9311
loss: 0.1915, acc: 0.9302
loss: 0.1925, acc: 0.9296
loss: 0.1922, acc: 0.9295
loss: 0.1939, acc: 0.9284
loss: 0.1947, acc: 0.9279
loss: 0.1945, acc: 0.9280
loss: 0.1968, acc: 0.9274
loss: 0.1975, acc: 0.9271
loss: 0.1973, acc: 0.9273
loss: 0.1971, acc: 0.9273
loss: 0.1993, acc: 0.9266
loss: 0.2003, acc: 0.9262
loss: 0.2010, acc: 0.9261
loss: 0.2016, acc: 0.9259
loss: 0.2018, acc: 0.9257
loss: 0.2026, acc: 0.9253
loss: 0.2033, acc: 0.9251
loss: 0.2036, acc: 0.9250
loss: 0.2051, acc: 0.9246
loss: 0.2063, acc: 0.9241
loss: 0.2066, acc: 0.9241
loss: 0.2068, acc: 0.9238
loss: 0.2089, acc: 0.9231
loss: 0.2117, acc: 0.9224
loss: 0.2123, acc: 0.9218
loss: 0.2122, acc: 0.9219
loss: 0.2125, acc: 0.9220
loss: 0.2125, acc: 0.9219
loss: 0.2125, acc: 0.9220
loss: 0.2133, acc: 0.9217
> val_acc: 0.8101, val_f1: 0.8059
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 15
loss: 0.2534, acc: 0.9125
loss: 0.2664, acc: 0.9031
loss: 0.2454, acc: 0.9104
loss: 0.2330, acc: 0.9094
loss: 0.2185, acc: 0.9125
loss: 0.2221, acc: 0.9104
loss: 0.2314, acc: 0.9071
loss: 0.2367, acc: 0.9039
loss: 0.2374, acc: 0.9042
loss: 0.2384, acc: 0.9056
loss: 0.2379, acc: 0.9062
loss: 0.2338, acc: 0.9089
loss: 0.2298, acc: 0.9096
loss: 0.2292, acc: 0.9098
loss: 0.2255, acc: 0.9108
loss: 0.2244, acc: 0.9125
loss: 0.2220, acc: 0.9140
loss: 0.2248, acc: 0.9125
loss: 0.2267, acc: 0.9122
loss: 0.2272, acc: 0.9125
loss: 0.2265, acc: 0.9137
loss: 0.2270, acc: 0.9136
loss: 0.2277, acc: 0.9128
loss: 0.2295, acc: 0.9128
loss: 0.2290, acc: 0.9127
loss: 0.2320, acc: 0.9127
loss: 0.2351, acc: 0.9113
loss: 0.2355, acc: 0.9112
loss: 0.2366, acc: 0.9095
loss: 0.2368, acc: 0.9090
loss: 0.2378, acc: 0.9091
loss: 0.2377, acc: 0.9090
loss: 0.2374, acc: 0.9098
loss: 0.2355, acc: 0.9103
loss: 0.2364, acc: 0.9100
loss: 0.2342, acc: 0.9109
loss: 0.2358, acc: 0.9101
loss: 0.2350, acc: 0.9110
loss: 0.2367, acc: 0.9101
loss: 0.2367, acc: 0.9098
loss: 0.2362, acc: 0.9102
loss: 0.2354, acc: 0.9107
loss: 0.2363, acc: 0.9103
loss: 0.2361, acc: 0.9102
loss: 0.2341, acc: 0.9110
loss: 0.2334, acc: 0.9114
loss: 0.2332, acc: 0.9118
loss: 0.2329, acc: 0.9118
loss: 0.2312, acc: 0.9121
loss: 0.2309, acc: 0.9124
loss: 0.2303, acc: 0.9131
loss: 0.2299, acc: 0.9135
loss: 0.2300, acc: 0.9136
loss: 0.2292, acc: 0.9135
loss: 0.2288, acc: 0.9136
loss: 0.2285, acc: 0.9136
loss: 0.2287, acc: 0.9139
loss: 0.2278, acc: 0.9140
loss: 0.2287, acc: 0.9137
loss: 0.2288, acc: 0.9137
loss: 0.2292, acc: 0.9137
loss: 0.2290, acc: 0.9137
loss: 0.2289, acc: 0.9138
loss: 0.2286, acc: 0.9141
loss: 0.2290, acc: 0.9142
loss: 0.2283, acc: 0.9146
loss: 0.2276, acc: 0.9150
loss: 0.2265, acc: 0.9155
loss: 0.2255, acc: 0.9159
loss: 0.2251, acc: 0.9160
> val_acc: 0.8183, val_f1: 0.8108
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 16
loss: 0.1572, acc: 0.9437
loss: 0.1414, acc: 0.9437
loss: 0.2070, acc: 0.9271
loss: 0.2105, acc: 0.9250
loss: 0.2121, acc: 0.9237
loss: 0.2277, acc: 0.9250
loss: 0.2294, acc: 0.9241
loss: 0.2259, acc: 0.9242
loss: 0.2238, acc: 0.9236
loss: 0.2271, acc: 0.9219
loss: 0.2229, acc: 0.9233
loss: 0.2244, acc: 0.9219
loss: 0.2200, acc: 0.9231
loss: 0.2242, acc: 0.9205
loss: 0.2255, acc: 0.9208
loss: 0.2229, acc: 0.9223
loss: 0.2234, acc: 0.9221
loss: 0.2220, acc: 0.9233
loss: 0.2261, acc: 0.9217
loss: 0.2253, acc: 0.9213
loss: 0.2284, acc: 0.9205
loss: 0.2316, acc: 0.9190
loss: 0.2312, acc: 0.9198
loss: 0.2279, acc: 0.9203
loss: 0.2239, acc: 0.9215
loss: 0.2260, acc: 0.9216
loss: 0.2240, acc: 0.9220
loss: 0.2217, acc: 0.9225
loss: 0.2186, acc: 0.9237
loss: 0.2201, acc: 0.9229
loss: 0.2176, acc: 0.9236
loss: 0.2161, acc: 0.9244
loss: 0.2161, acc: 0.9244
loss: 0.2169, acc: 0.9241
loss: 0.2156, acc: 0.9250
loss: 0.2165, acc: 0.9247
loss: 0.2161, acc: 0.9245
loss: 0.2168, acc: 0.9240
loss: 0.2177, acc: 0.9239
loss: 0.2170, acc: 0.9239
loss: 0.2167, acc: 0.9241
loss: 0.2169, acc: 0.9244
loss: 0.2167, acc: 0.9240
loss: 0.2167, acc: 0.9239
loss: 0.2173, acc: 0.9232
loss: 0.2161, acc: 0.9232
loss: 0.2154, acc: 0.9230
loss: 0.2163, acc: 0.9224
loss: 0.2164, acc: 0.9223
loss: 0.2163, acc: 0.9224
loss: 0.2165, acc: 0.9224
loss: 0.2175, acc: 0.9222
loss: 0.2166, acc: 0.9228
loss: 0.2164, acc: 0.9227
loss: 0.2160, acc: 0.9227
loss: 0.2187, acc: 0.9220
loss: 0.2175, acc: 0.9224
loss: 0.2173, acc: 0.9223
loss: 0.2176, acc: 0.9225
loss: 0.2166, acc: 0.9228
loss: 0.2156, acc: 0.9230
loss: 0.2170, acc: 0.9223
loss: 0.2160, acc: 0.9223
loss: 0.2154, acc: 0.9223
loss: 0.2153, acc: 0.9222
loss: 0.2149, acc: 0.9223
loss: 0.2144, acc: 0.9225
loss: 0.2139, acc: 0.9224
loss: 0.2133, acc: 0.9224
loss: 0.2126, acc: 0.9226
> val_acc: 0.8281, val_f1: 0.8208
>> saved: peft/bert_lora/mams//acc_0.8281_f1_0.8208_230828-0839
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 17
loss: 0.1859, acc: 0.9375
loss: 0.1375, acc: 0.9437
loss: 0.1516, acc: 0.9354
loss: 0.1500, acc: 0.9422
loss: 0.1441, acc: 0.9450
loss: 0.1542, acc: 0.9417
loss: 0.1498, acc: 0.9429
loss: 0.1491, acc: 0.9430
loss: 0.1518, acc: 0.9410
loss: 0.1515, acc: 0.9406
loss: 0.1534, acc: 0.9409
loss: 0.1611, acc: 0.9391
loss: 0.1625, acc: 0.9389
loss: 0.1670, acc: 0.9371
loss: 0.1672, acc: 0.9363
loss: 0.1665, acc: 0.9355
loss: 0.1685, acc: 0.9342
loss: 0.1736, acc: 0.9319
loss: 0.1727, acc: 0.9329
loss: 0.1759, acc: 0.9319
loss: 0.1803, acc: 0.9301
loss: 0.1817, acc: 0.9301
loss: 0.1798, acc: 0.9310
loss: 0.1799, acc: 0.9305
loss: 0.1786, acc: 0.9310
loss: 0.1774, acc: 0.9313
loss: 0.1774, acc: 0.9313
loss: 0.1810, acc: 0.9306
loss: 0.1804, acc: 0.9306
loss: 0.1850, acc: 0.9283
loss: 0.1874, acc: 0.9266
loss: 0.1884, acc: 0.9260
loss: 0.1867, acc: 0.9267
loss: 0.1847, acc: 0.9276
loss: 0.1831, acc: 0.9280
loss: 0.1849, acc: 0.9271
loss: 0.1824, acc: 0.9280
loss: 0.1829, acc: 0.9276
loss: 0.1846, acc: 0.9271
loss: 0.1841, acc: 0.9277
loss: 0.1845, acc: 0.9277
loss: 0.1870, acc: 0.9274
loss: 0.1868, acc: 0.9278
loss: 0.1866, acc: 0.9283
loss: 0.1873, acc: 0.9285
loss: 0.1864, acc: 0.9284
loss: 0.1858, acc: 0.9287
loss: 0.1867, acc: 0.9286
loss: 0.1871, acc: 0.9288
loss: 0.1877, acc: 0.9287
loss: 0.1886, acc: 0.9284
loss: 0.1902, acc: 0.9275
loss: 0.1907, acc: 0.9270
loss: 0.1903, acc: 0.9273
loss: 0.1913, acc: 0.9268
loss: 0.1939, acc: 0.9262
loss: 0.1939, acc: 0.9259
loss: 0.1960, acc: 0.9251
loss: 0.1977, acc: 0.9245
loss: 0.1977, acc: 0.9246
loss: 0.1974, acc: 0.9248
loss: 0.1975, acc: 0.9247
loss: 0.1989, acc: 0.9241
loss: 0.1994, acc: 0.9241
loss: 0.2014, acc: 0.9235
loss: 0.2014, acc: 0.9235
loss: 0.2016, acc: 0.9234
loss: 0.2022, acc: 0.9231
loss: 0.2036, acc: 0.9222
loss: 0.2039, acc: 0.9221
> val_acc: 0.8176, val_f1: 0.8137
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 18
loss: 0.1557, acc: 0.9437
loss: 0.1605, acc: 0.9375
loss: 0.1945, acc: 0.9313
loss: 0.1858, acc: 0.9328
loss: 0.1755, acc: 0.9375
loss: 0.1661, acc: 0.9396
loss: 0.1721, acc: 0.9402
loss: 0.1724, acc: 0.9375
loss: 0.1659, acc: 0.9410
loss: 0.1622, acc: 0.9406
loss: 0.1644, acc: 0.9403
loss: 0.1631, acc: 0.9401
loss: 0.1626, acc: 0.9394
loss: 0.1562, acc: 0.9415
loss: 0.1631, acc: 0.9396
loss: 0.1663, acc: 0.9379
loss: 0.1649, acc: 0.9382
loss: 0.1726, acc: 0.9358
loss: 0.1731, acc: 0.9362
loss: 0.1724, acc: 0.9363
loss: 0.1697, acc: 0.9375
loss: 0.1684, acc: 0.9384
loss: 0.1754, acc: 0.9364
loss: 0.1786, acc: 0.9354
loss: 0.1810, acc: 0.9335
loss: 0.1850, acc: 0.9325
loss: 0.1842, acc: 0.9329
loss: 0.1874, acc: 0.9310
loss: 0.1873, acc: 0.9308
loss: 0.1904, acc: 0.9302
loss: 0.1928, acc: 0.9300
loss: 0.1933, acc: 0.9297
loss: 0.1935, acc: 0.9297
loss: 0.1933, acc: 0.9289
loss: 0.1956, acc: 0.9282
loss: 0.1963, acc: 0.9280
loss: 0.1957, acc: 0.9284
loss: 0.1992, acc: 0.9273
loss: 0.1998, acc: 0.9268
loss: 0.2002, acc: 0.9261
loss: 0.2022, acc: 0.9255
loss: 0.2008, acc: 0.9259
loss: 0.1997, acc: 0.9259
loss: 0.2006, acc: 0.9253
loss: 0.2001, acc: 0.9250
loss: 0.2001, acc: 0.9250
loss: 0.2002, acc: 0.9249
loss: 0.1994, acc: 0.9254
loss: 0.1979, acc: 0.9259
loss: 0.1989, acc: 0.9253
loss: 0.1992, acc: 0.9248
loss: 0.1985, acc: 0.9252
loss: 0.1984, acc: 0.9248
loss: 0.1989, acc: 0.9245
loss: 0.1989, acc: 0.9242
loss: 0.1984, acc: 0.9243
loss: 0.1992, acc: 0.9241
loss: 0.1985, acc: 0.9244
loss: 0.1983, acc: 0.9242
loss: 0.1975, acc: 0.9242
loss: 0.1973, acc: 0.9243
loss: 0.1975, acc: 0.9241
loss: 0.1986, acc: 0.9241
loss: 0.1975, acc: 0.9245
loss: 0.1980, acc: 0.9245
loss: 0.1986, acc: 0.9247
loss: 0.2003, acc: 0.9243
loss: 0.1993, acc: 0.9245
loss: 0.1986, acc: 0.9250
loss: 0.1983, acc: 0.9248
> val_acc: 0.8176, val_f1: 0.8149
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 19
loss: 0.1915, acc: 0.9375
loss: 0.1820, acc: 0.9406
loss: 0.1963, acc: 0.9313
loss: 0.1984, acc: 0.9313
loss: 0.1926, acc: 0.9337
loss: 0.1882, acc: 0.9323
loss: 0.1782, acc: 0.9348
loss: 0.1816, acc: 0.9305
loss: 0.1827, acc: 0.9299
loss: 0.1912, acc: 0.9281
loss: 0.1940, acc: 0.9284
loss: 0.1899, acc: 0.9313
loss: 0.1908, acc: 0.9308
loss: 0.1960, acc: 0.9295
loss: 0.1972, acc: 0.9296
loss: 0.1939, acc: 0.9313
loss: 0.1909, acc: 0.9324
loss: 0.1886, acc: 0.9337
loss: 0.1893, acc: 0.9322
loss: 0.1906, acc: 0.9306
loss: 0.1925, acc: 0.9295
loss: 0.1922, acc: 0.9304
loss: 0.1921, acc: 0.9304
loss: 0.1911, acc: 0.9299
loss: 0.1913, acc: 0.9293
loss: 0.1911, acc: 0.9286
loss: 0.1907, acc: 0.9285
loss: 0.1917, acc: 0.9275
loss: 0.1922, acc: 0.9280
loss: 0.1938, acc: 0.9275
loss: 0.1942, acc: 0.9280
loss: 0.1947, acc: 0.9275
loss: 0.1940, acc: 0.9275
loss: 0.1934, acc: 0.9281
loss: 0.1946, acc: 0.9277
loss: 0.1939, acc: 0.9276
loss: 0.1933, acc: 0.9277
loss: 0.1941, acc: 0.9271
loss: 0.1925, acc: 0.9276
loss: 0.1928, acc: 0.9277
loss: 0.1931, acc: 0.9284
loss: 0.1923, acc: 0.9287
loss: 0.1947, acc: 0.9281
loss: 0.1960, acc: 0.9276
loss: 0.1980, acc: 0.9271
loss: 0.1987, acc: 0.9266
loss: 0.1987, acc: 0.9267
loss: 0.1992, acc: 0.9268
loss: 0.2004, acc: 0.9264
loss: 0.2004, acc: 0.9264
loss: 0.1996, acc: 0.9268
loss: 0.2002, acc: 0.9266
loss: 0.1993, acc: 0.9268
loss: 0.1988, acc: 0.9271
loss: 0.1976, acc: 0.9277
loss: 0.1971, acc: 0.9276
loss: 0.1969, acc: 0.9277
loss: 0.1967, acc: 0.9280
loss: 0.1974, acc: 0.9278
loss: 0.1975, acc: 0.9275
loss: 0.1981, acc: 0.9275
loss: 0.1987, acc: 0.9271
loss: 0.1984, acc: 0.9274
loss: 0.1974, acc: 0.9280
loss: 0.1970, acc: 0.9281
loss: 0.1970, acc: 0.9281
loss: 0.1972, acc: 0.9278
loss: 0.1969, acc: 0.9277
loss: 0.1959, acc: 0.9278
loss: 0.1953, acc: 0.9279
> val_acc: 0.8041, val_f1: 0.7989
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 20
loss: 0.1465, acc: 0.9313
loss: 0.1427, acc: 0.9344
loss: 0.1390, acc: 0.9396
loss: 0.1516, acc: 0.9406
loss: 0.1423, acc: 0.9437
loss: 0.1437, acc: 0.9448
loss: 0.1465, acc: 0.9429
loss: 0.1472, acc: 0.9437
loss: 0.1449, acc: 0.9451
loss: 0.1421, acc: 0.9450
loss: 0.1429, acc: 0.9443
loss: 0.1411, acc: 0.9448
loss: 0.1531, acc: 0.9413
loss: 0.1553, acc: 0.9397
loss: 0.1583, acc: 0.9383
loss: 0.1613, acc: 0.9367
loss: 0.1610, acc: 0.9368
loss: 0.1663, acc: 0.9351
loss: 0.1630, acc: 0.9365
loss: 0.1607, acc: 0.9375
loss: 0.1601, acc: 0.9381
loss: 0.1623, acc: 0.9369
loss: 0.1592, acc: 0.9378
loss: 0.1583, acc: 0.9380
loss: 0.1581, acc: 0.9377
loss: 0.1601, acc: 0.9368
loss: 0.1684, acc: 0.9350
loss: 0.1699, acc: 0.9339
loss: 0.1713, acc: 0.9336
loss: 0.1719, acc: 0.9340
loss: 0.1744, acc: 0.9325
loss: 0.1745, acc: 0.9332
loss: 0.1760, acc: 0.9331
loss: 0.1753, acc: 0.9336
loss: 0.1768, acc: 0.9323
loss: 0.1792, acc: 0.9313
loss: 0.1787, acc: 0.9316
loss: 0.1809, acc: 0.9306
loss: 0.1806, acc: 0.9308
loss: 0.1801, acc: 0.9311
loss: 0.1804, acc: 0.9316
loss: 0.1800, acc: 0.9313
loss: 0.1788, acc: 0.9317
loss: 0.1796, acc: 0.9317
loss: 0.1803, acc: 0.9314
loss: 0.1788, acc: 0.9322
loss: 0.1787, acc: 0.9319
loss: 0.1804, acc: 0.9310
loss: 0.1797, acc: 0.9314
loss: 0.1802, acc: 0.9314
loss: 0.1802, acc: 0.9315
loss: 0.1804, acc: 0.9309
loss: 0.1800, acc: 0.9309
loss: 0.1806, acc: 0.9306
loss: 0.1810, acc: 0.9301
loss: 0.1827, acc: 0.9290
loss: 0.1817, acc: 0.9293
loss: 0.1811, acc: 0.9295
loss: 0.1812, acc: 0.9292
loss: 0.1811, acc: 0.9294
loss: 0.1811, acc: 0.9293
loss: 0.1810, acc: 0.9293
loss: 0.1821, acc: 0.9290
loss: 0.1816, acc: 0.9290
loss: 0.1813, acc: 0.9291
loss: 0.1812, acc: 0.9292
loss: 0.1807, acc: 0.9296
loss: 0.1808, acc: 0.9295
loss: 0.1810, acc: 0.9296
loss: 0.1818, acc: 0.9291
> val_acc: 0.8153, val_f1: 0.8101
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 21
loss: 0.1625, acc: 0.9437
loss: 0.1533, acc: 0.9437
loss: 0.1572, acc: 0.9396
loss: 0.1523, acc: 0.9406
loss: 0.1432, acc: 0.9475
loss: 0.1475, acc: 0.9458
loss: 0.1428, acc: 0.9455
loss: 0.1441, acc: 0.9461
loss: 0.1386, acc: 0.9486
loss: 0.1417, acc: 0.9481
loss: 0.1446, acc: 0.9472
loss: 0.1426, acc: 0.9474
loss: 0.1458, acc: 0.9462
loss: 0.1469, acc: 0.9451
loss: 0.1536, acc: 0.9433
loss: 0.1605, acc: 0.9406
loss: 0.1610, acc: 0.9415
loss: 0.1674, acc: 0.9378
loss: 0.1710, acc: 0.9365
loss: 0.1700, acc: 0.9375
loss: 0.1669, acc: 0.9387
loss: 0.1685, acc: 0.9372
loss: 0.1669, acc: 0.9378
loss: 0.1674, acc: 0.9370
loss: 0.1672, acc: 0.9373
loss: 0.1673, acc: 0.9365
loss: 0.1699, acc: 0.9359
loss: 0.1705, acc: 0.9357
loss: 0.1702, acc: 0.9360
loss: 0.1683, acc: 0.9365
loss: 0.1673, acc: 0.9373
loss: 0.1695, acc: 0.9369
loss: 0.1686, acc: 0.9373
loss: 0.1683, acc: 0.9373
loss: 0.1686, acc: 0.9380
loss: 0.1706, acc: 0.9377
loss: 0.1702, acc: 0.9372
loss: 0.1712, acc: 0.9365
loss: 0.1700, acc: 0.9372
loss: 0.1699, acc: 0.9372
loss: 0.1695, acc: 0.9373
loss: 0.1706, acc: 0.9369
loss: 0.1709, acc: 0.9374
loss: 0.1705, acc: 0.9374
loss: 0.1713, acc: 0.9372
loss: 0.1716, acc: 0.9368
loss: 0.1717, acc: 0.9368
loss: 0.1713, acc: 0.9372
loss: 0.1720, acc: 0.9369
loss: 0.1717, acc: 0.9373
loss: 0.1729, acc: 0.9371
loss: 0.1739, acc: 0.9371
loss: 0.1744, acc: 0.9368
loss: 0.1741, acc: 0.9374
loss: 0.1746, acc: 0.9372
loss: 0.1742, acc: 0.9374
loss: 0.1743, acc: 0.9374
loss: 0.1742, acc: 0.9374
loss: 0.1742, acc: 0.9371
loss: 0.1735, acc: 0.9374
loss: 0.1740, acc: 0.9374
loss: 0.1751, acc: 0.9371
loss: 0.1745, acc: 0.9372
loss: 0.1746, acc: 0.9369
loss: 0.1760, acc: 0.9364
loss: 0.1758, acc: 0.9364
loss: 0.1748, acc: 0.9367
loss: 0.1749, acc: 0.9363
loss: 0.1751, acc: 0.9363
loss: 0.1773, acc: 0.9358
> val_acc: 0.8176, val_f1: 0.8078
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8249, test_f1: 0.8183
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./plm/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 594,438 || all params: 110,076,678 || trainable%: 0.5400217473859449
cuda memory allocated: 467580416
> n_trainable_params: 594438, n_nontrainable_params: 109482240
> training arguments:
>>> model_name: bert_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1611, acc: 0.4125
loss: 1.1866, acc: 0.4594
loss: 1.1725, acc: 0.4229
loss: 1.1504, acc: 0.4219
loss: 1.1223, acc: 0.4387
loss: 1.1086, acc: 0.4500
loss: 1.0955, acc: 0.4536
loss: 1.0786, acc: 0.4547
loss: 1.0601, acc: 0.4639
loss: 1.0505, acc: 0.4719
loss: 1.0383, acc: 0.4784
loss: 1.0330, acc: 0.4823
loss: 1.0176, acc: 0.4909
loss: 1.0050, acc: 0.5013
loss: 0.9906, acc: 0.5133
loss: 0.9791, acc: 0.5199
loss: 0.9705, acc: 0.5243
loss: 0.9626, acc: 0.5292
loss: 0.9558, acc: 0.5322
loss: 0.9498, acc: 0.5369
loss: 0.9495, acc: 0.5375
loss: 0.9421, acc: 0.5418
loss: 0.9321, acc: 0.5478
loss: 0.9181, acc: 0.5560
loss: 0.9127, acc: 0.5603
loss: 0.9047, acc: 0.5661
loss: 0.8935, acc: 0.5731
loss: 0.8801, acc: 0.5819
loss: 0.8717, acc: 0.5873
loss: 0.8652, acc: 0.5923
loss: 0.8592, acc: 0.5968
loss: 0.8527, acc: 0.5992
loss: 0.8488, acc: 0.6019
loss: 0.8410, acc: 0.6072
loss: 0.8359, acc: 0.6111
loss: 0.8295, acc: 0.6139
loss: 0.8243, acc: 0.6176
loss: 0.8202, acc: 0.6202
loss: 0.8166, acc: 0.6223
loss: 0.8114, acc: 0.6258
loss: 0.8056, acc: 0.6285
loss: 0.8024, acc: 0.6315
loss: 0.7962, acc: 0.6358
loss: 0.7928, acc: 0.6379
loss: 0.7894, acc: 0.6404
loss: 0.7844, acc: 0.6431
loss: 0.7840, acc: 0.6443
loss: 0.7805, acc: 0.6464
loss: 0.7752, acc: 0.6496
loss: 0.7711, acc: 0.6522
loss: 0.7677, acc: 0.6544
loss: 0.7634, acc: 0.6573
loss: 0.7578, acc: 0.6608
loss: 0.7547, acc: 0.6631
loss: 0.7536, acc: 0.6644
loss: 0.7500, acc: 0.6667
loss: 0.7455, acc: 0.6691
loss: 0.7402, acc: 0.6717
loss: 0.7396, acc: 0.6735
loss: 0.7371, acc: 0.6754
loss: 0.7347, acc: 0.6773
loss: 0.7331, acc: 0.6781
loss: 0.7305, acc: 0.6789
loss: 0.7275, acc: 0.6805
loss: 0.7251, acc: 0.6824
loss: 0.7241, acc: 0.6830
loss: 0.7224, acc: 0.6843
loss: 0.7196, acc: 0.6859
loss: 0.7165, acc: 0.6873
loss: 0.7127, acc: 0.6894
> val_acc: 0.7905, val_f1: 0.7849
>> saved: peft/bert_lora/mams//acc_0.7905_f1_0.7849_230828-0848
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.6224, acc: 0.7688
loss: 0.5386, acc: 0.7906
loss: 0.4881, acc: 0.8104
loss: 0.4863, acc: 0.8172
loss: 0.4995, acc: 0.8075
loss: 0.4999, acc: 0.8083
loss: 0.4917, acc: 0.8152
loss: 0.4836, acc: 0.8195
loss: 0.4862, acc: 0.8181
loss: 0.4699, acc: 0.8213
loss: 0.4741, acc: 0.8193
loss: 0.4763, acc: 0.8167
loss: 0.4774, acc: 0.8168
loss: 0.4766, acc: 0.8174
loss: 0.4887, acc: 0.8100
loss: 0.4924, acc: 0.8082
loss: 0.4941, acc: 0.8081
loss: 0.4938, acc: 0.8073
loss: 0.4963, acc: 0.8053
loss: 0.4983, acc: 0.8028
loss: 0.4978, acc: 0.8033
loss: 0.4978, acc: 0.8023
loss: 0.4972, acc: 0.8027
loss: 0.5019, acc: 0.8003
loss: 0.4992, acc: 0.8013
loss: 0.4984, acc: 0.8014
loss: 0.4971, acc: 0.8021
loss: 0.5020, acc: 0.8000
loss: 0.5029, acc: 0.7981
loss: 0.5043, acc: 0.7975
loss: 0.5037, acc: 0.7978
loss: 0.5000, acc: 0.8000
loss: 0.4985, acc: 0.8015
loss: 0.4992, acc: 0.8013
loss: 0.5005, acc: 0.8011
loss: 0.4976, acc: 0.8028
loss: 0.4996, acc: 0.8024
loss: 0.4988, acc: 0.8028
loss: 0.4995, acc: 0.8030
loss: 0.4977, acc: 0.8039
loss: 0.4976, acc: 0.8046
loss: 0.4987, acc: 0.8039
loss: 0.5006, acc: 0.8033
loss: 0.5002, acc: 0.8033
loss: 0.5013, acc: 0.8031
loss: 0.5011, acc: 0.8029
loss: 0.5013, acc: 0.8021
loss: 0.5028, acc: 0.8020
loss: 0.5034, acc: 0.8013
loss: 0.5033, acc: 0.8013
loss: 0.5036, acc: 0.8012
loss: 0.5031, acc: 0.8014
loss: 0.5032, acc: 0.8011
loss: 0.5030, acc: 0.8001
loss: 0.4998, acc: 0.8011
loss: 0.4993, acc: 0.8012
loss: 0.4992, acc: 0.8015
loss: 0.4996, acc: 0.8017
loss: 0.4991, acc: 0.8019
loss: 0.4989, acc: 0.8017
loss: 0.5011, acc: 0.8004
loss: 0.5013, acc: 0.8001
loss: 0.4990, acc: 0.8013
loss: 0.4996, acc: 0.8016
loss: 0.4994, acc: 0.8020
loss: 0.4999, acc: 0.8022
loss: 0.4982, acc: 0.8032
loss: 0.4978, acc: 0.8031
loss: 0.4974, acc: 0.8035
loss: 0.4970, acc: 0.8037
> val_acc: 0.7725, val_f1: 0.7668
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.3554, acc: 0.8812
loss: 0.4165, acc: 0.8375
loss: 0.4099, acc: 0.8396
loss: 0.4288, acc: 0.8375
loss: 0.4373, acc: 0.8375
loss: 0.4443, acc: 0.8323
loss: 0.4365, acc: 0.8357
loss: 0.4428, acc: 0.8328
loss: 0.4362, acc: 0.8333
loss: 0.4425, acc: 0.8325
loss: 0.4328, acc: 0.8352
loss: 0.4280, acc: 0.8354
loss: 0.4199, acc: 0.8380
loss: 0.4180, acc: 0.8411
loss: 0.4250, acc: 0.8387
loss: 0.4253, acc: 0.8375
loss: 0.4302, acc: 0.8364
loss: 0.4290, acc: 0.8375
loss: 0.4304, acc: 0.8352
loss: 0.4312, acc: 0.8366
loss: 0.4293, acc: 0.8369
loss: 0.4361, acc: 0.8347
loss: 0.4403, acc: 0.8329
loss: 0.4408, acc: 0.8323
loss: 0.4434, acc: 0.8303
loss: 0.4424, acc: 0.8303
loss: 0.4434, acc: 0.8289
loss: 0.4432, acc: 0.8292
loss: 0.4463, acc: 0.8274
loss: 0.4431, acc: 0.8281
loss: 0.4439, acc: 0.8274
loss: 0.4454, acc: 0.8277
loss: 0.4442, acc: 0.8286
loss: 0.4455, acc: 0.8281
loss: 0.4447, acc: 0.8279
loss: 0.4465, acc: 0.8271
loss: 0.4456, acc: 0.8270
loss: 0.4465, acc: 0.8268
loss: 0.4448, acc: 0.8276
loss: 0.4462, acc: 0.8269
loss: 0.4458, acc: 0.8261
loss: 0.4461, acc: 0.8259
loss: 0.4465, acc: 0.8259
loss: 0.4465, acc: 0.8264
loss: 0.4450, acc: 0.8268
loss: 0.4457, acc: 0.8261
loss: 0.4427, acc: 0.8278
loss: 0.4441, acc: 0.8271
loss: 0.4435, acc: 0.8273
loss: 0.4434, acc: 0.8275
loss: 0.4428, acc: 0.8275
loss: 0.4442, acc: 0.8273
loss: 0.4429, acc: 0.8279
loss: 0.4451, acc: 0.8272
loss: 0.4439, acc: 0.8275
loss: 0.4434, acc: 0.8275
loss: 0.4444, acc: 0.8271
loss: 0.4441, acc: 0.8267
loss: 0.4439, acc: 0.8269
loss: 0.4436, acc: 0.8267
loss: 0.4452, acc: 0.8260
loss: 0.4458, acc: 0.8256
loss: 0.4447, acc: 0.8260
loss: 0.4445, acc: 0.8264
loss: 0.4445, acc: 0.8260
loss: 0.4431, acc: 0.8267
loss: 0.4430, acc: 0.8267
loss: 0.4434, acc: 0.8260
loss: 0.4433, acc: 0.8264
loss: 0.4435, acc: 0.8258
> val_acc: 0.7973, val_f1: 0.7914
>> saved: peft/bert_lora/mams//acc_0.7973_f1_0.7914_230828-0851
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4126, acc: 0.8562
loss: 0.4144, acc: 0.8438
loss: 0.3912, acc: 0.8521
loss: 0.4183, acc: 0.8469
loss: 0.4011, acc: 0.8512
loss: 0.3960, acc: 0.8510
loss: 0.4027, acc: 0.8464
loss: 0.4051, acc: 0.8461
loss: 0.3952, acc: 0.8493
loss: 0.3909, acc: 0.8512
loss: 0.3907, acc: 0.8500
loss: 0.3881, acc: 0.8505
loss: 0.3906, acc: 0.8500
loss: 0.3879, acc: 0.8509
loss: 0.3864, acc: 0.8521
loss: 0.3819, acc: 0.8539
loss: 0.3805, acc: 0.8548
loss: 0.3858, acc: 0.8517
loss: 0.3853, acc: 0.8530
loss: 0.3856, acc: 0.8525
loss: 0.3881, acc: 0.8509
loss: 0.3894, acc: 0.8497
loss: 0.3960, acc: 0.8470
loss: 0.3980, acc: 0.8458
loss: 0.4005, acc: 0.8438
loss: 0.4029, acc: 0.8428
loss: 0.4035, acc: 0.8428
loss: 0.4073, acc: 0.8417
loss: 0.4082, acc: 0.8414
loss: 0.4100, acc: 0.8408
loss: 0.4098, acc: 0.8409
loss: 0.4120, acc: 0.8393
loss: 0.4088, acc: 0.8405
loss: 0.4080, acc: 0.8410
loss: 0.4063, acc: 0.8416
loss: 0.4083, acc: 0.8406
loss: 0.4089, acc: 0.8410
loss: 0.4118, acc: 0.8398
loss: 0.4123, acc: 0.8389
loss: 0.4122, acc: 0.8392
loss: 0.4117, acc: 0.8392
loss: 0.4129, acc: 0.8390
loss: 0.4136, acc: 0.8382
loss: 0.4142, acc: 0.8385
loss: 0.4161, acc: 0.8372
loss: 0.4178, acc: 0.8361
loss: 0.4161, acc: 0.8371
loss: 0.4166, acc: 0.8372
loss: 0.4161, acc: 0.8375
loss: 0.4160, acc: 0.8373
loss: 0.4171, acc: 0.8366
loss: 0.4177, acc: 0.8365
loss: 0.4168, acc: 0.8376
loss: 0.4177, acc: 0.8374
loss: 0.4167, acc: 0.8376
loss: 0.4172, acc: 0.8373
loss: 0.4164, acc: 0.8376
loss: 0.4155, acc: 0.8378
loss: 0.4153, acc: 0.8379
loss: 0.4164, acc: 0.8375
loss: 0.4175, acc: 0.8372
loss: 0.4177, acc: 0.8362
loss: 0.4186, acc: 0.8354
loss: 0.4192, acc: 0.8354
loss: 0.4211, acc: 0.8347
loss: 0.4192, acc: 0.8356
loss: 0.4181, acc: 0.8360
loss: 0.4173, acc: 0.8365
loss: 0.4162, acc: 0.8366
loss: 0.4165, acc: 0.8361
> val_acc: 0.8056, val_f1: 0.7976
>> saved: peft/bert_lora/mams//acc_0.8056_f1_0.7976_230828-0852
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.3962, acc: 0.8250
loss: 0.4211, acc: 0.8281
loss: 0.4010, acc: 0.8375
loss: 0.3689, acc: 0.8516
loss: 0.3605, acc: 0.8538
loss: 0.3558, acc: 0.8573
loss: 0.3595, acc: 0.8536
loss: 0.3626, acc: 0.8516
loss: 0.3731, acc: 0.8465
loss: 0.3698, acc: 0.8488
loss: 0.3728, acc: 0.8489
loss: 0.3654, acc: 0.8536
loss: 0.3634, acc: 0.8543
loss: 0.3591, acc: 0.8554
loss: 0.3620, acc: 0.8542
loss: 0.3726, acc: 0.8496
loss: 0.3688, acc: 0.8511
loss: 0.3650, acc: 0.8531
loss: 0.3633, acc: 0.8536
loss: 0.3595, acc: 0.8556
loss: 0.3574, acc: 0.8560
loss: 0.3579, acc: 0.8568
loss: 0.3549, acc: 0.8584
loss: 0.3543, acc: 0.8583
loss: 0.3529, acc: 0.8585
loss: 0.3564, acc: 0.8589
loss: 0.3581, acc: 0.8593
loss: 0.3583, acc: 0.8594
loss: 0.3594, acc: 0.8575
loss: 0.3574, acc: 0.8592
loss: 0.3599, acc: 0.8591
loss: 0.3600, acc: 0.8594
loss: 0.3603, acc: 0.8589
loss: 0.3599, acc: 0.8594
loss: 0.3573, acc: 0.8605
loss: 0.3564, acc: 0.8606
loss: 0.3605, acc: 0.8588
loss: 0.3599, acc: 0.8590
loss: 0.3583, acc: 0.8603
loss: 0.3582, acc: 0.8606
loss: 0.3615, acc: 0.8598
loss: 0.3647, acc: 0.8582
loss: 0.3663, acc: 0.8576
loss: 0.3652, acc: 0.8585
loss: 0.3669, acc: 0.8585
loss: 0.3685, acc: 0.8582
loss: 0.3689, acc: 0.8573
loss: 0.3712, acc: 0.8564
loss: 0.3710, acc: 0.8565
loss: 0.3716, acc: 0.8559
loss: 0.3741, acc: 0.8545
loss: 0.3723, acc: 0.8553
loss: 0.3760, acc: 0.8537
loss: 0.3771, acc: 0.8534
loss: 0.3770, acc: 0.8535
loss: 0.3757, acc: 0.8542
loss: 0.3764, acc: 0.8539
loss: 0.3767, acc: 0.8542
loss: 0.3759, acc: 0.8548
loss: 0.3756, acc: 0.8548
loss: 0.3750, acc: 0.8551
loss: 0.3746, acc: 0.8554
loss: 0.3739, acc: 0.8554
loss: 0.3732, acc: 0.8560
loss: 0.3747, acc: 0.8554
loss: 0.3746, acc: 0.8554
loss: 0.3757, acc: 0.8548
loss: 0.3768, acc: 0.8540
loss: 0.3776, acc: 0.8534
loss: 0.3767, acc: 0.8537
> val_acc: 0.8183, val_f1: 0.8150
>> saved: peft/bert_lora/mams//acc_0.8183_f1_0.815_230828-0854
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.2499, acc: 0.9000
loss: 0.2774, acc: 0.9000
loss: 0.2954, acc: 0.8938
loss: 0.3118, acc: 0.8828
loss: 0.3042, acc: 0.8862
loss: 0.2997, acc: 0.8844
loss: 0.2925, acc: 0.8884
loss: 0.3094, acc: 0.8820
loss: 0.3119, acc: 0.8812
loss: 0.3100, acc: 0.8806
loss: 0.3135, acc: 0.8812
loss: 0.3177, acc: 0.8781
loss: 0.3186, acc: 0.8755
loss: 0.3163, acc: 0.8777
loss: 0.3137, acc: 0.8796
loss: 0.3198, acc: 0.8758
loss: 0.3153, acc: 0.8768
loss: 0.3194, acc: 0.8760
loss: 0.3231, acc: 0.8730
loss: 0.3213, acc: 0.8738
loss: 0.3230, acc: 0.8720
loss: 0.3273, acc: 0.8705
loss: 0.3251, acc: 0.8720
loss: 0.3239, acc: 0.8737
loss: 0.3207, acc: 0.8750
loss: 0.3226, acc: 0.8743
loss: 0.3236, acc: 0.8748
loss: 0.3232, acc: 0.8743
loss: 0.3265, acc: 0.8726
loss: 0.3258, acc: 0.8727
loss: 0.3235, acc: 0.8736
loss: 0.3226, acc: 0.8742
loss: 0.3196, acc: 0.8754
loss: 0.3185, acc: 0.8765
loss: 0.3215, acc: 0.8761
loss: 0.3257, acc: 0.8741
loss: 0.3282, acc: 0.8726
loss: 0.3272, acc: 0.8734
loss: 0.3303, acc: 0.8724
loss: 0.3299, acc: 0.8727
loss: 0.3308, acc: 0.8729
loss: 0.3302, acc: 0.8731
loss: 0.3308, acc: 0.8738
loss: 0.3299, acc: 0.8743
loss: 0.3283, acc: 0.8749
loss: 0.3289, acc: 0.8749
loss: 0.3285, acc: 0.8751
loss: 0.3304, acc: 0.8746
loss: 0.3306, acc: 0.8746
loss: 0.3302, acc: 0.8739
loss: 0.3301, acc: 0.8735
loss: 0.3303, acc: 0.8737
loss: 0.3306, acc: 0.8736
loss: 0.3300, acc: 0.8737
loss: 0.3325, acc: 0.8731
loss: 0.3333, acc: 0.8725
loss: 0.3353, acc: 0.8718
loss: 0.3353, acc: 0.8719
loss: 0.3354, acc: 0.8717
loss: 0.3346, acc: 0.8722
loss: 0.3353, acc: 0.8720
loss: 0.3356, acc: 0.8720
loss: 0.3363, acc: 0.8716
loss: 0.3361, acc: 0.8716
loss: 0.3378, acc: 0.8706
loss: 0.3370, acc: 0.8710
loss: 0.3387, acc: 0.8701
loss: 0.3393, acc: 0.8699
loss: 0.3390, acc: 0.8701
loss: 0.3406, acc: 0.8697
> val_acc: 0.8071, val_f1: 0.7970
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2274, acc: 0.9125
loss: 0.2838, acc: 0.8875
loss: 0.2697, acc: 0.8896
loss: 0.2605, acc: 0.8984
loss: 0.2720, acc: 0.8875
loss: 0.2748, acc: 0.8865
loss: 0.2691, acc: 0.8821
loss: 0.2884, acc: 0.8781
loss: 0.2853, acc: 0.8819
loss: 0.2931, acc: 0.8819
loss: 0.2970, acc: 0.8812
loss: 0.3016, acc: 0.8792
loss: 0.3000, acc: 0.8793
loss: 0.3014, acc: 0.8781
loss: 0.3028, acc: 0.8779
loss: 0.3020, acc: 0.8785
loss: 0.3022, acc: 0.8798
loss: 0.3017, acc: 0.8802
loss: 0.2998, acc: 0.8799
loss: 0.2940, acc: 0.8834
loss: 0.2973, acc: 0.8830
loss: 0.2982, acc: 0.8827
loss: 0.2997, acc: 0.8834
loss: 0.3025, acc: 0.8818
loss: 0.3044, acc: 0.8820
loss: 0.3053, acc: 0.8810
loss: 0.3060, acc: 0.8799
loss: 0.3044, acc: 0.8801
loss: 0.3044, acc: 0.8808
loss: 0.3029, acc: 0.8819
loss: 0.3049, acc: 0.8815
loss: 0.3023, acc: 0.8828
loss: 0.3054, acc: 0.8822
loss: 0.3059, acc: 0.8820
loss: 0.3069, acc: 0.8820
loss: 0.3057, acc: 0.8821
loss: 0.3059, acc: 0.8819
loss: 0.3066, acc: 0.8822
loss: 0.3083, acc: 0.8816
loss: 0.3097, acc: 0.8817
loss: 0.3106, acc: 0.8817
loss: 0.3095, acc: 0.8824
loss: 0.3108, acc: 0.8818
loss: 0.3112, acc: 0.8820
loss: 0.3091, acc: 0.8831
loss: 0.3093, acc: 0.8826
loss: 0.3088, acc: 0.8830
loss: 0.3100, acc: 0.8826
loss: 0.3099, acc: 0.8825
loss: 0.3118, acc: 0.8812
loss: 0.3108, acc: 0.8817
loss: 0.3122, acc: 0.8811
loss: 0.3125, acc: 0.8808
loss: 0.3141, acc: 0.8802
loss: 0.3144, acc: 0.8798
loss: 0.3141, acc: 0.8798
loss: 0.3139, acc: 0.8799
loss: 0.3144, acc: 0.8794
loss: 0.3148, acc: 0.8789
loss: 0.3153, acc: 0.8785
loss: 0.3181, acc: 0.8779
loss: 0.3186, acc: 0.8776
loss: 0.3179, acc: 0.8781
loss: 0.3177, acc: 0.8782
loss: 0.3178, acc: 0.8780
loss: 0.3201, acc: 0.8771
loss: 0.3208, acc: 0.8767
loss: 0.3209, acc: 0.8767
loss: 0.3202, acc: 0.8771
loss: 0.3197, acc: 0.8775
> val_acc: 0.8131, val_f1: 0.8046
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2912, acc: 0.9250
loss: 0.2697, acc: 0.9156
loss: 0.2688, acc: 0.9083
loss: 0.2768, acc: 0.9062
loss: 0.2787, acc: 0.9025
loss: 0.2736, acc: 0.9021
loss: 0.2638, acc: 0.9054
loss: 0.2614, acc: 0.9062
loss: 0.2702, acc: 0.9028
loss: 0.2781, acc: 0.9000
loss: 0.2796, acc: 0.8983
loss: 0.2791, acc: 0.8995
loss: 0.2867, acc: 0.8971
loss: 0.2845, acc: 0.8973
loss: 0.2804, acc: 0.8983
loss: 0.2768, acc: 0.8988
loss: 0.2776, acc: 0.8996
loss: 0.2773, acc: 0.8997
loss: 0.2787, acc: 0.8990
loss: 0.2833, acc: 0.8969
loss: 0.2856, acc: 0.8961
loss: 0.2869, acc: 0.8943
loss: 0.2868, acc: 0.8935
loss: 0.2877, acc: 0.8927
loss: 0.2891, acc: 0.8915
loss: 0.2900, acc: 0.8911
loss: 0.2935, acc: 0.8900
loss: 0.2931, acc: 0.8908
loss: 0.2937, acc: 0.8912
loss: 0.2930, acc: 0.8912
loss: 0.2942, acc: 0.8907
loss: 0.2960, acc: 0.8904
loss: 0.2964, acc: 0.8898
loss: 0.2964, acc: 0.8899
loss: 0.2994, acc: 0.8884
loss: 0.2965, acc: 0.8896
loss: 0.2950, acc: 0.8897
loss: 0.2956, acc: 0.8890
loss: 0.2941, acc: 0.8894
loss: 0.2953, acc: 0.8886
loss: 0.2945, acc: 0.8892
loss: 0.2986, acc: 0.8876
loss: 0.2988, acc: 0.8874
loss: 0.2986, acc: 0.8878
loss: 0.2979, acc: 0.8881
loss: 0.2962, acc: 0.8887
loss: 0.2943, acc: 0.8896
loss: 0.2923, acc: 0.8908
loss: 0.2973, acc: 0.8889
loss: 0.2964, acc: 0.8889
loss: 0.2977, acc: 0.8877
loss: 0.2985, acc: 0.8871
loss: 0.2992, acc: 0.8871
loss: 0.2986, acc: 0.8873
loss: 0.2988, acc: 0.8874
loss: 0.3011, acc: 0.8865
loss: 0.3003, acc: 0.8870
loss: 0.2998, acc: 0.8870
loss: 0.2993, acc: 0.8872
loss: 0.2991, acc: 0.8868
loss: 0.2988, acc: 0.8867
loss: 0.2998, acc: 0.8867
loss: 0.3032, acc: 0.8855
loss: 0.3038, acc: 0.8854
loss: 0.3050, acc: 0.8847
loss: 0.3054, acc: 0.8845
loss: 0.3042, acc: 0.8849
loss: 0.3049, acc: 0.8847
loss: 0.3063, acc: 0.8840
loss: 0.3064, acc: 0.8840
> val_acc: 0.8266, val_f1: 0.8194
>> saved: peft/bert_lora/mams//acc_0.8266_f1_0.8194_230828-0858
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.2162, acc: 0.9313
loss: 0.2306, acc: 0.9219
loss: 0.2351, acc: 0.9229
loss: 0.2160, acc: 0.9266
loss: 0.2511, acc: 0.9137
loss: 0.2587, acc: 0.9094
loss: 0.2615, acc: 0.9080
loss: 0.2692, acc: 0.9016
loss: 0.2677, acc: 0.9021
loss: 0.2618, acc: 0.9019
loss: 0.2564, acc: 0.9034
loss: 0.2550, acc: 0.9031
loss: 0.2579, acc: 0.9024
loss: 0.2577, acc: 0.9022
loss: 0.2685, acc: 0.8979
loss: 0.2667, acc: 0.8980
loss: 0.2686, acc: 0.8967
loss: 0.2673, acc: 0.8979
loss: 0.2699, acc: 0.8957
loss: 0.2745, acc: 0.8922
loss: 0.2743, acc: 0.8926
loss: 0.2722, acc: 0.8935
loss: 0.2702, acc: 0.8946
loss: 0.2716, acc: 0.8932
loss: 0.2723, acc: 0.8928
loss: 0.2729, acc: 0.8923
loss: 0.2720, acc: 0.8935
loss: 0.2704, acc: 0.8951
loss: 0.2689, acc: 0.8963
loss: 0.2667, acc: 0.8977
loss: 0.2644, acc: 0.8982
loss: 0.2631, acc: 0.8986
loss: 0.2657, acc: 0.8979
loss: 0.2643, acc: 0.8989
loss: 0.2634, acc: 0.8998
loss: 0.2641, acc: 0.8990
loss: 0.2661, acc: 0.8978
loss: 0.2652, acc: 0.8980
loss: 0.2647, acc: 0.8987
loss: 0.2668, acc: 0.8977
loss: 0.2659, acc: 0.8977
loss: 0.2677, acc: 0.8967
loss: 0.2678, acc: 0.8961
loss: 0.2679, acc: 0.8959
loss: 0.2692, acc: 0.8953
loss: 0.2694, acc: 0.8954
loss: 0.2709, acc: 0.8949
loss: 0.2703, acc: 0.8949
loss: 0.2695, acc: 0.8952
loss: 0.2727, acc: 0.8942
loss: 0.2724, acc: 0.8941
loss: 0.2727, acc: 0.8941
loss: 0.2727, acc: 0.8942
loss: 0.2730, acc: 0.8936
loss: 0.2732, acc: 0.8936
loss: 0.2746, acc: 0.8935
loss: 0.2736, acc: 0.8941
loss: 0.2745, acc: 0.8933
loss: 0.2752, acc: 0.8927
loss: 0.2743, acc: 0.8929
loss: 0.2748, acc: 0.8933
loss: 0.2745, acc: 0.8933
loss: 0.2768, acc: 0.8930
loss: 0.2765, acc: 0.8931
loss: 0.2775, acc: 0.8933
loss: 0.2774, acc: 0.8932
loss: 0.2781, acc: 0.8929
loss: 0.2786, acc: 0.8926
loss: 0.2787, acc: 0.8927
loss: 0.2791, acc: 0.8925
> val_acc: 0.8131, val_f1: 0.8065
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.1835, acc: 0.9375
loss: 0.2026, acc: 0.9250
loss: 0.1964, acc: 0.9229
loss: 0.2066, acc: 0.9187
loss: 0.2034, acc: 0.9200
loss: 0.2079, acc: 0.9167
loss: 0.2142, acc: 0.9179
loss: 0.2207, acc: 0.9141
loss: 0.2176, acc: 0.9160
loss: 0.2222, acc: 0.9156
loss: 0.2203, acc: 0.9182
loss: 0.2269, acc: 0.9151
loss: 0.2270, acc: 0.9149
loss: 0.2280, acc: 0.9138
loss: 0.2295, acc: 0.9129
loss: 0.2294, acc: 0.9137
loss: 0.2237, acc: 0.9173
loss: 0.2231, acc: 0.9174
loss: 0.2254, acc: 0.9151
loss: 0.2276, acc: 0.9150
loss: 0.2296, acc: 0.9140
loss: 0.2305, acc: 0.9128
loss: 0.2343, acc: 0.9125
loss: 0.2364, acc: 0.9122
loss: 0.2375, acc: 0.9117
loss: 0.2368, acc: 0.9120
loss: 0.2378, acc: 0.9125
loss: 0.2376, acc: 0.9134
loss: 0.2378, acc: 0.9136
loss: 0.2376, acc: 0.9137
loss: 0.2383, acc: 0.9133
loss: 0.2368, acc: 0.9131
loss: 0.2387, acc: 0.9125
loss: 0.2378, acc: 0.9123
loss: 0.2381, acc: 0.9121
loss: 0.2368, acc: 0.9125
loss: 0.2381, acc: 0.9123
loss: 0.2403, acc: 0.9117
loss: 0.2399, acc: 0.9115
loss: 0.2457, acc: 0.9089
loss: 0.2485, acc: 0.9079
loss: 0.2510, acc: 0.9065
loss: 0.2567, acc: 0.9042
loss: 0.2618, acc: 0.9027
loss: 0.2641, acc: 0.9018
loss: 0.2651, acc: 0.9015
loss: 0.2670, acc: 0.9011
loss: 0.2684, acc: 0.9005
loss: 0.2691, acc: 0.8996
loss: 0.2703, acc: 0.8996
loss: 0.2710, acc: 0.8989
loss: 0.2728, acc: 0.8972
loss: 0.2724, acc: 0.8975
loss: 0.2746, acc: 0.8964
loss: 0.2750, acc: 0.8962
loss: 0.2753, acc: 0.8962
loss: 0.2746, acc: 0.8966
loss: 0.2747, acc: 0.8969
loss: 0.2759, acc: 0.8966
loss: 0.2761, acc: 0.8968
loss: 0.2758, acc: 0.8971
loss: 0.2779, acc: 0.8962
loss: 0.2784, acc: 0.8960
loss: 0.2803, acc: 0.8953
loss: 0.2803, acc: 0.8955
loss: 0.2808, acc: 0.8952
loss: 0.2809, acc: 0.8949
loss: 0.2810, acc: 0.8950
loss: 0.2815, acc: 0.8948
loss: 0.2813, acc: 0.8948
> val_acc: 0.8071, val_f1: 0.8032
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.2298, acc: 0.9062
loss: 0.2182, acc: 0.9094
loss: 0.2245, acc: 0.9021
loss: 0.2056, acc: 0.9156
loss: 0.2142, acc: 0.9175
loss: 0.2218, acc: 0.9187
loss: 0.2287, acc: 0.9143
loss: 0.2352, acc: 0.9086
loss: 0.2312, acc: 0.9125
loss: 0.2416, acc: 0.9087
loss: 0.2435, acc: 0.9091
loss: 0.2419, acc: 0.9089
loss: 0.2412, acc: 0.9096
loss: 0.2488, acc: 0.9094
loss: 0.2482, acc: 0.9100
loss: 0.2522, acc: 0.9074
loss: 0.2516, acc: 0.9077
loss: 0.2501, acc: 0.9087
loss: 0.2473, acc: 0.9092
loss: 0.2460, acc: 0.9100
loss: 0.2440, acc: 0.9098
loss: 0.2430, acc: 0.9102
loss: 0.2437, acc: 0.9098
loss: 0.2432, acc: 0.9094
loss: 0.2440, acc: 0.9090
loss: 0.2467, acc: 0.9079
loss: 0.2477, acc: 0.9081
loss: 0.2474, acc: 0.9074
loss: 0.2497, acc: 0.9069
loss: 0.2501, acc: 0.9073
loss: 0.2541, acc: 0.9050
loss: 0.2551, acc: 0.9047
loss: 0.2584, acc: 0.9036
loss: 0.2606, acc: 0.9033
loss: 0.2594, acc: 0.9038
loss: 0.2598, acc: 0.9036
loss: 0.2561, acc: 0.9052
loss: 0.2567, acc: 0.9053
loss: 0.2568, acc: 0.9056
loss: 0.2565, acc: 0.9062
loss: 0.2566, acc: 0.9062
loss: 0.2563, acc: 0.9064
loss: 0.2554, acc: 0.9068
loss: 0.2561, acc: 0.9070
loss: 0.2557, acc: 0.9069
loss: 0.2548, acc: 0.9075
loss: 0.2572, acc: 0.9068
loss: 0.2599, acc: 0.9057
loss: 0.2625, acc: 0.9047
loss: 0.2639, acc: 0.9045
loss: 0.2641, acc: 0.9045
loss: 0.2641, acc: 0.9048
loss: 0.2643, acc: 0.9046
loss: 0.2636, acc: 0.9049
loss: 0.2650, acc: 0.9043
loss: 0.2653, acc: 0.9038
loss: 0.2671, acc: 0.9029
loss: 0.2682, acc: 0.9026
loss: 0.2694, acc: 0.9017
loss: 0.2684, acc: 0.9018
loss: 0.2690, acc: 0.9018
loss: 0.2690, acc: 0.9020
loss: 0.2687, acc: 0.9024
loss: 0.2695, acc: 0.9021
loss: 0.2704, acc: 0.9014
loss: 0.2701, acc: 0.9010
loss: 0.2716, acc: 0.9005
loss: 0.2716, acc: 0.9008
loss: 0.2721, acc: 0.9004
loss: 0.2723, acc: 0.9006
> val_acc: 0.8138, val_f1: 0.8064
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.1738, acc: 0.9437
loss: 0.1823, acc: 0.9344
loss: 0.1997, acc: 0.9187
loss: 0.1980, acc: 0.9234
loss: 0.1937, acc: 0.9250
loss: 0.2036, acc: 0.9229
loss: 0.2025, acc: 0.9241
loss: 0.2089, acc: 0.9227
loss: 0.2142, acc: 0.9194
loss: 0.2143, acc: 0.9219
loss: 0.2096, acc: 0.9233
loss: 0.2069, acc: 0.9240
loss: 0.2088, acc: 0.9226
loss: 0.2079, acc: 0.9228
loss: 0.2104, acc: 0.9208
loss: 0.2075, acc: 0.9215
loss: 0.2095, acc: 0.9213
loss: 0.2125, acc: 0.9219
loss: 0.2180, acc: 0.9197
loss: 0.2162, acc: 0.9197
loss: 0.2146, acc: 0.9205
loss: 0.2148, acc: 0.9210
loss: 0.2163, acc: 0.9201
loss: 0.2186, acc: 0.9201
loss: 0.2205, acc: 0.9190
loss: 0.2248, acc: 0.9180
loss: 0.2267, acc: 0.9162
loss: 0.2288, acc: 0.9143
loss: 0.2302, acc: 0.9136
loss: 0.2312, acc: 0.9129
loss: 0.2304, acc: 0.9135
loss: 0.2337, acc: 0.9125
loss: 0.2357, acc: 0.9125
loss: 0.2359, acc: 0.9123
loss: 0.2358, acc: 0.9123
loss: 0.2378, acc: 0.9118
loss: 0.2382, acc: 0.9117
loss: 0.2392, acc: 0.9113
loss: 0.2410, acc: 0.9104
loss: 0.2385, acc: 0.9113
loss: 0.2412, acc: 0.9104
loss: 0.2429, acc: 0.9100
loss: 0.2461, acc: 0.9084
loss: 0.2452, acc: 0.9088
loss: 0.2442, acc: 0.9092
loss: 0.2445, acc: 0.9092
loss: 0.2441, acc: 0.9094
loss: 0.2464, acc: 0.9082
loss: 0.2488, acc: 0.9077
loss: 0.2501, acc: 0.9070
loss: 0.2512, acc: 0.9067
loss: 0.2506, acc: 0.9076
loss: 0.2514, acc: 0.9074
loss: 0.2508, acc: 0.9076
loss: 0.2515, acc: 0.9072
loss: 0.2520, acc: 0.9068
loss: 0.2531, acc: 0.9062
loss: 0.2534, acc: 0.9058
loss: 0.2533, acc: 0.9054
loss: 0.2523, acc: 0.9058
loss: 0.2525, acc: 0.9058
loss: 0.2527, acc: 0.9053
loss: 0.2541, acc: 0.9053
loss: 0.2543, acc: 0.9055
loss: 0.2550, acc: 0.9051
loss: 0.2557, acc: 0.9048
loss: 0.2571, acc: 0.9039
loss: 0.2580, acc: 0.9035
loss: 0.2585, acc: 0.9033
loss: 0.2577, acc: 0.9035
> val_acc: 0.8063, val_f1: 0.8013
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.1494, acc: 0.9375
loss: 0.1698, acc: 0.9281
loss: 0.1894, acc: 0.9250
loss: 0.1812, acc: 0.9297
loss: 0.1743, acc: 0.9300
loss: 0.1711, acc: 0.9271
loss: 0.1742, acc: 0.9277
loss: 0.1974, acc: 0.9195
loss: 0.2159, acc: 0.9118
loss: 0.2174, acc: 0.9125
loss: 0.2164, acc: 0.9136
loss: 0.2191, acc: 0.9130
loss: 0.2259, acc: 0.9115
loss: 0.2233, acc: 0.9129
loss: 0.2233, acc: 0.9125
loss: 0.2192, acc: 0.9152
loss: 0.2200, acc: 0.9147
loss: 0.2166, acc: 0.9156
loss: 0.2202, acc: 0.9138
loss: 0.2221, acc: 0.9131
loss: 0.2207, acc: 0.9140
loss: 0.2192, acc: 0.9151
loss: 0.2191, acc: 0.9147
loss: 0.2164, acc: 0.9154
loss: 0.2184, acc: 0.9127
loss: 0.2193, acc: 0.9125
loss: 0.2187, acc: 0.9132
loss: 0.2184, acc: 0.9141
loss: 0.2205, acc: 0.9138
loss: 0.2231, acc: 0.9127
loss: 0.2259, acc: 0.9115
loss: 0.2260, acc: 0.9115
loss: 0.2273, acc: 0.9108
loss: 0.2303, acc: 0.9094
loss: 0.2317, acc: 0.9089
loss: 0.2341, acc: 0.9082
loss: 0.2344, acc: 0.9084
loss: 0.2336, acc: 0.9087
loss: 0.2324, acc: 0.9093
loss: 0.2328, acc: 0.9092
loss: 0.2337, acc: 0.9085
loss: 0.2339, acc: 0.9089
loss: 0.2350, acc: 0.9086
loss: 0.2364, acc: 0.9081
loss: 0.2384, acc: 0.9078
loss: 0.2377, acc: 0.9082
loss: 0.2382, acc: 0.9078
loss: 0.2381, acc: 0.9078
loss: 0.2376, acc: 0.9082
loss: 0.2372, acc: 0.9086
loss: 0.2366, acc: 0.9092
loss: 0.2368, acc: 0.9088
loss: 0.2387, acc: 0.9081
loss: 0.2369, acc: 0.9089
loss: 0.2368, acc: 0.9092
loss: 0.2359, acc: 0.9095
loss: 0.2349, acc: 0.9098
loss: 0.2344, acc: 0.9103
loss: 0.2351, acc: 0.9104
loss: 0.2348, acc: 0.9105
loss: 0.2336, acc: 0.9108
loss: 0.2335, acc: 0.9110
loss: 0.2339, acc: 0.9105
loss: 0.2362, acc: 0.9100
loss: 0.2365, acc: 0.9100
loss: 0.2374, acc: 0.9098
loss: 0.2375, acc: 0.9097
loss: 0.2383, acc: 0.9097
loss: 0.2384, acc: 0.9099
loss: 0.2384, acc: 0.9100
> val_acc: 0.8311, val_f1: 0.8228
>> saved: peft/bert_lora/mams//acc_0.8311_f1_0.8228_230828-0906
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.1880, acc: 0.9250
loss: 0.1682, acc: 0.9344
loss: 0.1831, acc: 0.9375
loss: 0.1781, acc: 0.9406
loss: 0.1838, acc: 0.9387
loss: 0.1806, acc: 0.9375
loss: 0.1847, acc: 0.9375
loss: 0.1887, acc: 0.9359
loss: 0.1833, acc: 0.9375
loss: 0.1802, acc: 0.9375
loss: 0.1858, acc: 0.9352
loss: 0.2012, acc: 0.9302
loss: 0.2057, acc: 0.9279
loss: 0.2044, acc: 0.9272
loss: 0.2040, acc: 0.9267
loss: 0.2078, acc: 0.9242
loss: 0.2070, acc: 0.9235
loss: 0.2031, acc: 0.9250
loss: 0.2063, acc: 0.9243
loss: 0.2092, acc: 0.9231
loss: 0.2095, acc: 0.9223
loss: 0.2068, acc: 0.9239
loss: 0.2089, acc: 0.9234
loss: 0.2129, acc: 0.9216
loss: 0.2110, acc: 0.9230
loss: 0.2136, acc: 0.9216
loss: 0.2129, acc: 0.9215
loss: 0.2138, acc: 0.9212
loss: 0.2145, acc: 0.9213
loss: 0.2148, acc: 0.9213
loss: 0.2181, acc: 0.9196
loss: 0.2166, acc: 0.9201
loss: 0.2164, acc: 0.9205
loss: 0.2172, acc: 0.9193
loss: 0.2156, acc: 0.9195
loss: 0.2161, acc: 0.9193
loss: 0.2168, acc: 0.9194
loss: 0.2182, acc: 0.9186
loss: 0.2189, acc: 0.9179
loss: 0.2187, acc: 0.9177
loss: 0.2183, acc: 0.9178
loss: 0.2199, acc: 0.9171
loss: 0.2205, acc: 0.9166
loss: 0.2208, acc: 0.9161
loss: 0.2219, acc: 0.9154
loss: 0.2215, acc: 0.9158
loss: 0.2212, acc: 0.9158
loss: 0.2210, acc: 0.9160
loss: 0.2209, acc: 0.9162
loss: 0.2226, acc: 0.9157
loss: 0.2230, acc: 0.9154
loss: 0.2224, acc: 0.9159
loss: 0.2217, acc: 0.9160
loss: 0.2230, acc: 0.9153
loss: 0.2222, acc: 0.9156
loss: 0.2225, acc: 0.9154
loss: 0.2227, acc: 0.9149
loss: 0.2234, acc: 0.9151
loss: 0.2237, acc: 0.9150
loss: 0.2227, acc: 0.9156
loss: 0.2230, acc: 0.9154
loss: 0.2230, acc: 0.9153
loss: 0.2235, acc: 0.9152
loss: 0.2230, acc: 0.9152
loss: 0.2222, acc: 0.9156
loss: 0.2213, acc: 0.9160
loss: 0.2224, acc: 0.9153
loss: 0.2221, acc: 0.9154
loss: 0.2213, acc: 0.9157
loss: 0.2225, acc: 0.9153
> val_acc: 0.8153, val_f1: 0.8116
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 0.1403, acc: 0.9563
loss: 0.1610, acc: 0.9344
loss: 0.1611, acc: 0.9292
loss: 0.1526, acc: 0.9359
loss: 0.1674, acc: 0.9275
loss: 0.1545, acc: 0.9323
loss: 0.1538, acc: 0.9339
loss: 0.1565, acc: 0.9352
loss: 0.1593, acc: 0.9354
loss: 0.1624, acc: 0.9356
loss: 0.1642, acc: 0.9375
loss: 0.1692, acc: 0.9380
loss: 0.1692, acc: 0.9375
loss: 0.1691, acc: 0.9384
loss: 0.1744, acc: 0.9383
loss: 0.1746, acc: 0.9375
loss: 0.1750, acc: 0.9357
loss: 0.1766, acc: 0.9351
loss: 0.1764, acc: 0.9355
loss: 0.1786, acc: 0.9350
loss: 0.1803, acc: 0.9348
loss: 0.1838, acc: 0.9344
loss: 0.1846, acc: 0.9342
loss: 0.1855, acc: 0.9341
loss: 0.1841, acc: 0.9345
loss: 0.1843, acc: 0.9337
loss: 0.1870, acc: 0.9329
loss: 0.1886, acc: 0.9328
loss: 0.1930, acc: 0.9313
loss: 0.1958, acc: 0.9306
loss: 0.1955, acc: 0.9308
loss: 0.1955, acc: 0.9303
loss: 0.1967, acc: 0.9295
loss: 0.1983, acc: 0.9281
loss: 0.1995, acc: 0.9279
loss: 0.2020, acc: 0.9271
loss: 0.2015, acc: 0.9269
loss: 0.2025, acc: 0.9262
loss: 0.2005, acc: 0.9271
loss: 0.1996, acc: 0.9272
loss: 0.2001, acc: 0.9271
loss: 0.2003, acc: 0.9266
loss: 0.1994, acc: 0.9269
loss: 0.2011, acc: 0.9268
loss: 0.2013, acc: 0.9268
loss: 0.2006, acc: 0.9270
loss: 0.2003, acc: 0.9274
loss: 0.2018, acc: 0.9272
loss: 0.2022, acc: 0.9269
loss: 0.2025, acc: 0.9269
loss: 0.2026, acc: 0.9268
loss: 0.2022, acc: 0.9270
loss: 0.2049, acc: 0.9258
loss: 0.2051, acc: 0.9255
loss: 0.2042, acc: 0.9259
loss: 0.2054, acc: 0.9253
loss: 0.2061, acc: 0.9250
loss: 0.2061, acc: 0.9248
loss: 0.2066, acc: 0.9248
loss: 0.2080, acc: 0.9244
loss: 0.2098, acc: 0.9233
loss: 0.2096, acc: 0.9234
loss: 0.2095, acc: 0.9232
loss: 0.2114, acc: 0.9225
loss: 0.2114, acc: 0.9225
loss: 0.2125, acc: 0.9222
loss: 0.2131, acc: 0.9221
loss: 0.2131, acc: 0.9222
loss: 0.2149, acc: 0.9217
loss: 0.2146, acc: 0.9220
> val_acc: 0.8221, val_f1: 0.8153
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 15
loss: 0.2144, acc: 0.9375
loss: 0.1956, acc: 0.9344
loss: 0.2021, acc: 0.9354
loss: 0.1850, acc: 0.9359
loss: 0.1917, acc: 0.9325
loss: 0.1766, acc: 0.9375
loss: 0.1781, acc: 0.9384
loss: 0.1759, acc: 0.9398
loss: 0.1773, acc: 0.9375
loss: 0.1785, acc: 0.9363
loss: 0.1754, acc: 0.9364
loss: 0.1775, acc: 0.9359
loss: 0.1706, acc: 0.9385
loss: 0.1769, acc: 0.9375
loss: 0.1798, acc: 0.9358
loss: 0.1869, acc: 0.9340
loss: 0.1889, acc: 0.9335
loss: 0.1886, acc: 0.9330
loss: 0.1885, acc: 0.9332
loss: 0.1886, acc: 0.9341
loss: 0.1879, acc: 0.9351
loss: 0.1912, acc: 0.9347
loss: 0.1927, acc: 0.9334
loss: 0.1928, acc: 0.9333
loss: 0.1921, acc: 0.9340
loss: 0.1934, acc: 0.9329
loss: 0.1925, acc: 0.9331
loss: 0.1938, acc: 0.9326
loss: 0.1935, acc: 0.9321
loss: 0.1962, acc: 0.9304
loss: 0.1984, acc: 0.9296
loss: 0.1991, acc: 0.9291
loss: 0.1978, acc: 0.9290
loss: 0.1973, acc: 0.9289
loss: 0.1952, acc: 0.9295
loss: 0.1959, acc: 0.9290
loss: 0.1943, acc: 0.9292
loss: 0.1937, acc: 0.9291
loss: 0.1931, acc: 0.9290
loss: 0.1930, acc: 0.9289
loss: 0.1923, acc: 0.9290
loss: 0.1938, acc: 0.9289
loss: 0.1958, acc: 0.9283
loss: 0.1965, acc: 0.9277
loss: 0.2001, acc: 0.9263
loss: 0.2009, acc: 0.9260
loss: 0.2004, acc: 0.9259
loss: 0.1990, acc: 0.9262
loss: 0.1994, acc: 0.9260
loss: 0.1990, acc: 0.9259
loss: 0.1988, acc: 0.9256
loss: 0.1991, acc: 0.9256
loss: 0.2003, acc: 0.9249
loss: 0.2002, acc: 0.9251
loss: 0.2012, acc: 0.9244
loss: 0.2009, acc: 0.9244
loss: 0.2008, acc: 0.9248
loss: 0.2000, acc: 0.9248
loss: 0.2013, acc: 0.9243
loss: 0.2016, acc: 0.9243
loss: 0.2030, acc: 0.9240
loss: 0.2045, acc: 0.9233
loss: 0.2057, acc: 0.9227
loss: 0.2079, acc: 0.9219
loss: 0.2111, acc: 0.9209
loss: 0.2112, acc: 0.9207
loss: 0.2125, acc: 0.9204
loss: 0.2124, acc: 0.9206
loss: 0.2129, acc: 0.9202
loss: 0.2133, acc: 0.9202
> val_acc: 0.8183, val_f1: 0.8117
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 16
loss: 0.1695, acc: 0.9313
loss: 0.1497, acc: 0.9500
loss: 0.1440, acc: 0.9417
loss: 0.1456, acc: 0.9406
loss: 0.1598, acc: 0.9400
loss: 0.1724, acc: 0.9344
loss: 0.1857, acc: 0.9313
loss: 0.1910, acc: 0.9305
loss: 0.1873, acc: 0.9319
loss: 0.1863, acc: 0.9331
loss: 0.1897, acc: 0.9318
loss: 0.1932, acc: 0.9307
loss: 0.1917, acc: 0.9303
loss: 0.1885, acc: 0.9317
loss: 0.1829, acc: 0.9329
loss: 0.1778, acc: 0.9344
loss: 0.1829, acc: 0.9335
loss: 0.1829, acc: 0.9333
loss: 0.1835, acc: 0.9326
loss: 0.1835, acc: 0.9319
loss: 0.1902, acc: 0.9298
loss: 0.1923, acc: 0.9276
loss: 0.1961, acc: 0.9258
loss: 0.1963, acc: 0.9255
loss: 0.1973, acc: 0.9255
loss: 0.1996, acc: 0.9240
loss: 0.2003, acc: 0.9236
loss: 0.1990, acc: 0.9239
loss: 0.2001, acc: 0.9235
loss: 0.2029, acc: 0.9215
loss: 0.2062, acc: 0.9208
loss: 0.2084, acc: 0.9193
loss: 0.2079, acc: 0.9193
loss: 0.2073, acc: 0.9193
loss: 0.2064, acc: 0.9196
loss: 0.2056, acc: 0.9198
loss: 0.2067, acc: 0.9191
loss: 0.2067, acc: 0.9199
loss: 0.2062, acc: 0.9202
loss: 0.2065, acc: 0.9195
loss: 0.2046, acc: 0.9198
loss: 0.2038, acc: 0.9201
loss: 0.2037, acc: 0.9205
loss: 0.2043, acc: 0.9203
loss: 0.2043, acc: 0.9203
loss: 0.2036, acc: 0.9208
loss: 0.2023, acc: 0.9217
loss: 0.2037, acc: 0.9212
loss: 0.2031, acc: 0.9216
loss: 0.2039, acc: 0.9213
loss: 0.2045, acc: 0.9211
loss: 0.2046, acc: 0.9207
loss: 0.2046, acc: 0.9205
loss: 0.2044, acc: 0.9206
loss: 0.2039, acc: 0.9207
loss: 0.2040, acc: 0.9210
loss: 0.2049, acc: 0.9207
loss: 0.2050, acc: 0.9210
loss: 0.2057, acc: 0.9206
loss: 0.2045, acc: 0.9211
loss: 0.2047, acc: 0.9212
loss: 0.2050, acc: 0.9212
loss: 0.2047, acc: 0.9216
loss: 0.2053, acc: 0.9215
loss: 0.2052, acc: 0.9213
loss: 0.2053, acc: 0.9211
loss: 0.2071, acc: 0.9206
loss: 0.2069, acc: 0.9209
loss: 0.2081, acc: 0.9201
loss: 0.2102, acc: 0.9197
> val_acc: 0.8243, val_f1: 0.8183
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 17
loss: 0.1894, acc: 0.9375
loss: 0.1724, acc: 0.9437
loss: 0.1588, acc: 0.9417
loss: 0.1390, acc: 0.9500
loss: 0.1341, acc: 0.9500
loss: 0.1547, acc: 0.9437
loss: 0.1466, acc: 0.9473
loss: 0.1479, acc: 0.9461
loss: 0.1475, acc: 0.9458
loss: 0.1481, acc: 0.9463
loss: 0.1535, acc: 0.9443
loss: 0.1654, acc: 0.9411
loss: 0.1706, acc: 0.9404
loss: 0.1700, acc: 0.9393
loss: 0.1752, acc: 0.9383
loss: 0.1729, acc: 0.9387
loss: 0.1783, acc: 0.9371
loss: 0.1755, acc: 0.9382
loss: 0.1775, acc: 0.9368
loss: 0.1798, acc: 0.9347
loss: 0.1845, acc: 0.9321
loss: 0.1832, acc: 0.9327
loss: 0.1805, acc: 0.9340
loss: 0.1842, acc: 0.9326
loss: 0.1823, acc: 0.9337
loss: 0.1825, acc: 0.9334
loss: 0.1807, acc: 0.9345
loss: 0.1810, acc: 0.9342
loss: 0.1854, acc: 0.9328
loss: 0.1877, acc: 0.9317
loss: 0.1875, acc: 0.9317
loss: 0.1891, acc: 0.9309
loss: 0.1892, acc: 0.9314
loss: 0.1906, acc: 0.9309
loss: 0.1907, acc: 0.9304
loss: 0.1915, acc: 0.9300
loss: 0.1900, acc: 0.9304
loss: 0.1917, acc: 0.9296
loss: 0.1935, acc: 0.9292
loss: 0.1925, acc: 0.9297
loss: 0.1944, acc: 0.9285
loss: 0.1957, acc: 0.9281
loss: 0.1969, acc: 0.9275
loss: 0.1982, acc: 0.9264
loss: 0.1982, acc: 0.9265
loss: 0.1984, acc: 0.9265
loss: 0.2013, acc: 0.9255
loss: 0.2034, acc: 0.9251
loss: 0.2033, acc: 0.9250
loss: 0.2015, acc: 0.9259
loss: 0.2013, acc: 0.9256
loss: 0.2044, acc: 0.9248
loss: 0.2062, acc: 0.9243
loss: 0.2070, acc: 0.9241
loss: 0.2077, acc: 0.9236
loss: 0.2074, acc: 0.9238
loss: 0.2073, acc: 0.9236
loss: 0.2068, acc: 0.9239
loss: 0.2071, acc: 0.9236
loss: 0.2076, acc: 0.9235
loss: 0.2083, acc: 0.9233
loss: 0.2084, acc: 0.9231
loss: 0.2101, acc: 0.9223
loss: 0.2103, acc: 0.9223
loss: 0.2109, acc: 0.9218
loss: 0.2119, acc: 0.9216
loss: 0.2127, acc: 0.9211
loss: 0.2129, acc: 0.9212
loss: 0.2120, acc: 0.9216
loss: 0.2122, acc: 0.9212
> val_acc: 0.8288, val_f1: 0.8220
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8159, test_f1: 0.8062
>> test_acc: 0.8323, test_f1: 0.8258
>> test_acc: 0.8249, test_f1: 0.8183
>> test_acc: 0.8159, test_f1: 0.8062

>> avg_test_acc: 0.8244, avg_test_f1: 0.8167
>> max_test_acc: 0.8323, max_test_f1: 0.8258
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 504626176
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1528, acc: 0.3937
loss: 1.1307, acc: 0.3688
loss: 1.1141, acc: 0.3875
loss: 1.0971, acc: 0.3953
loss: 1.0915, acc: 0.3975
loss: 1.0739, acc: 0.4135
loss: 1.0576, acc: 0.4223
loss: 1.0247, acc: 0.4453
loss: 1.0035, acc: 0.4674
loss: 0.9882, acc: 0.4763
loss: 0.9760, acc: 0.4858
loss: 0.9578, acc: 0.5062
loss: 0.9450, acc: 0.5178
loss: 0.9403, acc: 0.5277
loss: 0.9225, acc: 0.5408
loss: 0.9094, acc: 0.5496
loss: 0.8986, acc: 0.5581
loss: 0.8888, acc: 0.5646
loss: 0.8742, acc: 0.5720
loss: 0.8686, acc: 0.5766
loss: 0.8642, acc: 0.5807
loss: 0.8588, acc: 0.5847
loss: 0.8519, acc: 0.5913
loss: 0.8391, acc: 0.6005
loss: 0.8311, acc: 0.6062
loss: 0.8234, acc: 0.6120
loss: 0.8182, acc: 0.6167
loss: 0.8099, acc: 0.6230
loss: 0.8001, acc: 0.6293
loss: 0.7980, acc: 0.6321
loss: 0.7944, acc: 0.6347
loss: 0.7902, acc: 0.6381
loss: 0.7814, acc: 0.6436
loss: 0.7736, acc: 0.6482
loss: 0.7692, acc: 0.6514
loss: 0.7637, acc: 0.6547
loss: 0.7583, acc: 0.6583
loss: 0.7541, acc: 0.6607
loss: 0.7489, acc: 0.6635
loss: 0.7455, acc: 0.6652
loss: 0.7421, acc: 0.6672
loss: 0.7384, acc: 0.6699
loss: 0.7366, acc: 0.6715
loss: 0.7333, acc: 0.6727
loss: 0.7300, acc: 0.6740
loss: 0.7297, acc: 0.6745
loss: 0.7261, acc: 0.6765
loss: 0.7217, acc: 0.6788
loss: 0.7195, acc: 0.6801
loss: 0.7182, acc: 0.6811
loss: 0.7161, acc: 0.6828
loss: 0.7121, acc: 0.6850
loss: 0.7093, acc: 0.6867
loss: 0.7065, acc: 0.6876
loss: 0.7040, acc: 0.6887
loss: 0.7013, acc: 0.6904
loss: 0.6999, acc: 0.6917
loss: 0.6972, acc: 0.6937
loss: 0.6948, acc: 0.6944
loss: 0.6930, acc: 0.6954
loss: 0.6906, acc: 0.6965
loss: 0.6913, acc: 0.6970
loss: 0.6890, acc: 0.6984
loss: 0.6850, acc: 0.7004
loss: 0.6823, acc: 0.7020
loss: 0.6803, acc: 0.7031
loss: 0.6790, acc: 0.7037
loss: 0.6779, acc: 0.7051
loss: 0.6763, acc: 0.7066
loss: 0.6748, acc: 0.7073
> val_acc: 0.7913, val_f1: 0.7805
>> saved: peft/roberta_lora/mams//acc_0.7913_f1_0.7805_230828-0915
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5028, acc: 0.7875
loss: 0.5154, acc: 0.8000
loss: 0.4901, acc: 0.8063
loss: 0.4640, acc: 0.8172
loss: 0.4938, acc: 0.8075
loss: 0.5049, acc: 0.8031
loss: 0.4785, acc: 0.8161
loss: 0.4926, acc: 0.8094
loss: 0.4936, acc: 0.8069
loss: 0.5059, acc: 0.7994
loss: 0.5117, acc: 0.7989
loss: 0.5122, acc: 0.7990
loss: 0.5064, acc: 0.8005
loss: 0.5101, acc: 0.7960
loss: 0.5034, acc: 0.7983
loss: 0.5038, acc: 0.7977
loss: 0.5017, acc: 0.7967
loss: 0.5041, acc: 0.7958
loss: 0.5008, acc: 0.7964
loss: 0.4960, acc: 0.7997
loss: 0.4974, acc: 0.7985
loss: 0.4933, acc: 0.8009
loss: 0.5002, acc: 0.7989
loss: 0.4981, acc: 0.8003
loss: 0.5004, acc: 0.8005
loss: 0.5006, acc: 0.8007
loss: 0.4978, acc: 0.8021
loss: 0.5020, acc: 0.7984
loss: 0.5045, acc: 0.7976
loss: 0.5019, acc: 0.7992
loss: 0.5033, acc: 0.7984
loss: 0.5024, acc: 0.7977
loss: 0.5065, acc: 0.7953
loss: 0.5092, acc: 0.7943
loss: 0.5081, acc: 0.7950
loss: 0.5086, acc: 0.7946
loss: 0.5114, acc: 0.7929
loss: 0.5131, acc: 0.7926
loss: 0.5120, acc: 0.7928
loss: 0.5120, acc: 0.7923
loss: 0.5133, acc: 0.7918
loss: 0.5124, acc: 0.7921
loss: 0.5110, acc: 0.7930
loss: 0.5098, acc: 0.7939
loss: 0.5087, acc: 0.7940
loss: 0.5070, acc: 0.7946
loss: 0.5055, acc: 0.7960
loss: 0.5054, acc: 0.7964
loss: 0.5065, acc: 0.7960
loss: 0.5053, acc: 0.7963
loss: 0.5052, acc: 0.7964
loss: 0.5071, acc: 0.7960
loss: 0.5069, acc: 0.7965
loss: 0.5050, acc: 0.7972
loss: 0.5050, acc: 0.7974
loss: 0.5042, acc: 0.7977
loss: 0.5042, acc: 0.7980
loss: 0.5035, acc: 0.7984
loss: 0.5037, acc: 0.7986
loss: 0.5031, acc: 0.7990
loss: 0.5002, acc: 0.8001
loss: 0.5004, acc: 0.8003
loss: 0.5014, acc: 0.8002
loss: 0.5018, acc: 0.7994
loss: 0.5024, acc: 0.7998
loss: 0.5031, acc: 0.7991
loss: 0.5023, acc: 0.7994
loss: 0.5023, acc: 0.7994
loss: 0.5007, acc: 0.8005
loss: 0.5018, acc: 0.8009
> val_acc: 0.8056, val_f1: 0.7973
>> saved: peft/roberta_lora/mams//acc_0.8056_f1_0.7973_230828-0916
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4150, acc: 0.8438
loss: 0.4658, acc: 0.8125
loss: 0.4562, acc: 0.8208
loss: 0.4369, acc: 0.8281
loss: 0.4377, acc: 0.8300
loss: 0.4474, acc: 0.8260
loss: 0.4552, acc: 0.8241
loss: 0.4421, acc: 0.8313
loss: 0.4314, acc: 0.8354
loss: 0.4521, acc: 0.8269
loss: 0.4555, acc: 0.8261
loss: 0.4651, acc: 0.8219
loss: 0.4579, acc: 0.8255
loss: 0.4561, acc: 0.8250
loss: 0.4577, acc: 0.8263
loss: 0.4534, acc: 0.8289
loss: 0.4563, acc: 0.8265
loss: 0.4553, acc: 0.8278
loss: 0.4545, acc: 0.8273
loss: 0.4526, acc: 0.8281
loss: 0.4557, acc: 0.8274
loss: 0.4578, acc: 0.8256
loss: 0.4566, acc: 0.8264
loss: 0.4553, acc: 0.8266
loss: 0.4563, acc: 0.8260
loss: 0.4542, acc: 0.8267
loss: 0.4574, acc: 0.8250
loss: 0.4600, acc: 0.8232
loss: 0.4591, acc: 0.8237
loss: 0.4586, acc: 0.8237
loss: 0.4610, acc: 0.8224
loss: 0.4571, acc: 0.8238
loss: 0.4583, acc: 0.8233
loss: 0.4573, acc: 0.8232
loss: 0.4609, acc: 0.8213
loss: 0.4633, acc: 0.8205
loss: 0.4614, acc: 0.8216
loss: 0.4583, acc: 0.8230
loss: 0.4575, acc: 0.8234
loss: 0.4564, acc: 0.8239
loss: 0.4579, acc: 0.8229
loss: 0.4582, acc: 0.8229
loss: 0.4568, acc: 0.8235
loss: 0.4565, acc: 0.8244
loss: 0.4559, acc: 0.8250
loss: 0.4570, acc: 0.8243
loss: 0.4568, acc: 0.8250
loss: 0.4545, acc: 0.8255
loss: 0.4569, acc: 0.8246
loss: 0.4569, acc: 0.8247
loss: 0.4552, acc: 0.8257
loss: 0.4549, acc: 0.8257
loss: 0.4577, acc: 0.8250
loss: 0.4563, acc: 0.8262
loss: 0.4585, acc: 0.8245
loss: 0.4573, acc: 0.8252
loss: 0.4559, acc: 0.8257
loss: 0.4566, acc: 0.8255
loss: 0.4570, acc: 0.8252
loss: 0.4564, acc: 0.8260
loss: 0.4568, acc: 0.8259
loss: 0.4583, acc: 0.8252
loss: 0.4587, acc: 0.8251
loss: 0.4586, acc: 0.8251
loss: 0.4591, acc: 0.8249
loss: 0.4597, acc: 0.8245
loss: 0.4600, acc: 0.8242
loss: 0.4620, acc: 0.8236
loss: 0.4626, acc: 0.8232
loss: 0.4625, acc: 0.8232
> val_acc: 0.8101, val_f1: 0.8005
>> saved: peft/roberta_lora/mams//acc_0.8101_f1_0.8005_230828-0918
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4137, acc: 0.8750
loss: 0.3787, acc: 0.8594
loss: 0.3950, acc: 0.8521
loss: 0.3810, acc: 0.8516
loss: 0.4049, acc: 0.8337
loss: 0.4146, acc: 0.8271
loss: 0.4143, acc: 0.8286
loss: 0.4403, acc: 0.8219
loss: 0.4284, acc: 0.8292
loss: 0.4248, acc: 0.8294
loss: 0.4342, acc: 0.8301
loss: 0.4339, acc: 0.8313
loss: 0.4313, acc: 0.8337
loss: 0.4290, acc: 0.8326
loss: 0.4255, acc: 0.8354
loss: 0.4267, acc: 0.8340
loss: 0.4274, acc: 0.8335
loss: 0.4288, acc: 0.8319
loss: 0.4266, acc: 0.8332
loss: 0.4265, acc: 0.8344
loss: 0.4286, acc: 0.8324
loss: 0.4319, acc: 0.8313
loss: 0.4355, acc: 0.8302
loss: 0.4348, acc: 0.8310
loss: 0.4342, acc: 0.8315
loss: 0.4311, acc: 0.8317
loss: 0.4311, acc: 0.8310
loss: 0.4319, acc: 0.8310
loss: 0.4310, acc: 0.8325
loss: 0.4321, acc: 0.8327
loss: 0.4317, acc: 0.8325
loss: 0.4301, acc: 0.8328
loss: 0.4297, acc: 0.8328
loss: 0.4270, acc: 0.8331
loss: 0.4254, acc: 0.8327
loss: 0.4238, acc: 0.8337
loss: 0.4239, acc: 0.8333
loss: 0.4250, acc: 0.8324
loss: 0.4269, acc: 0.8322
loss: 0.4276, acc: 0.8311
loss: 0.4282, acc: 0.8311
loss: 0.4278, acc: 0.8321
loss: 0.4266, acc: 0.8323
loss: 0.4281, acc: 0.8322
loss: 0.4286, acc: 0.8328
loss: 0.4285, acc: 0.8326
loss: 0.4293, acc: 0.8320
loss: 0.4291, acc: 0.8320
loss: 0.4303, acc: 0.8316
loss: 0.4306, acc: 0.8313
loss: 0.4332, acc: 0.8299
loss: 0.4329, acc: 0.8303
loss: 0.4322, acc: 0.8305
loss: 0.4335, acc: 0.8296
loss: 0.4357, acc: 0.8292
loss: 0.4354, acc: 0.8297
loss: 0.4358, acc: 0.8300
loss: 0.4381, acc: 0.8291
loss: 0.4384, acc: 0.8293
loss: 0.4391, acc: 0.8285
loss: 0.4384, acc: 0.8287
loss: 0.4376, acc: 0.8287
loss: 0.4399, acc: 0.8280
loss: 0.4405, acc: 0.8275
loss: 0.4404, acc: 0.8276
loss: 0.4401, acc: 0.8275
loss: 0.4404, acc: 0.8272
loss: 0.4408, acc: 0.8271
loss: 0.4420, acc: 0.8262
loss: 0.4422, acc: 0.8257
> val_acc: 0.8146, val_f1: 0.8042
>> saved: peft/roberta_lora/mams//acc_0.8146_f1_0.8042_230828-0919
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4675, acc: 0.8688
loss: 0.4187, acc: 0.8750
loss: 0.4364, acc: 0.8562
loss: 0.4334, acc: 0.8547
loss: 0.4404, acc: 0.8538
loss: 0.4423, acc: 0.8500
loss: 0.4379, acc: 0.8500
loss: 0.4298, acc: 0.8531
loss: 0.4314, acc: 0.8528
loss: 0.4349, acc: 0.8500
loss: 0.4367, acc: 0.8455
loss: 0.4273, acc: 0.8490
loss: 0.4222, acc: 0.8505
loss: 0.4240, acc: 0.8496
loss: 0.4218, acc: 0.8504
loss: 0.4234, acc: 0.8500
loss: 0.4230, acc: 0.8485
loss: 0.4151, acc: 0.8517
loss: 0.4206, acc: 0.8503
loss: 0.4226, acc: 0.8491
loss: 0.4241, acc: 0.8458
loss: 0.4289, acc: 0.8438
loss: 0.4319, acc: 0.8408
loss: 0.4314, acc: 0.8401
loss: 0.4315, acc: 0.8395
loss: 0.4288, acc: 0.8401
loss: 0.4301, acc: 0.8394
loss: 0.4292, acc: 0.8384
loss: 0.4284, acc: 0.8388
loss: 0.4271, acc: 0.8392
loss: 0.4263, acc: 0.8395
loss: 0.4268, acc: 0.8391
loss: 0.4246, acc: 0.8403
loss: 0.4241, acc: 0.8406
loss: 0.4228, acc: 0.8414
loss: 0.4216, acc: 0.8410
loss: 0.4214, acc: 0.8409
loss: 0.4231, acc: 0.8403
loss: 0.4219, acc: 0.8409
loss: 0.4233, acc: 0.8402
loss: 0.4250, acc: 0.8393
loss: 0.4211, acc: 0.8412
loss: 0.4219, acc: 0.8416
loss: 0.4245, acc: 0.8408
loss: 0.4267, acc: 0.8397
loss: 0.4259, acc: 0.8398
loss: 0.4241, acc: 0.8402
loss: 0.4255, acc: 0.8402
loss: 0.4263, acc: 0.8401
loss: 0.4245, acc: 0.8405
loss: 0.4227, acc: 0.8406
loss: 0.4239, acc: 0.8398
loss: 0.4254, acc: 0.8397
loss: 0.4235, acc: 0.8399
loss: 0.4225, acc: 0.8400
loss: 0.4236, acc: 0.8396
loss: 0.4232, acc: 0.8397
loss: 0.4226, acc: 0.8402
loss: 0.4232, acc: 0.8398
loss: 0.4228, acc: 0.8402
loss: 0.4222, acc: 0.8402
loss: 0.4236, acc: 0.8395
loss: 0.4243, acc: 0.8396
loss: 0.4243, acc: 0.8395
loss: 0.4255, acc: 0.8385
loss: 0.4243, acc: 0.8391
loss: 0.4256, acc: 0.8386
loss: 0.4250, acc: 0.8387
loss: 0.4245, acc: 0.8391
loss: 0.4249, acc: 0.8386
> val_acc: 0.8206, val_f1: 0.8137
>> saved: peft/roberta_lora/mams//acc_0.8206_f1_0.8137_230828-0921
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3878, acc: 0.8250
loss: 0.3787, acc: 0.8469
loss: 0.3703, acc: 0.8521
loss: 0.3636, acc: 0.8594
loss: 0.3582, acc: 0.8588
loss: 0.3614, acc: 0.8562
loss: 0.3817, acc: 0.8527
loss: 0.3819, acc: 0.8562
loss: 0.3756, acc: 0.8597
loss: 0.3657, acc: 0.8631
loss: 0.3568, acc: 0.8642
loss: 0.3618, acc: 0.8620
loss: 0.3647, acc: 0.8596
loss: 0.3743, acc: 0.8567
loss: 0.3747, acc: 0.8562
loss: 0.3745, acc: 0.8574
loss: 0.3823, acc: 0.8533
loss: 0.3812, acc: 0.8552
loss: 0.3828, acc: 0.8539
loss: 0.3834, acc: 0.8528
loss: 0.3927, acc: 0.8485
loss: 0.3896, acc: 0.8500
loss: 0.3940, acc: 0.8484
loss: 0.3967, acc: 0.8474
loss: 0.3984, acc: 0.8452
loss: 0.4009, acc: 0.8452
loss: 0.4013, acc: 0.8456
loss: 0.3984, acc: 0.8473
loss: 0.3999, acc: 0.8470
loss: 0.4004, acc: 0.8469
loss: 0.4051, acc: 0.8448
loss: 0.4043, acc: 0.8449
loss: 0.4032, acc: 0.8451
loss: 0.4074, acc: 0.8428
loss: 0.4081, acc: 0.8420
loss: 0.4050, acc: 0.8436
loss: 0.4042, acc: 0.8441
loss: 0.4038, acc: 0.8449
loss: 0.4032, acc: 0.8452
loss: 0.4013, acc: 0.8461
loss: 0.4017, acc: 0.8462
loss: 0.4014, acc: 0.8461
loss: 0.3998, acc: 0.8471
loss: 0.4001, acc: 0.8466
loss: 0.3980, acc: 0.8475
loss: 0.4009, acc: 0.8469
loss: 0.4018, acc: 0.8465
loss: 0.4011, acc: 0.8470
loss: 0.4042, acc: 0.8457
loss: 0.4068, acc: 0.8444
loss: 0.4065, acc: 0.8440
loss: 0.4060, acc: 0.8439
loss: 0.4073, acc: 0.8434
loss: 0.4058, acc: 0.8436
loss: 0.4056, acc: 0.8438
loss: 0.4055, acc: 0.8436
loss: 0.4072, acc: 0.8425
loss: 0.4076, acc: 0.8418
loss: 0.4078, acc: 0.8421
loss: 0.4066, acc: 0.8423
loss: 0.4044, acc: 0.8433
loss: 0.4059, acc: 0.8432
loss: 0.4067, acc: 0.8433
loss: 0.4079, acc: 0.8424
loss: 0.4072, acc: 0.8429
loss: 0.4068, acc: 0.8428
loss: 0.4067, acc: 0.8432
loss: 0.4081, acc: 0.8425
loss: 0.4075, acc: 0.8428
loss: 0.4074, acc: 0.8428
> val_acc: 0.7965, val_f1: 0.7919
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.3360, acc: 0.8688
loss: 0.3145, acc: 0.8656
loss: 0.3559, acc: 0.8500
loss: 0.3600, acc: 0.8562
loss: 0.3733, acc: 0.8538
loss: 0.3799, acc: 0.8490
loss: 0.3756, acc: 0.8473
loss: 0.3662, acc: 0.8531
loss: 0.3622, acc: 0.8576
loss: 0.3570, acc: 0.8606
loss: 0.3533, acc: 0.8602
loss: 0.3542, acc: 0.8604
loss: 0.3520, acc: 0.8630
loss: 0.3475, acc: 0.8643
loss: 0.3476, acc: 0.8658
loss: 0.3497, acc: 0.8648
loss: 0.3553, acc: 0.8614
loss: 0.3593, acc: 0.8601
loss: 0.3563, acc: 0.8612
loss: 0.3545, acc: 0.8612
loss: 0.3553, acc: 0.8613
loss: 0.3586, acc: 0.8602
loss: 0.3606, acc: 0.8592
loss: 0.3602, acc: 0.8591
loss: 0.3622, acc: 0.8578
loss: 0.3623, acc: 0.8587
loss: 0.3638, acc: 0.8586
loss: 0.3649, acc: 0.8580
loss: 0.3633, acc: 0.8586
loss: 0.3620, acc: 0.8600
loss: 0.3629, acc: 0.8599
loss: 0.3634, acc: 0.8594
loss: 0.3665, acc: 0.8581
loss: 0.3673, acc: 0.8575
loss: 0.3691, acc: 0.8557
loss: 0.3680, acc: 0.8559
loss: 0.3669, acc: 0.8566
loss: 0.3684, acc: 0.8551
loss: 0.3687, acc: 0.8550
loss: 0.3683, acc: 0.8552
loss: 0.3704, acc: 0.8547
loss: 0.3703, acc: 0.8549
loss: 0.3711, acc: 0.8541
loss: 0.3688, acc: 0.8553
loss: 0.3671, acc: 0.8557
loss: 0.3664, acc: 0.8562
loss: 0.3669, acc: 0.8560
loss: 0.3690, acc: 0.8552
loss: 0.3692, acc: 0.8548
loss: 0.3706, acc: 0.8542
loss: 0.3717, acc: 0.8537
loss: 0.3719, acc: 0.8541
loss: 0.3711, acc: 0.8547
loss: 0.3694, acc: 0.8557
loss: 0.3703, acc: 0.8555
loss: 0.3701, acc: 0.8557
loss: 0.3692, acc: 0.8566
loss: 0.3691, acc: 0.8565
loss: 0.3700, acc: 0.8565
loss: 0.3702, acc: 0.8566
loss: 0.3708, acc: 0.8566
loss: 0.3705, acc: 0.8570
loss: 0.3703, acc: 0.8572
loss: 0.3700, acc: 0.8573
loss: 0.3709, acc: 0.8572
loss: 0.3702, acc: 0.8574
loss: 0.3689, acc: 0.8577
loss: 0.3683, acc: 0.8583
loss: 0.3678, acc: 0.8589
loss: 0.3679, acc: 0.8587
> val_acc: 0.8161, val_f1: 0.8132
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3589, acc: 0.8625
loss: 0.3643, acc: 0.8688
loss: 0.3842, acc: 0.8583
loss: 0.3459, acc: 0.8734
loss: 0.3729, acc: 0.8588
loss: 0.3583, acc: 0.8646
loss: 0.3670, acc: 0.8652
loss: 0.3587, acc: 0.8680
loss: 0.3506, acc: 0.8708
loss: 0.3438, acc: 0.8731
loss: 0.3434, acc: 0.8744
loss: 0.3551, acc: 0.8714
loss: 0.3544, acc: 0.8716
loss: 0.3584, acc: 0.8710
loss: 0.3554, acc: 0.8717
loss: 0.3577, acc: 0.8703
loss: 0.3548, acc: 0.8710
loss: 0.3544, acc: 0.8712
loss: 0.3520, acc: 0.8714
loss: 0.3524, acc: 0.8697
loss: 0.3536, acc: 0.8679
loss: 0.3525, acc: 0.8676
loss: 0.3528, acc: 0.8663
loss: 0.3542, acc: 0.8661
loss: 0.3576, acc: 0.8638
loss: 0.3539, acc: 0.8656
loss: 0.3579, acc: 0.8639
loss: 0.3603, acc: 0.8621
loss: 0.3574, acc: 0.8629
loss: 0.3562, acc: 0.8633
loss: 0.3527, acc: 0.8641
loss: 0.3535, acc: 0.8639
loss: 0.3550, acc: 0.8627
loss: 0.3557, acc: 0.8618
loss: 0.3581, acc: 0.8609
loss: 0.3568, acc: 0.8623
loss: 0.3550, acc: 0.8633
loss: 0.3582, acc: 0.8628
loss: 0.3570, acc: 0.8636
loss: 0.3583, acc: 0.8631
loss: 0.3579, acc: 0.8628
loss: 0.3570, acc: 0.8632
loss: 0.3573, acc: 0.8629
loss: 0.3582, acc: 0.8628
loss: 0.3567, acc: 0.8640
loss: 0.3547, acc: 0.8648
loss: 0.3528, acc: 0.8653
loss: 0.3555, acc: 0.8647
loss: 0.3546, acc: 0.8654
loss: 0.3533, acc: 0.8660
loss: 0.3522, acc: 0.8668
loss: 0.3526, acc: 0.8667
loss: 0.3544, acc: 0.8660
loss: 0.3526, acc: 0.8670
loss: 0.3519, acc: 0.8668
loss: 0.3517, acc: 0.8670
loss: 0.3521, acc: 0.8671
loss: 0.3520, acc: 0.8673
loss: 0.3535, acc: 0.8671
loss: 0.3542, acc: 0.8669
loss: 0.3542, acc: 0.8667
loss: 0.3552, acc: 0.8658
loss: 0.3567, acc: 0.8651
loss: 0.3569, acc: 0.8646
loss: 0.3567, acc: 0.8648
loss: 0.3573, acc: 0.8648
loss: 0.3585, acc: 0.8645
loss: 0.3589, acc: 0.8645
loss: 0.3606, acc: 0.8639
loss: 0.3619, acc: 0.8637
> val_acc: 0.8018, val_f1: 0.8009
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3725, acc: 0.8688
loss: 0.3969, acc: 0.8625
loss: 0.3444, acc: 0.8812
loss: 0.3201, acc: 0.8891
loss: 0.3373, acc: 0.8800
loss: 0.3396, acc: 0.8823
loss: 0.3356, acc: 0.8839
loss: 0.3342, acc: 0.8844
loss: 0.3315, acc: 0.8840
loss: 0.3343, acc: 0.8812
loss: 0.3350, acc: 0.8790
loss: 0.3407, acc: 0.8750
loss: 0.3440, acc: 0.8750
loss: 0.3429, acc: 0.8750
loss: 0.3453, acc: 0.8733
loss: 0.3462, acc: 0.8750
loss: 0.3414, acc: 0.8765
loss: 0.3382, acc: 0.8785
loss: 0.3384, acc: 0.8783
loss: 0.3383, acc: 0.8788
loss: 0.3408, acc: 0.8768
loss: 0.3390, acc: 0.8781
loss: 0.3429, acc: 0.8777
loss: 0.3453, acc: 0.8758
loss: 0.3464, acc: 0.8748
loss: 0.3491, acc: 0.8743
loss: 0.3515, acc: 0.8734
loss: 0.3525, acc: 0.8719
loss: 0.3528, acc: 0.8716
loss: 0.3546, acc: 0.8706
loss: 0.3533, acc: 0.8710
loss: 0.3544, acc: 0.8701
loss: 0.3532, acc: 0.8701
loss: 0.3554, acc: 0.8700
loss: 0.3568, acc: 0.8693
loss: 0.3584, acc: 0.8679
loss: 0.3587, acc: 0.8684
loss: 0.3622, acc: 0.8666
loss: 0.3634, acc: 0.8667
loss: 0.3652, acc: 0.8653
loss: 0.3650, acc: 0.8652
loss: 0.3660, acc: 0.8647
loss: 0.3651, acc: 0.8654
loss: 0.3655, acc: 0.8655
loss: 0.3675, acc: 0.8650
loss: 0.3695, acc: 0.8643
loss: 0.3700, acc: 0.8642
loss: 0.3683, acc: 0.8646
loss: 0.3678, acc: 0.8648
loss: 0.3674, acc: 0.8644
loss: 0.3693, acc: 0.8634
loss: 0.3683, acc: 0.8635
loss: 0.3679, acc: 0.8636
loss: 0.3684, acc: 0.8635
loss: 0.3673, acc: 0.8633
loss: 0.3686, acc: 0.8628
loss: 0.3698, acc: 0.8618
loss: 0.3699, acc: 0.8621
loss: 0.3702, acc: 0.8621
loss: 0.3715, acc: 0.8616
loss: 0.3731, acc: 0.8610
loss: 0.3729, acc: 0.8612
loss: 0.3713, acc: 0.8623
loss: 0.3712, acc: 0.8622
loss: 0.3720, acc: 0.8622
loss: 0.3709, acc: 0.8628
loss: 0.3710, acc: 0.8624
loss: 0.3718, acc: 0.8620
loss: 0.3730, acc: 0.8612
loss: 0.3734, acc: 0.8609
> val_acc: 0.8116, val_f1: 0.8096
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3603, acc: 0.8375
loss: 0.3734, acc: 0.8594
loss: 0.3272, acc: 0.8729
loss: 0.3109, acc: 0.8812
loss: 0.2942, acc: 0.8850
loss: 0.2868, acc: 0.8906
loss: 0.3050, acc: 0.8839
loss: 0.3048, acc: 0.8812
loss: 0.3161, acc: 0.8764
loss: 0.3234, acc: 0.8756
loss: 0.3211, acc: 0.8773
loss: 0.3139, acc: 0.8812
loss: 0.3191, acc: 0.8812
loss: 0.3115, acc: 0.8835
loss: 0.3161, acc: 0.8804
loss: 0.3180, acc: 0.8789
loss: 0.3175, acc: 0.8794
loss: 0.3172, acc: 0.8802
loss: 0.3161, acc: 0.8812
loss: 0.3131, acc: 0.8828
loss: 0.3141, acc: 0.8818
loss: 0.3127, acc: 0.8832
loss: 0.3127, acc: 0.8842
loss: 0.3165, acc: 0.8839
loss: 0.3222, acc: 0.8812
loss: 0.3269, acc: 0.8805
loss: 0.3242, acc: 0.8817
loss: 0.3238, acc: 0.8821
loss: 0.3208, acc: 0.8828
loss: 0.3219, acc: 0.8827
loss: 0.3246, acc: 0.8825
loss: 0.3273, acc: 0.8811
loss: 0.3279, acc: 0.8812
loss: 0.3269, acc: 0.8807
loss: 0.3288, acc: 0.8804
loss: 0.3279, acc: 0.8809
loss: 0.3299, acc: 0.8804
loss: 0.3294, acc: 0.8806
loss: 0.3311, acc: 0.8801
loss: 0.3297, acc: 0.8800
loss: 0.3315, acc: 0.8791
loss: 0.3345, acc: 0.8781
loss: 0.3334, acc: 0.8786
loss: 0.3328, acc: 0.8786
loss: 0.3327, acc: 0.8785
loss: 0.3337, acc: 0.8776
loss: 0.3354, acc: 0.8766
loss: 0.3360, acc: 0.8764
loss: 0.3349, acc: 0.8764
loss: 0.3333, acc: 0.8770
loss: 0.3331, acc: 0.8772
loss: 0.3348, acc: 0.8764
loss: 0.3343, acc: 0.8768
loss: 0.3335, acc: 0.8766
loss: 0.3333, acc: 0.8769
loss: 0.3331, acc: 0.8763
loss: 0.3319, acc: 0.8766
loss: 0.3349, acc: 0.8755
loss: 0.3358, acc: 0.8750
loss: 0.3361, acc: 0.8747
loss: 0.3360, acc: 0.8748
loss: 0.3362, acc: 0.8747
loss: 0.3371, acc: 0.8744
loss: 0.3377, acc: 0.8742
loss: 0.3370, acc: 0.8748
loss: 0.3382, acc: 0.8744
loss: 0.3395, acc: 0.8738
loss: 0.3397, acc: 0.8735
loss: 0.3393, acc: 0.8735
loss: 0.3404, acc: 0.8727
> val_acc: 0.8101, val_f1: 0.8070
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8413, test_f1: 0.8357
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 560920064
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.0668, acc: 0.5312
loss: 1.1247, acc: 0.4594
loss: 1.1302, acc: 0.4396
loss: 1.1185, acc: 0.4203
loss: 1.1090, acc: 0.4163
loss: 1.0960, acc: 0.4188
loss: 1.0875, acc: 0.4214
loss: 1.0711, acc: 0.4313
loss: 1.0463, acc: 0.4500
loss: 1.0206, acc: 0.4719
loss: 1.0065, acc: 0.4864
loss: 0.9880, acc: 0.5031
loss: 0.9733, acc: 0.5139
loss: 0.9566, acc: 0.5259
loss: 0.9513, acc: 0.5321
loss: 0.9331, acc: 0.5449
loss: 0.9111, acc: 0.5574
loss: 0.9040, acc: 0.5632
loss: 0.8950, acc: 0.5714
loss: 0.8846, acc: 0.5787
loss: 0.8769, acc: 0.5848
loss: 0.8648, acc: 0.5918
loss: 0.8513, acc: 0.5981
loss: 0.8378, acc: 0.6049
loss: 0.8306, acc: 0.6102
loss: 0.8224, acc: 0.6147
loss: 0.8140, acc: 0.6204
loss: 0.8071, acc: 0.6248
loss: 0.8029, acc: 0.6259
loss: 0.7974, acc: 0.6306
loss: 0.7928, acc: 0.6331
loss: 0.7857, acc: 0.6373
loss: 0.7832, acc: 0.6390
loss: 0.7776, acc: 0.6430
loss: 0.7735, acc: 0.6446
loss: 0.7682, acc: 0.6476
loss: 0.7629, acc: 0.6510
loss: 0.7573, acc: 0.6546
loss: 0.7516, acc: 0.6582
loss: 0.7461, acc: 0.6613
loss: 0.7409, acc: 0.6645
loss: 0.7357, acc: 0.6670
loss: 0.7318, acc: 0.6693
loss: 0.7280, acc: 0.6720
loss: 0.7241, acc: 0.6754
loss: 0.7214, acc: 0.6769
loss: 0.7203, acc: 0.6775
loss: 0.7166, acc: 0.6797
loss: 0.7131, acc: 0.6815
loss: 0.7094, acc: 0.6840
loss: 0.7035, acc: 0.6869
loss: 0.7024, acc: 0.6875
loss: 0.6994, acc: 0.6893
loss: 0.6967, acc: 0.6903
loss: 0.6920, acc: 0.6928
loss: 0.6896, acc: 0.6949
loss: 0.6885, acc: 0.6954
loss: 0.6868, acc: 0.6966
loss: 0.6850, acc: 0.6975
loss: 0.6831, acc: 0.6986
loss: 0.6806, acc: 0.7001
loss: 0.6775, acc: 0.7020
loss: 0.6764, acc: 0.7026
loss: 0.6738, acc: 0.7046
loss: 0.6729, acc: 0.7051
loss: 0.6708, acc: 0.7059
loss: 0.6676, acc: 0.7076
loss: 0.6664, acc: 0.7085
loss: 0.6652, acc: 0.7089
loss: 0.6640, acc: 0.7099
> val_acc: 0.8018, val_f1: 0.7918
>> saved: peft/roberta_lora/mams//acc_0.8018_f1_0.7918_230828-0930
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5444, acc: 0.7812
loss: 0.5247, acc: 0.7906
loss: 0.5287, acc: 0.7979
loss: 0.5120, acc: 0.8078
loss: 0.4840, acc: 0.8213
loss: 0.4835, acc: 0.8167
loss: 0.5043, acc: 0.8063
loss: 0.5106, acc: 0.8023
loss: 0.5036, acc: 0.8000
loss: 0.5156, acc: 0.7937
loss: 0.5124, acc: 0.7966
loss: 0.5073, acc: 0.8005
loss: 0.5072, acc: 0.8014
loss: 0.5143, acc: 0.7982
loss: 0.5088, acc: 0.8004
loss: 0.5156, acc: 0.7977
loss: 0.5112, acc: 0.7985
loss: 0.5093, acc: 0.7990
loss: 0.5102, acc: 0.7990
loss: 0.5135, acc: 0.7975
loss: 0.5173, acc: 0.7958
loss: 0.5183, acc: 0.7952
loss: 0.5191, acc: 0.7954
loss: 0.5193, acc: 0.7953
loss: 0.5189, acc: 0.7950
loss: 0.5183, acc: 0.7952
loss: 0.5170, acc: 0.7956
loss: 0.5183, acc: 0.7953
loss: 0.5143, acc: 0.7959
loss: 0.5187, acc: 0.7923
loss: 0.5159, acc: 0.7937
loss: 0.5132, acc: 0.7943
loss: 0.5162, acc: 0.7928
loss: 0.5182, acc: 0.7912
loss: 0.5197, acc: 0.7893
loss: 0.5173, acc: 0.7899
loss: 0.5187, acc: 0.7899
loss: 0.5153, acc: 0.7911
loss: 0.5131, acc: 0.7923
loss: 0.5129, acc: 0.7922
loss: 0.5129, acc: 0.7918
loss: 0.5134, acc: 0.7918
loss: 0.5155, acc: 0.7914
loss: 0.5166, acc: 0.7913
loss: 0.5174, acc: 0.7907
loss: 0.5139, acc: 0.7924
loss: 0.5171, acc: 0.7914
loss: 0.5133, acc: 0.7930
loss: 0.5118, acc: 0.7927
loss: 0.5110, acc: 0.7929
loss: 0.5121, acc: 0.7926
loss: 0.5130, acc: 0.7928
loss: 0.5117, acc: 0.7937
loss: 0.5114, acc: 0.7944
loss: 0.5092, acc: 0.7953
loss: 0.5073, acc: 0.7960
loss: 0.5056, acc: 0.7965
loss: 0.5076, acc: 0.7955
loss: 0.5063, acc: 0.7962
loss: 0.5072, acc: 0.7958
loss: 0.5089, acc: 0.7953
loss: 0.5082, acc: 0.7959
loss: 0.5091, acc: 0.7955
loss: 0.5078, acc: 0.7964
loss: 0.5072, acc: 0.7968
loss: 0.5068, acc: 0.7977
loss: 0.5063, acc: 0.7980
loss: 0.5063, acc: 0.7982
loss: 0.5064, acc: 0.7982
loss: 0.5070, acc: 0.7983
> val_acc: 0.7860, val_f1: 0.7789
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.5869, acc: 0.7812
loss: 0.5473, acc: 0.8000
loss: 0.5103, acc: 0.8083
loss: 0.5098, acc: 0.7969
loss: 0.5067, acc: 0.7937
loss: 0.4955, acc: 0.8031
loss: 0.4901, acc: 0.8098
loss: 0.4919, acc: 0.8117
loss: 0.4934, acc: 0.8111
loss: 0.4988, acc: 0.8087
loss: 0.4904, acc: 0.8136
loss: 0.4852, acc: 0.8156
loss: 0.4804, acc: 0.8163
loss: 0.4817, acc: 0.8143
loss: 0.4780, acc: 0.8158
loss: 0.4761, acc: 0.8172
loss: 0.4672, acc: 0.8202
loss: 0.4679, acc: 0.8201
loss: 0.4617, acc: 0.8230
loss: 0.4625, acc: 0.8231
loss: 0.4695, acc: 0.8193
loss: 0.4749, acc: 0.8162
loss: 0.4732, acc: 0.8163
loss: 0.4729, acc: 0.8161
loss: 0.4721, acc: 0.8160
loss: 0.4700, acc: 0.8161
loss: 0.4686, acc: 0.8169
loss: 0.4668, acc: 0.8176
loss: 0.4668, acc: 0.8177
loss: 0.4670, acc: 0.8167
loss: 0.4678, acc: 0.8157
loss: 0.4702, acc: 0.8150
loss: 0.4679, acc: 0.8159
loss: 0.4684, acc: 0.8158
loss: 0.4723, acc: 0.8148
loss: 0.4711, acc: 0.8155
loss: 0.4706, acc: 0.8164
loss: 0.4682, acc: 0.8176
loss: 0.4680, acc: 0.8170
loss: 0.4690, acc: 0.8159
loss: 0.4688, acc: 0.8162
loss: 0.4677, acc: 0.8167
loss: 0.4648, acc: 0.8182
loss: 0.4649, acc: 0.8182
loss: 0.4641, acc: 0.8183
loss: 0.4656, acc: 0.8175
loss: 0.4666, acc: 0.8174
loss: 0.4647, acc: 0.8184
loss: 0.4623, acc: 0.8190
loss: 0.4615, acc: 0.8195
loss: 0.4638, acc: 0.8186
loss: 0.4624, acc: 0.8196
loss: 0.4637, acc: 0.8198
loss: 0.4621, acc: 0.8208
loss: 0.4613, acc: 0.8211
loss: 0.4620, acc: 0.8206
loss: 0.4590, acc: 0.8214
loss: 0.4617, acc: 0.8206
loss: 0.4630, acc: 0.8204
loss: 0.4635, acc: 0.8202
loss: 0.4626, acc: 0.8204
loss: 0.4614, acc: 0.8205
loss: 0.4614, acc: 0.8201
loss: 0.4615, acc: 0.8203
loss: 0.4601, acc: 0.8211
loss: 0.4602, acc: 0.8203
loss: 0.4606, acc: 0.8204
loss: 0.4594, acc: 0.8204
loss: 0.4608, acc: 0.8195
loss: 0.4630, acc: 0.8185
> val_acc: 0.7808, val_f1: 0.7770
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4352, acc: 0.8625
loss: 0.4482, acc: 0.8500
loss: 0.4243, acc: 0.8583
loss: 0.4060, acc: 0.8594
loss: 0.3998, acc: 0.8538
loss: 0.4112, acc: 0.8500
loss: 0.4064, acc: 0.8554
loss: 0.4076, acc: 0.8531
loss: 0.4175, acc: 0.8431
loss: 0.4229, acc: 0.8419
loss: 0.4220, acc: 0.8403
loss: 0.4209, acc: 0.8391
loss: 0.4249, acc: 0.8375
loss: 0.4250, acc: 0.8362
loss: 0.4304, acc: 0.8333
loss: 0.4330, acc: 0.8297
loss: 0.4341, acc: 0.8298
loss: 0.4275, acc: 0.8319
loss: 0.4268, acc: 0.8332
loss: 0.4267, acc: 0.8325
loss: 0.4277, acc: 0.8327
loss: 0.4279, acc: 0.8330
loss: 0.4292, acc: 0.8329
loss: 0.4251, acc: 0.8346
loss: 0.4209, acc: 0.8365
loss: 0.4192, acc: 0.8380
loss: 0.4146, acc: 0.8391
loss: 0.4176, acc: 0.8377
loss: 0.4167, acc: 0.8381
loss: 0.4199, acc: 0.8371
loss: 0.4176, acc: 0.8379
loss: 0.4184, acc: 0.8377
loss: 0.4174, acc: 0.8377
loss: 0.4179, acc: 0.8375
loss: 0.4204, acc: 0.8368
loss: 0.4232, acc: 0.8361
loss: 0.4244, acc: 0.8363
loss: 0.4235, acc: 0.8360
loss: 0.4221, acc: 0.8365
loss: 0.4223, acc: 0.8363
loss: 0.4213, acc: 0.8370
loss: 0.4228, acc: 0.8360
loss: 0.4270, acc: 0.8344
loss: 0.4265, acc: 0.8348
loss: 0.4257, acc: 0.8353
loss: 0.4270, acc: 0.8345
loss: 0.4262, acc: 0.8350
loss: 0.4259, acc: 0.8355
loss: 0.4249, acc: 0.8360
loss: 0.4259, acc: 0.8355
loss: 0.4263, acc: 0.8354
loss: 0.4273, acc: 0.8353
loss: 0.4287, acc: 0.8344
loss: 0.4280, acc: 0.8345
loss: 0.4281, acc: 0.8348
loss: 0.4274, acc: 0.8352
loss: 0.4281, acc: 0.8348
loss: 0.4269, acc: 0.8353
loss: 0.4278, acc: 0.8357
loss: 0.4280, acc: 0.8357
loss: 0.4279, acc: 0.8359
loss: 0.4270, acc: 0.8362
loss: 0.4268, acc: 0.8362
loss: 0.4287, acc: 0.8355
loss: 0.4307, acc: 0.8353
loss: 0.4308, acc: 0.8351
loss: 0.4306, acc: 0.8355
loss: 0.4290, acc: 0.8359
loss: 0.4272, acc: 0.8367
loss: 0.4282, acc: 0.8363
> val_acc: 0.8138, val_f1: 0.8049
>> saved: peft/roberta_lora/mams//acc_0.8138_f1_0.8049_230828-0934
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.5573, acc: 0.7812
loss: 0.5131, acc: 0.8031
loss: 0.4523, acc: 0.8229
loss: 0.4338, acc: 0.8359
loss: 0.4212, acc: 0.8462
loss: 0.4180, acc: 0.8500
loss: 0.4415, acc: 0.8366
loss: 0.4262, acc: 0.8406
loss: 0.4245, acc: 0.8424
loss: 0.4237, acc: 0.8438
loss: 0.4250, acc: 0.8426
loss: 0.4248, acc: 0.8406
loss: 0.4280, acc: 0.8380
loss: 0.4242, acc: 0.8406
loss: 0.4211, acc: 0.8421
loss: 0.4168, acc: 0.8438
loss: 0.4190, acc: 0.8426
loss: 0.4166, acc: 0.8438
loss: 0.4135, acc: 0.8447
loss: 0.4143, acc: 0.8450
loss: 0.4168, acc: 0.8435
loss: 0.4181, acc: 0.8418
loss: 0.4169, acc: 0.8413
loss: 0.4167, acc: 0.8417
loss: 0.4144, acc: 0.8425
loss: 0.4125, acc: 0.8430
loss: 0.4125, acc: 0.8433
loss: 0.4095, acc: 0.8449
loss: 0.4079, acc: 0.8442
loss: 0.4077, acc: 0.8446
loss: 0.4087, acc: 0.8438
loss: 0.4079, acc: 0.8430
loss: 0.4058, acc: 0.8434
loss: 0.4071, acc: 0.8423
loss: 0.4059, acc: 0.8430
loss: 0.4054, acc: 0.8431
loss: 0.4063, acc: 0.8426
loss: 0.4033, acc: 0.8438
loss: 0.4007, acc: 0.8449
loss: 0.4022, acc: 0.8445
loss: 0.4029, acc: 0.8448
loss: 0.4032, acc: 0.8449
loss: 0.4049, acc: 0.8443
loss: 0.4033, acc: 0.8453
loss: 0.4039, acc: 0.8453
loss: 0.4037, acc: 0.8450
loss: 0.4024, acc: 0.8455
loss: 0.4012, acc: 0.8462
loss: 0.3991, acc: 0.8467
loss: 0.3995, acc: 0.8464
loss: 0.4001, acc: 0.8460
loss: 0.4031, acc: 0.8447
loss: 0.4041, acc: 0.8441
loss: 0.4047, acc: 0.8440
loss: 0.4052, acc: 0.8435
loss: 0.4070, acc: 0.8430
loss: 0.4057, acc: 0.8433
loss: 0.4064, acc: 0.8428
loss: 0.4073, acc: 0.8426
loss: 0.4074, acc: 0.8428
loss: 0.4085, acc: 0.8424
loss: 0.4073, acc: 0.8432
loss: 0.4070, acc: 0.8437
loss: 0.4056, acc: 0.8437
loss: 0.4052, acc: 0.8439
loss: 0.4052, acc: 0.8437
loss: 0.4051, acc: 0.8435
loss: 0.4052, acc: 0.8430
loss: 0.4056, acc: 0.8419
loss: 0.4052, acc: 0.8422
> val_acc: 0.8183, val_f1: 0.8117
>> saved: peft/roberta_lora/mams//acc_0.8183_f1_0.8117_230828-0936
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3361, acc: 0.8875
loss: 0.3398, acc: 0.8906
loss: 0.3458, acc: 0.8792
loss: 0.3361, acc: 0.8781
loss: 0.3352, acc: 0.8788
loss: 0.3497, acc: 0.8740
loss: 0.3484, acc: 0.8741
loss: 0.3484, acc: 0.8734
loss: 0.3419, acc: 0.8771
loss: 0.3293, acc: 0.8800
loss: 0.3510, acc: 0.8739
loss: 0.3507, acc: 0.8724
loss: 0.3538, acc: 0.8707
loss: 0.3517, acc: 0.8696
loss: 0.3531, acc: 0.8708
loss: 0.3541, acc: 0.8707
loss: 0.3552, acc: 0.8717
loss: 0.3539, acc: 0.8715
loss: 0.3577, acc: 0.8707
loss: 0.3572, acc: 0.8706
loss: 0.3596, acc: 0.8693
loss: 0.3635, acc: 0.8670
loss: 0.3659, acc: 0.8655
loss: 0.3687, acc: 0.8646
loss: 0.3702, acc: 0.8640
loss: 0.3690, acc: 0.8635
loss: 0.3685, acc: 0.8630
loss: 0.3704, acc: 0.8614
loss: 0.3727, acc: 0.8595
loss: 0.3730, acc: 0.8594
loss: 0.3763, acc: 0.8577
loss: 0.3815, acc: 0.8559
loss: 0.3816, acc: 0.8559
loss: 0.3855, acc: 0.8548
loss: 0.3886, acc: 0.8543
loss: 0.3865, acc: 0.8552
loss: 0.3872, acc: 0.8547
loss: 0.3862, acc: 0.8548
loss: 0.3869, acc: 0.8550
loss: 0.3874, acc: 0.8545
loss: 0.3877, acc: 0.8549
loss: 0.3885, acc: 0.8546
loss: 0.3883, acc: 0.8548
loss: 0.3875, acc: 0.8545
loss: 0.3874, acc: 0.8547
loss: 0.3859, acc: 0.8546
loss: 0.3855, acc: 0.8553
loss: 0.3837, acc: 0.8560
loss: 0.3836, acc: 0.8560
loss: 0.3863, acc: 0.8550
loss: 0.3856, acc: 0.8554
loss: 0.3850, acc: 0.8555
loss: 0.3839, acc: 0.8554
loss: 0.3841, acc: 0.8553
loss: 0.3878, acc: 0.8540
loss: 0.3871, acc: 0.8540
loss: 0.3872, acc: 0.8539
loss: 0.3865, acc: 0.8537
loss: 0.3876, acc: 0.8533
loss: 0.3891, acc: 0.8528
loss: 0.3889, acc: 0.8529
loss: 0.3888, acc: 0.8531
loss: 0.3890, acc: 0.8531
loss: 0.3890, acc: 0.8530
loss: 0.3904, acc: 0.8520
loss: 0.3891, acc: 0.8527
loss: 0.3893, acc: 0.8524
loss: 0.3884, acc: 0.8524
loss: 0.3879, acc: 0.8526
loss: 0.3889, acc: 0.8524
> val_acc: 0.7980, val_f1: 0.7953
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.2460, acc: 0.9000
loss: 0.2622, acc: 0.8938
loss: 0.2998, acc: 0.8833
loss: 0.3383, acc: 0.8656
loss: 0.3602, acc: 0.8500
loss: 0.3728, acc: 0.8469
loss: 0.3719, acc: 0.8464
loss: 0.3782, acc: 0.8453
loss: 0.3828, acc: 0.8493
loss: 0.3783, acc: 0.8544
loss: 0.3756, acc: 0.8574
loss: 0.3695, acc: 0.8604
loss: 0.3670, acc: 0.8587
loss: 0.3594, acc: 0.8621
loss: 0.3644, acc: 0.8592
loss: 0.3676, acc: 0.8586
loss: 0.3730, acc: 0.8562
loss: 0.3712, acc: 0.8573
loss: 0.3719, acc: 0.8559
loss: 0.3717, acc: 0.8559
loss: 0.3685, acc: 0.8565
loss: 0.3687, acc: 0.8565
loss: 0.3666, acc: 0.8579
loss: 0.3635, acc: 0.8586
loss: 0.3654, acc: 0.8580
loss: 0.3639, acc: 0.8582
loss: 0.3631, acc: 0.8586
loss: 0.3627, acc: 0.8594
loss: 0.3619, acc: 0.8599
loss: 0.3616, acc: 0.8598
loss: 0.3605, acc: 0.8603
loss: 0.3584, acc: 0.8617
loss: 0.3566, acc: 0.8627
loss: 0.3580, acc: 0.8629
loss: 0.3568, acc: 0.8636
loss: 0.3591, acc: 0.8623
loss: 0.3610, acc: 0.8611
loss: 0.3620, acc: 0.8599
loss: 0.3620, acc: 0.8601
loss: 0.3605, acc: 0.8605
loss: 0.3614, acc: 0.8604
loss: 0.3625, acc: 0.8592
loss: 0.3625, acc: 0.8592
loss: 0.3619, acc: 0.8597
loss: 0.3610, acc: 0.8596
loss: 0.3637, acc: 0.8594
loss: 0.3668, acc: 0.8576
loss: 0.3670, acc: 0.8574
loss: 0.3657, acc: 0.8578
loss: 0.3670, acc: 0.8574
loss: 0.3668, acc: 0.8574
loss: 0.3659, acc: 0.8577
loss: 0.3662, acc: 0.8570
loss: 0.3668, acc: 0.8566
loss: 0.3685, acc: 0.8559
loss: 0.3693, acc: 0.8554
loss: 0.3704, acc: 0.8552
loss: 0.3712, acc: 0.8547
loss: 0.3715, acc: 0.8549
loss: 0.3705, acc: 0.8554
loss: 0.3683, acc: 0.8567
loss: 0.3685, acc: 0.8568
loss: 0.3700, acc: 0.8564
loss: 0.3688, acc: 0.8568
loss: 0.3686, acc: 0.8568
loss: 0.3701, acc: 0.8562
loss: 0.3719, acc: 0.8557
loss: 0.3722, acc: 0.8554
loss: 0.3707, acc: 0.8560
loss: 0.3713, acc: 0.8558
> val_acc: 0.8266, val_f1: 0.8198
>> saved: peft/roberta_lora/mams//acc_0.8266_f1_0.8198_230828-0939
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.2496, acc: 0.9125
loss: 0.2857, acc: 0.8938
loss: 0.3055, acc: 0.8812
loss: 0.3087, acc: 0.8844
loss: 0.3034, acc: 0.8850
loss: 0.3124, acc: 0.8802
loss: 0.3214, acc: 0.8777
loss: 0.3141, acc: 0.8805
loss: 0.3207, acc: 0.8785
loss: 0.3285, acc: 0.8750
loss: 0.3309, acc: 0.8722
loss: 0.3329, acc: 0.8708
loss: 0.3355, acc: 0.8702
loss: 0.3316, acc: 0.8710
loss: 0.3346, acc: 0.8704
loss: 0.3410, acc: 0.8691
loss: 0.3465, acc: 0.8673
loss: 0.3422, acc: 0.8698
loss: 0.3443, acc: 0.8681
loss: 0.3398, acc: 0.8694
loss: 0.3441, acc: 0.8679
loss: 0.3456, acc: 0.8676
loss: 0.3418, acc: 0.8685
loss: 0.3412, acc: 0.8682
loss: 0.3432, acc: 0.8685
loss: 0.3500, acc: 0.8661
loss: 0.3506, acc: 0.8646
loss: 0.3518, acc: 0.8636
loss: 0.3518, acc: 0.8640
loss: 0.3511, acc: 0.8646
loss: 0.3514, acc: 0.8641
loss: 0.3580, acc: 0.8609
loss: 0.3595, acc: 0.8600
loss: 0.3585, acc: 0.8603
loss: 0.3582, acc: 0.8602
loss: 0.3572, acc: 0.8611
loss: 0.3594, acc: 0.8601
loss: 0.3602, acc: 0.8602
loss: 0.3589, acc: 0.8614
loss: 0.3587, acc: 0.8614
loss: 0.3574, acc: 0.8620
loss: 0.3565, acc: 0.8625
loss: 0.3569, acc: 0.8624
loss: 0.3537, acc: 0.8639
loss: 0.3558, acc: 0.8635
loss: 0.3558, acc: 0.8640
loss: 0.3568, acc: 0.8637
loss: 0.3559, acc: 0.8643
loss: 0.3538, acc: 0.8657
loss: 0.3530, acc: 0.8661
loss: 0.3521, acc: 0.8668
loss: 0.3536, acc: 0.8665
loss: 0.3545, acc: 0.8660
loss: 0.3549, acc: 0.8657
loss: 0.3548, acc: 0.8664
loss: 0.3561, acc: 0.8661
loss: 0.3559, acc: 0.8662
loss: 0.3551, acc: 0.8663
loss: 0.3554, acc: 0.8660
loss: 0.3540, acc: 0.8662
loss: 0.3543, acc: 0.8663
loss: 0.3545, acc: 0.8667
loss: 0.3535, acc: 0.8673
loss: 0.3529, acc: 0.8677
loss: 0.3530, acc: 0.8677
loss: 0.3545, acc: 0.8669
loss: 0.3543, acc: 0.8672
loss: 0.3550, acc: 0.8672
loss: 0.3562, acc: 0.8666
loss: 0.3576, acc: 0.8659
> val_acc: 0.7868, val_f1: 0.7853
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3011, acc: 0.8875
loss: 0.3525, acc: 0.8844
loss: 0.3452, acc: 0.8708
loss: 0.3368, acc: 0.8719
loss: 0.3409, acc: 0.8738
loss: 0.3445, acc: 0.8688
loss: 0.3364, acc: 0.8732
loss: 0.3201, acc: 0.8766
loss: 0.3241, acc: 0.8757
loss: 0.3167, acc: 0.8800
loss: 0.3234, acc: 0.8761
loss: 0.3261, acc: 0.8729
loss: 0.3233, acc: 0.8760
loss: 0.3237, acc: 0.8741
loss: 0.3364, acc: 0.8683
loss: 0.3377, acc: 0.8672
loss: 0.3377, acc: 0.8676
loss: 0.3374, acc: 0.8688
loss: 0.3394, acc: 0.8681
loss: 0.3365, acc: 0.8694
loss: 0.3392, acc: 0.8690
loss: 0.3398, acc: 0.8699
loss: 0.3362, acc: 0.8715
loss: 0.3355, acc: 0.8719
loss: 0.3294, acc: 0.8752
loss: 0.3288, acc: 0.8757
loss: 0.3271, acc: 0.8771
loss: 0.3272, acc: 0.8777
loss: 0.3254, acc: 0.8784
loss: 0.3248, acc: 0.8788
loss: 0.3243, acc: 0.8792
loss: 0.3244, acc: 0.8789
loss: 0.3222, acc: 0.8795
loss: 0.3246, acc: 0.8789
loss: 0.3269, acc: 0.8775
loss: 0.3274, acc: 0.8767
loss: 0.3276, acc: 0.8769
loss: 0.3292, acc: 0.8758
loss: 0.3302, acc: 0.8761
loss: 0.3308, acc: 0.8764
loss: 0.3306, acc: 0.8768
loss: 0.3316, acc: 0.8765
loss: 0.3315, acc: 0.8762
loss: 0.3313, acc: 0.8767
loss: 0.3308, acc: 0.8767
loss: 0.3323, acc: 0.8764
loss: 0.3334, acc: 0.8754
loss: 0.3345, acc: 0.8755
loss: 0.3355, acc: 0.8750
loss: 0.3367, acc: 0.8745
loss: 0.3381, acc: 0.8733
loss: 0.3389, acc: 0.8727
loss: 0.3413, acc: 0.8722
loss: 0.3411, acc: 0.8725
loss: 0.3413, acc: 0.8720
loss: 0.3424, acc: 0.8715
loss: 0.3428, acc: 0.8716
loss: 0.3426, acc: 0.8717
loss: 0.3430, acc: 0.8717
loss: 0.3430, acc: 0.8715
loss: 0.3418, acc: 0.8722
loss: 0.3413, acc: 0.8726
loss: 0.3429, acc: 0.8721
loss: 0.3433, acc: 0.8719
loss: 0.3425, acc: 0.8725
loss: 0.3410, acc: 0.8731
loss: 0.3415, acc: 0.8731
loss: 0.3406, acc: 0.8738
loss: 0.3411, acc: 0.8743
loss: 0.3405, acc: 0.8746
> val_acc: 0.8063, val_f1: 0.7985
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3551, acc: 0.8562
loss: 0.3580, acc: 0.8781
loss: 0.3231, acc: 0.8875
loss: 0.3126, acc: 0.8875
loss: 0.3224, acc: 0.8825
loss: 0.3209, acc: 0.8802
loss: 0.3263, acc: 0.8777
loss: 0.3211, acc: 0.8773
loss: 0.3154, acc: 0.8812
loss: 0.3148, acc: 0.8819
loss: 0.3109, acc: 0.8847
loss: 0.3170, acc: 0.8828
loss: 0.3268, acc: 0.8779
loss: 0.3327, acc: 0.8746
loss: 0.3326, acc: 0.8750
loss: 0.3343, acc: 0.8742
loss: 0.3371, acc: 0.8724
loss: 0.3344, acc: 0.8736
loss: 0.3318, acc: 0.8737
loss: 0.3257, acc: 0.8753
loss: 0.3226, acc: 0.8768
loss: 0.3256, acc: 0.8756
loss: 0.3273, acc: 0.8750
loss: 0.3271, acc: 0.8747
loss: 0.3276, acc: 0.8742
loss: 0.3306, acc: 0.8733
loss: 0.3325, acc: 0.8727
loss: 0.3296, acc: 0.8739
loss: 0.3286, acc: 0.8744
loss: 0.3280, acc: 0.8750
loss: 0.3283, acc: 0.8742
loss: 0.3283, acc: 0.8744
loss: 0.3277, acc: 0.8752
loss: 0.3294, acc: 0.8750
loss: 0.3296, acc: 0.8748
loss: 0.3279, acc: 0.8755
loss: 0.3289, acc: 0.8757
loss: 0.3301, acc: 0.8745
loss: 0.3296, acc: 0.8742
loss: 0.3295, acc: 0.8741
loss: 0.3306, acc: 0.8741
loss: 0.3323, acc: 0.8732
loss: 0.3329, acc: 0.8735
loss: 0.3340, acc: 0.8734
loss: 0.3343, acc: 0.8736
loss: 0.3357, acc: 0.8735
loss: 0.3358, acc: 0.8733
loss: 0.3369, acc: 0.8727
loss: 0.3352, acc: 0.8735
loss: 0.3340, acc: 0.8739
loss: 0.3350, acc: 0.8735
loss: 0.3357, acc: 0.8734
loss: 0.3344, acc: 0.8735
loss: 0.3348, acc: 0.8731
loss: 0.3341, acc: 0.8734
loss: 0.3341, acc: 0.8734
loss: 0.3340, acc: 0.8735
loss: 0.3347, acc: 0.8731
loss: 0.3347, acc: 0.8731
loss: 0.3348, acc: 0.8730
loss: 0.3352, acc: 0.8732
loss: 0.3356, acc: 0.8731
loss: 0.3362, acc: 0.8724
loss: 0.3352, acc: 0.8729
loss: 0.3353, acc: 0.8730
loss: 0.3367, acc: 0.8723
loss: 0.3359, acc: 0.8728
loss: 0.3356, acc: 0.8724
loss: 0.3346, acc: 0.8729
loss: 0.3335, acc: 0.8735
> val_acc: 0.8296, val_f1: 0.8232
>> saved: peft/roberta_lora/mams//acc_0.8296_f1_0.8232_230828-0943
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
loss: 0.3555, acc: 0.8875
loss: 0.3197, acc: 0.9031
loss: 0.3240, acc: 0.8917
loss: 0.2871, acc: 0.9047
loss: 0.2758, acc: 0.9087
loss: 0.2836, acc: 0.9073
loss: 0.3105, acc: 0.8929
loss: 0.3056, acc: 0.8938
loss: 0.2983, acc: 0.8965
loss: 0.3065, acc: 0.8956
loss: 0.2934, acc: 0.8994
loss: 0.2899, acc: 0.9021
loss: 0.2934, acc: 0.9000
loss: 0.2984, acc: 0.8973
loss: 0.2972, acc: 0.8971
loss: 0.2955, acc: 0.8941
loss: 0.2932, acc: 0.8941
loss: 0.2941, acc: 0.8944
loss: 0.2930, acc: 0.8944
loss: 0.2932, acc: 0.8928
loss: 0.2965, acc: 0.8905
loss: 0.2951, acc: 0.8909
loss: 0.2957, acc: 0.8916
loss: 0.2927, acc: 0.8922
loss: 0.2909, acc: 0.8930
loss: 0.2921, acc: 0.8933
loss: 0.2981, acc: 0.8900
loss: 0.2988, acc: 0.8895
loss: 0.3007, acc: 0.8901
loss: 0.2994, acc: 0.8908
loss: 0.3003, acc: 0.8901
loss: 0.2983, acc: 0.8910
loss: 0.2975, acc: 0.8915
loss: 0.2981, acc: 0.8914
loss: 0.2985, acc: 0.8914
loss: 0.2973, acc: 0.8915
loss: 0.3003, acc: 0.8902
loss: 0.3036, acc: 0.8885
loss: 0.3049, acc: 0.8875
loss: 0.3056, acc: 0.8869
loss: 0.3033, acc: 0.8880
loss: 0.3026, acc: 0.8879
loss: 0.3021, acc: 0.8876
loss: 0.3028, acc: 0.8874
loss: 0.3036, acc: 0.8865
loss: 0.3063, acc: 0.8856
loss: 0.3057, acc: 0.8858
loss: 0.3077, acc: 0.8849
loss: 0.3097, acc: 0.8846
loss: 0.3086, acc: 0.8848
loss: 0.3102, acc: 0.8839
loss: 0.3137, acc: 0.8831
loss: 0.3138, acc: 0.8824
loss: 0.3148, acc: 0.8826
loss: 0.3160, acc: 0.8825
loss: 0.3183, acc: 0.8815
loss: 0.3194, acc: 0.8814
loss: 0.3209, acc: 0.8808
loss: 0.3214, acc: 0.8802
loss: 0.3216, acc: 0.8799
loss: 0.3234, acc: 0.8792
loss: 0.3231, acc: 0.8794
loss: 0.3250, acc: 0.8791
loss: 0.3260, acc: 0.8784
loss: 0.3269, acc: 0.8781
loss: 0.3268, acc: 0.8779
loss: 0.3266, acc: 0.8782
loss: 0.3265, acc: 0.8782
loss: 0.3263, acc: 0.8782
loss: 0.3264, acc: 0.8781
> val_acc: 0.7988, val_f1: 0.7881
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
loss: 0.2372, acc: 0.9250
loss: 0.2935, acc: 0.9000
loss: 0.2716, acc: 0.9062
loss: 0.2790, acc: 0.8984
loss: 0.2976, acc: 0.8912
loss: 0.2981, acc: 0.8906
loss: 0.3022, acc: 0.8884
loss: 0.2900, acc: 0.8914
loss: 0.2924, acc: 0.8917
loss: 0.2912, acc: 0.8900
loss: 0.2890, acc: 0.8903
loss: 0.2908, acc: 0.8906
loss: 0.2962, acc: 0.8880
loss: 0.2971, acc: 0.8862
loss: 0.3087, acc: 0.8812
loss: 0.3116, acc: 0.8812
loss: 0.3093, acc: 0.8831
loss: 0.3053, acc: 0.8861
loss: 0.2996, acc: 0.8888
loss: 0.3021, acc: 0.8894
loss: 0.3042, acc: 0.8881
loss: 0.3113, acc: 0.8855
loss: 0.3184, acc: 0.8834
loss: 0.3220, acc: 0.8818
loss: 0.3223, acc: 0.8822
loss: 0.3229, acc: 0.8820
loss: 0.3231, acc: 0.8812
loss: 0.3209, acc: 0.8821
loss: 0.3214, acc: 0.8819
loss: 0.3257, acc: 0.8802
loss: 0.3280, acc: 0.8794
loss: 0.3300, acc: 0.8789
loss: 0.3299, acc: 0.8784
loss: 0.3288, acc: 0.8779
loss: 0.3281, acc: 0.8788
loss: 0.3303, acc: 0.8769
loss: 0.3287, acc: 0.8772
loss: 0.3276, acc: 0.8778
loss: 0.3257, acc: 0.8788
loss: 0.3282, acc: 0.8778
loss: 0.3273, acc: 0.8787
loss: 0.3299, acc: 0.8778
loss: 0.3310, acc: 0.8775
loss: 0.3312, acc: 0.8774
loss: 0.3314, acc: 0.8776
loss: 0.3322, acc: 0.8774
loss: 0.3310, acc: 0.8778
loss: 0.3311, acc: 0.8775
loss: 0.3322, acc: 0.8772
loss: 0.3318, acc: 0.8776
loss: 0.3335, acc: 0.8767
loss: 0.3337, acc: 0.8768
loss: 0.3363, acc: 0.8757
loss: 0.3357, acc: 0.8759
loss: 0.3393, acc: 0.8748
loss: 0.3402, acc: 0.8737
loss: 0.3406, acc: 0.8730
loss: 0.3419, acc: 0.8724
loss: 0.3450, acc: 0.8713
loss: 0.3451, acc: 0.8707
loss: 0.3457, acc: 0.8703
loss: 0.3450, acc: 0.8709
loss: 0.3451, acc: 0.8712
loss: 0.3457, acc: 0.8712
loss: 0.3453, acc: 0.8712
loss: 0.3449, acc: 0.8715
loss: 0.3453, acc: 0.8713
loss: 0.3455, acc: 0.8708
loss: 0.3454, acc: 0.8704
loss: 0.3456, acc: 0.8704
> val_acc: 0.7733, val_f1: 0.7718
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
loss: 0.3436, acc: 0.8438
loss: 0.2745, acc: 0.8906
loss: 0.2645, acc: 0.8938
loss: 0.2659, acc: 0.8938
loss: 0.2775, acc: 0.8938
loss: 0.2801, acc: 0.8958
loss: 0.2723, acc: 0.8991
loss: 0.2659, acc: 0.9031
loss: 0.2739, acc: 0.9007
loss: 0.2708, acc: 0.9000
loss: 0.2726, acc: 0.9000
loss: 0.2711, acc: 0.9000
loss: 0.2782, acc: 0.8990
loss: 0.2818, acc: 0.9000
loss: 0.2804, acc: 0.9017
loss: 0.2843, acc: 0.9004
loss: 0.2830, acc: 0.9004
loss: 0.2848, acc: 0.8993
loss: 0.2847, acc: 0.8993
loss: 0.2848, acc: 0.8994
loss: 0.2797, acc: 0.9006
loss: 0.2764, acc: 0.9020
loss: 0.2776, acc: 0.9011
loss: 0.2851, acc: 0.8982
loss: 0.2905, acc: 0.8952
loss: 0.2939, acc: 0.8930
loss: 0.2971, acc: 0.8905
loss: 0.2958, acc: 0.8900
loss: 0.2994, acc: 0.8879
loss: 0.3058, acc: 0.8862
loss: 0.3052, acc: 0.8863
loss: 0.3038, acc: 0.8875
loss: 0.3076, acc: 0.8860
loss: 0.3096, acc: 0.8847
loss: 0.3109, acc: 0.8839
loss: 0.3126, acc: 0.8828
loss: 0.3140, acc: 0.8821
loss: 0.3127, acc: 0.8824
loss: 0.3139, acc: 0.8816
loss: 0.3183, acc: 0.8800
loss: 0.3208, acc: 0.8794
loss: 0.3193, acc: 0.8798
loss: 0.3190, acc: 0.8802
loss: 0.3182, acc: 0.8804
loss: 0.3191, acc: 0.8801
loss: 0.3207, acc: 0.8796
loss: 0.3193, acc: 0.8802
loss: 0.3215, acc: 0.8793
loss: 0.3226, acc: 0.8788
loss: 0.3236, acc: 0.8788
loss: 0.3225, acc: 0.8790
loss: 0.3240, acc: 0.8784
loss: 0.3227, acc: 0.8789
loss: 0.3229, acc: 0.8786
loss: 0.3209, acc: 0.8792
loss: 0.3206, acc: 0.8796
loss: 0.3209, acc: 0.8796
loss: 0.3222, acc: 0.8794
loss: 0.3229, acc: 0.8790
loss: 0.3238, acc: 0.8788
loss: 0.3240, acc: 0.8790
loss: 0.3232, acc: 0.8795
loss: 0.3213, acc: 0.8800
loss: 0.3212, acc: 0.8798
loss: 0.3224, acc: 0.8792
loss: 0.3234, acc: 0.8787
loss: 0.3236, acc: 0.8789
loss: 0.3224, acc: 0.8796
loss: 0.3209, acc: 0.8803
loss: 0.3204, acc: 0.8804
> val_acc: 0.8191, val_f1: 0.8119
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
loss: 0.3357, acc: 0.8875
loss: 0.3531, acc: 0.8781
loss: 0.3375, acc: 0.8812
loss: 0.3260, acc: 0.8875
loss: 0.3267, acc: 0.8838
loss: 0.3159, acc: 0.8865
loss: 0.3085, acc: 0.8902
loss: 0.3084, acc: 0.8875
loss: 0.2984, acc: 0.8910
loss: 0.3013, acc: 0.8912
loss: 0.2997, acc: 0.8915
loss: 0.2984, acc: 0.8927
loss: 0.3104, acc: 0.8889
loss: 0.3026, acc: 0.8920
loss: 0.2996, acc: 0.8929
loss: 0.2954, acc: 0.8949
loss: 0.2906, acc: 0.8978
loss: 0.2816, acc: 0.9007
loss: 0.2832, acc: 0.9000
loss: 0.2887, acc: 0.8966
loss: 0.2932, acc: 0.8943
loss: 0.2932, acc: 0.8952
loss: 0.2921, acc: 0.8957
loss: 0.2934, acc: 0.8940
loss: 0.2912, acc: 0.8952
loss: 0.2905, acc: 0.8954
loss: 0.2883, acc: 0.8958
loss: 0.2936, acc: 0.8946
loss: 0.2942, acc: 0.8942
loss: 0.2985, acc: 0.8925
loss: 0.2975, acc: 0.8929
loss: 0.2977, acc: 0.8934
loss: 0.2991, acc: 0.8926
loss: 0.2992, acc: 0.8934
loss: 0.3013, acc: 0.8927
loss: 0.3017, acc: 0.8925
loss: 0.3035, acc: 0.8919
loss: 0.3032, acc: 0.8921
loss: 0.3071, acc: 0.8910
loss: 0.3061, acc: 0.8912
loss: 0.3049, acc: 0.8918
loss: 0.3049, acc: 0.8915
loss: 0.3046, acc: 0.8917
loss: 0.3050, acc: 0.8912
loss: 0.3053, acc: 0.8912
loss: 0.3046, acc: 0.8912
loss: 0.3040, acc: 0.8916
loss: 0.3034, acc: 0.8921
loss: 0.3046, acc: 0.8915
loss: 0.3054, acc: 0.8914
loss: 0.3047, acc: 0.8914
loss: 0.3035, acc: 0.8921
loss: 0.3027, acc: 0.8917
loss: 0.3026, acc: 0.8918
loss: 0.3036, acc: 0.8912
loss: 0.3037, acc: 0.8913
loss: 0.3027, acc: 0.8922
loss: 0.3025, acc: 0.8921
loss: 0.3020, acc: 0.8921
loss: 0.3036, acc: 0.8912
loss: 0.3043, acc: 0.8907
loss: 0.3055, acc: 0.8897
loss: 0.3058, acc: 0.8899
loss: 0.3072, acc: 0.8898
loss: 0.3070, acc: 0.8896
loss: 0.3082, acc: 0.8888
loss: 0.3103, acc: 0.8882
loss: 0.3101, acc: 0.8880
loss: 0.3102, acc: 0.8877
loss: 0.3096, acc: 0.8877
> val_acc: 0.8101, val_f1: 0.8016
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
loss: 0.2738, acc: 0.8938
loss: 0.2948, acc: 0.8875
loss: 0.2615, acc: 0.8979
loss: 0.2525, acc: 0.9062
loss: 0.2694, acc: 0.9062
loss: 0.2624, acc: 0.9094
loss: 0.2554, acc: 0.9089
loss: 0.2488, acc: 0.9102
loss: 0.2476, acc: 0.9097
loss: 0.2505, acc: 0.9069
loss: 0.2469, acc: 0.9068
loss: 0.2436, acc: 0.9073
loss: 0.2496, acc: 0.9072
loss: 0.2535, acc: 0.9067
loss: 0.2533, acc: 0.9058
loss: 0.2579, acc: 0.9043
loss: 0.2630, acc: 0.9018
loss: 0.2670, acc: 0.9007
loss: 0.2706, acc: 0.8997
loss: 0.2758, acc: 0.8978
loss: 0.2792, acc: 0.8967
loss: 0.2794, acc: 0.8972
loss: 0.2828, acc: 0.8957
loss: 0.2838, acc: 0.8951
loss: 0.2852, acc: 0.8950
loss: 0.2893, acc: 0.8925
loss: 0.2910, acc: 0.8919
loss: 0.2938, acc: 0.8902
loss: 0.2926, acc: 0.8905
loss: 0.2964, acc: 0.8894
loss: 0.2943, acc: 0.8905
loss: 0.2924, acc: 0.8912
loss: 0.2920, acc: 0.8919
loss: 0.2931, acc: 0.8917
loss: 0.2926, acc: 0.8920
loss: 0.2901, acc: 0.8931
loss: 0.2917, acc: 0.8922
loss: 0.2922, acc: 0.8918
loss: 0.2952, acc: 0.8899
loss: 0.2972, acc: 0.8891
loss: 0.2960, acc: 0.8893
loss: 0.2953, acc: 0.8896
loss: 0.2995, acc: 0.8875
loss: 0.2986, acc: 0.8881
loss: 0.3002, acc: 0.8879
loss: 0.2991, acc: 0.8880
loss: 0.2981, acc: 0.8888
loss: 0.2976, acc: 0.8895
loss: 0.2977, acc: 0.8895
loss: 0.2983, acc: 0.8890
loss: 0.2967, acc: 0.8897
loss: 0.2983, acc: 0.8888
loss: 0.2995, acc: 0.8882
loss: 0.3021, acc: 0.8870
loss: 0.3025, acc: 0.8865
loss: 0.3017, acc: 0.8867
loss: 0.3009, acc: 0.8868
loss: 0.3005, acc: 0.8870
loss: 0.3005, acc: 0.8873
loss: 0.3006, acc: 0.8871
loss: 0.3003, acc: 0.8872
loss: 0.2986, acc: 0.8877
loss: 0.3013, acc: 0.8867
loss: 0.3025, acc: 0.8864
loss: 0.3028, acc: 0.8863
loss: 0.3032, acc: 0.8861
loss: 0.3032, acc: 0.8863
loss: 0.3032, acc: 0.8866
loss: 0.3036, acc: 0.8864
loss: 0.3034, acc: 0.8866
> val_acc: 0.8153, val_f1: 0.8090
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8353, test_f1: 0.8280
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./plm/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 1,775,622 || all params: 125,830,662 || trainable%: 1.4111202879946703
cuda memory allocated: 542569984
> n_trainable_params: 1775622, n_nontrainable_params: 124055040
> training arguments:
>>> model_name: roberta_lora
>>> dataset: mams
>>> optimizer: adam
>>> repeat: 3
>>> lr: 0.001
>>> l2reg: 0.0001
>>> num_epoch: 100
>>> batch_size: 16
>>> log_step: 10
>>> bert_dim: 768
>>> max_seq_len: 128
>>> polarities_dim: 3
>>> patience: 5
>>> device: cuda
>>> seed: 42
>>> save_model_dir: /media/b115/Backup/NLP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
loss: 1.1394, acc: 0.3812
loss: 1.1211, acc: 0.4281
loss: 1.1096, acc: 0.4021
loss: 1.0986, acc: 0.3984
loss: 1.0861, acc: 0.4150
loss: 1.0802, acc: 0.4125
loss: 1.0768, acc: 0.4152
loss: 1.0713, acc: 0.4109
loss: 1.0621, acc: 0.4222
loss: 1.0604, acc: 0.4238
loss: 1.0583, acc: 0.4290
loss: 1.0550, acc: 0.4302
loss: 1.0472, acc: 0.4341
loss: 1.0431, acc: 0.4326
loss: 1.0391, acc: 0.4317
loss: 1.0314, acc: 0.4367
loss: 1.0254, acc: 0.4437
loss: 1.0166, acc: 0.4493
loss: 1.0109, acc: 0.4579
loss: 0.9979, acc: 0.4675
loss: 0.9911, acc: 0.4765
loss: 0.9809, acc: 0.4852
loss: 0.9694, acc: 0.4957
loss: 0.9617, acc: 0.5021
loss: 0.9535, acc: 0.5092
loss: 0.9387, acc: 0.5197
loss: 0.9318, acc: 0.5262
loss: 0.9214, acc: 0.5324
loss: 0.9086, acc: 0.5412
loss: 0.9016, acc: 0.5475
loss: 0.8948, acc: 0.5528
loss: 0.8905, acc: 0.5568
loss: 0.8846, acc: 0.5608
loss: 0.8771, acc: 0.5651
loss: 0.8679, acc: 0.5730
loss: 0.8585, acc: 0.5800
loss: 0.8527, acc: 0.5853
loss: 0.8448, acc: 0.5898
loss: 0.8427, acc: 0.5923
loss: 0.8359, acc: 0.5966
loss: 0.8287, acc: 0.6012
loss: 0.8241, acc: 0.6048
loss: 0.8186, acc: 0.6081
loss: 0.8124, acc: 0.6129
loss: 0.8053, acc: 0.6175
loss: 0.7995, acc: 0.6213
loss: 0.7970, acc: 0.6243
loss: 0.7917, acc: 0.6281
loss: 0.7876, acc: 0.6306
loss: 0.7870, acc: 0.6314
loss: 0.7854, acc: 0.6331
loss: 0.7816, acc: 0.6355
loss: 0.7783, acc: 0.6376
loss: 0.7729, acc: 0.6411
loss: 0.7681, acc: 0.6439
loss: 0.7682, acc: 0.6448
loss: 0.7675, acc: 0.6457
loss: 0.7649, acc: 0.6473
loss: 0.7602, acc: 0.6497
loss: 0.7575, acc: 0.6519
loss: 0.7553, acc: 0.6537
loss: 0.7525, acc: 0.6554
loss: 0.7486, acc: 0.6578
loss: 0.7457, acc: 0.6596
loss: 0.7430, acc: 0.6615
loss: 0.7395, acc: 0.6635
loss: 0.7389, acc: 0.6650
loss: 0.7358, acc: 0.6664
loss: 0.7345, acc: 0.6671
loss: 0.7325, acc: 0.6681
> val_acc: 0.7943, val_f1: 0.7848
>> saved: peft/roberta_lora/mams//acc_0.7943_f1_0.7848_230828-0952
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
loss: 0.5459, acc: 0.7875
loss: 0.5393, acc: 0.7844
loss: 0.5275, acc: 0.7979
loss: 0.5189, acc: 0.7953
loss: 0.5156, acc: 0.8050
loss: 0.5138, acc: 0.8021
loss: 0.5217, acc: 0.7991
loss: 0.5199, acc: 0.8023
loss: 0.5239, acc: 0.7951
loss: 0.5302, acc: 0.7900
loss: 0.5288, acc: 0.7886
loss: 0.5238, acc: 0.7922
loss: 0.5194, acc: 0.7942
loss: 0.5154, acc: 0.7964
loss: 0.5130, acc: 0.7983
loss: 0.5131, acc: 0.8004
loss: 0.5111, acc: 0.8007
loss: 0.5205, acc: 0.7986
loss: 0.5166, acc: 0.8007
loss: 0.5217, acc: 0.7987
loss: 0.5257, acc: 0.7982
loss: 0.5238, acc: 0.7980
loss: 0.5246, acc: 0.7973
loss: 0.5236, acc: 0.7990
loss: 0.5214, acc: 0.7997
loss: 0.5187, acc: 0.8010
loss: 0.5239, acc: 0.7993
loss: 0.5239, acc: 0.7987
loss: 0.5243, acc: 0.7981
loss: 0.5258, acc: 0.7977
loss: 0.5245, acc: 0.7970
loss: 0.5252, acc: 0.7961
loss: 0.5287, acc: 0.7936
loss: 0.5288, acc: 0.7930
loss: 0.5284, acc: 0.7932
loss: 0.5275, acc: 0.7929
loss: 0.5250, acc: 0.7946
loss: 0.5254, acc: 0.7944
loss: 0.5271, acc: 0.7933
loss: 0.5260, acc: 0.7936
loss: 0.5252, acc: 0.7933
loss: 0.5225, acc: 0.7937
loss: 0.5235, acc: 0.7935
loss: 0.5230, acc: 0.7936
loss: 0.5230, acc: 0.7940
loss: 0.5206, acc: 0.7957
loss: 0.5192, acc: 0.7955
loss: 0.5203, acc: 0.7949
loss: 0.5222, acc: 0.7939
loss: 0.5208, acc: 0.7944
loss: 0.5227, acc: 0.7937
loss: 0.5246, acc: 0.7925
loss: 0.5245, acc: 0.7925
loss: 0.5240, acc: 0.7918
loss: 0.5255, acc: 0.7911
loss: 0.5242, acc: 0.7915
loss: 0.5259, acc: 0.7912
loss: 0.5270, acc: 0.7908
loss: 0.5256, acc: 0.7919
loss: 0.5248, acc: 0.7923
loss: 0.5245, acc: 0.7922
loss: 0.5271, acc: 0.7912
loss: 0.5274, acc: 0.7906
loss: 0.5273, acc: 0.7911
loss: 0.5263, acc: 0.7913
loss: 0.5255, acc: 0.7919
loss: 0.5243, acc: 0.7921
loss: 0.5237, acc: 0.7922
loss: 0.5228, acc: 0.7921
loss: 0.5221, acc: 0.7926
> val_acc: 0.8026, val_f1: 0.7991
>> saved: peft/roberta_lora/mams//acc_0.8026_f1_0.7991_230828-0953
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
loss: 0.4749, acc: 0.7937
loss: 0.4351, acc: 0.8281
loss: 0.4459, acc: 0.8271
loss: 0.4719, acc: 0.8141
loss: 0.4803, acc: 0.8150
loss: 0.4743, acc: 0.8167
loss: 0.4580, acc: 0.8232
loss: 0.4499, acc: 0.8289
loss: 0.4631, acc: 0.8229
loss: 0.4706, acc: 0.8200
loss: 0.4686, acc: 0.8227
loss: 0.4672, acc: 0.8234
loss: 0.4644, acc: 0.8212
loss: 0.4638, acc: 0.8246
loss: 0.4573, acc: 0.8283
loss: 0.4487, acc: 0.8328
loss: 0.4562, acc: 0.8279
loss: 0.4567, acc: 0.8271
loss: 0.4573, acc: 0.8273
loss: 0.4553, acc: 0.8272
loss: 0.4520, acc: 0.8283
loss: 0.4495, acc: 0.8293
loss: 0.4514, acc: 0.8274
loss: 0.4532, acc: 0.8268
loss: 0.4542, acc: 0.8277
loss: 0.4609, acc: 0.8245
loss: 0.4597, acc: 0.8259
loss: 0.4584, acc: 0.8257
loss: 0.4579, acc: 0.8256
loss: 0.4584, acc: 0.8256
loss: 0.4604, acc: 0.8244
loss: 0.4587, acc: 0.8250
loss: 0.4585, acc: 0.8248
loss: 0.4610, acc: 0.8241
loss: 0.4630, acc: 0.8223
loss: 0.4625, acc: 0.8219
loss: 0.4665, acc: 0.8199
loss: 0.4671, acc: 0.8194
loss: 0.4680, acc: 0.8184
loss: 0.4721, acc: 0.8172
loss: 0.4736, acc: 0.8165
loss: 0.4722, acc: 0.8170
loss: 0.4696, acc: 0.8182
loss: 0.4701, acc: 0.8176
loss: 0.4680, acc: 0.8181
loss: 0.4692, acc: 0.8181
loss: 0.4689, acc: 0.8178
loss: 0.4683, acc: 0.8181
loss: 0.4682, acc: 0.8187
loss: 0.4690, acc: 0.8180
loss: 0.4704, acc: 0.8170
loss: 0.4716, acc: 0.8168
loss: 0.4720, acc: 0.8169
loss: 0.4711, acc: 0.8174
loss: 0.4710, acc: 0.8174
loss: 0.4708, acc: 0.8175
loss: 0.4676, acc: 0.8192
loss: 0.4667, acc: 0.8195
loss: 0.4680, acc: 0.8194
loss: 0.4696, acc: 0.8183
loss: 0.4711, acc: 0.8171
loss: 0.4695, acc: 0.8178
loss: 0.4724, acc: 0.8167
loss: 0.4707, acc: 0.8173
loss: 0.4704, acc: 0.8177
loss: 0.4696, acc: 0.8186
loss: 0.4709, acc: 0.8185
loss: 0.4697, acc: 0.8187
loss: 0.4722, acc: 0.8174
loss: 0.4737, acc: 0.8168
> val_acc: 0.8281, val_f1: 0.8204
>> saved: peft/roberta_lora/mams//acc_0.8281_f1_0.8204_230828-0955
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
loss: 0.4013, acc: 0.8562
loss: 0.4516, acc: 0.8500
loss: 0.4289, acc: 0.8583
loss: 0.4007, acc: 0.8656
loss: 0.3934, acc: 0.8625
loss: 0.4093, acc: 0.8562
loss: 0.4154, acc: 0.8518
loss: 0.4236, acc: 0.8453
loss: 0.4143, acc: 0.8479
loss: 0.4209, acc: 0.8450
loss: 0.4142, acc: 0.8483
loss: 0.4101, acc: 0.8479
loss: 0.4075, acc: 0.8486
loss: 0.4085, acc: 0.8487
loss: 0.4230, acc: 0.8450
loss: 0.4237, acc: 0.8457
loss: 0.4195, acc: 0.8463
loss: 0.4102, acc: 0.8493
loss: 0.4174, acc: 0.8474
loss: 0.4169, acc: 0.8478
loss: 0.4194, acc: 0.8461
loss: 0.4197, acc: 0.8460
loss: 0.4217, acc: 0.8451
loss: 0.4280, acc: 0.8417
loss: 0.4275, acc: 0.8417
loss: 0.4284, acc: 0.8411
loss: 0.4289, acc: 0.8407
loss: 0.4273, acc: 0.8417
loss: 0.4254, acc: 0.8420
loss: 0.4266, acc: 0.8415
loss: 0.4294, acc: 0.8397
loss: 0.4300, acc: 0.8398
loss: 0.4293, acc: 0.8400
loss: 0.4304, acc: 0.8395
loss: 0.4322, acc: 0.8382
loss: 0.4329, acc: 0.8375
loss: 0.4307, acc: 0.8378
loss: 0.4325, acc: 0.8368
loss: 0.4307, acc: 0.8375
loss: 0.4281, acc: 0.8383
loss: 0.4253, acc: 0.8396
loss: 0.4257, acc: 0.8390
loss: 0.4256, acc: 0.8391
loss: 0.4265, acc: 0.8386
loss: 0.4285, acc: 0.8381
loss: 0.4272, acc: 0.8385
loss: 0.4295, acc: 0.8379
loss: 0.4298, acc: 0.8378
loss: 0.4299, acc: 0.8375
loss: 0.4297, acc: 0.8375
loss: 0.4307, acc: 0.8374
loss: 0.4311, acc: 0.8365
loss: 0.4326, acc: 0.8357
loss: 0.4333, acc: 0.8353
loss: 0.4335, acc: 0.8351
loss: 0.4305, acc: 0.8365
loss: 0.4313, acc: 0.8360
loss: 0.4322, acc: 0.8360
loss: 0.4322, acc: 0.8358
loss: 0.4316, acc: 0.8364
loss: 0.4326, acc: 0.8357
loss: 0.4337, acc: 0.8351
loss: 0.4332, acc: 0.8352
loss: 0.4347, acc: 0.8342
loss: 0.4360, acc: 0.8335
loss: 0.4353, acc: 0.8332
loss: 0.4353, acc: 0.8333
loss: 0.4343, acc: 0.8336
loss: 0.4360, acc: 0.8328
loss: 0.4362, acc: 0.8323
> val_acc: 0.8033, val_f1: 0.7925
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
loss: 0.4410, acc: 0.8250
loss: 0.4048, acc: 0.8406
loss: 0.3939, acc: 0.8479
loss: 0.3882, acc: 0.8438
loss: 0.3767, acc: 0.8475
loss: 0.3906, acc: 0.8406
loss: 0.3911, acc: 0.8393
loss: 0.3968, acc: 0.8367
loss: 0.3917, acc: 0.8417
loss: 0.3839, acc: 0.8444
loss: 0.3962, acc: 0.8386
loss: 0.4049, acc: 0.8375
loss: 0.4100, acc: 0.8351
loss: 0.4103, acc: 0.8339
loss: 0.4136, acc: 0.8337
loss: 0.4079, acc: 0.8379
loss: 0.4055, acc: 0.8390
loss: 0.4017, acc: 0.8410
loss: 0.3997, acc: 0.8411
loss: 0.3961, acc: 0.8416
loss: 0.3979, acc: 0.8405
loss: 0.3974, acc: 0.8406
loss: 0.4022, acc: 0.8386
loss: 0.4033, acc: 0.8370
loss: 0.3980, acc: 0.8393
loss: 0.4024, acc: 0.8380
loss: 0.4044, acc: 0.8377
loss: 0.4066, acc: 0.8375
loss: 0.4077, acc: 0.8373
loss: 0.4039, acc: 0.8392
loss: 0.4015, acc: 0.8407
loss: 0.4019, acc: 0.8412
loss: 0.4022, acc: 0.8411
loss: 0.4009, acc: 0.8415
loss: 0.4013, acc: 0.8416
loss: 0.4028, acc: 0.8417
loss: 0.4024, acc: 0.8417
loss: 0.4052, acc: 0.8414
loss: 0.4057, acc: 0.8405
loss: 0.4070, acc: 0.8398
loss: 0.4063, acc: 0.8399
loss: 0.4091, acc: 0.8390
loss: 0.4093, acc: 0.8397
loss: 0.4095, acc: 0.8395
loss: 0.4110, acc: 0.8393
loss: 0.4110, acc: 0.8391
loss: 0.4115, acc: 0.8390
loss: 0.4110, acc: 0.8392
loss: 0.4090, acc: 0.8398
loss: 0.4074, acc: 0.8406
loss: 0.4068, acc: 0.8406
loss: 0.4061, acc: 0.8410
loss: 0.4077, acc: 0.8406
loss: 0.4077, acc: 0.8400
loss: 0.4099, acc: 0.8393
loss: 0.4096, acc: 0.8395
loss: 0.4093, acc: 0.8395
loss: 0.4103, acc: 0.8394
loss: 0.4089, acc: 0.8400
loss: 0.4093, acc: 0.8395
loss: 0.4103, acc: 0.8386
loss: 0.4106, acc: 0.8388
loss: 0.4099, acc: 0.8387
loss: 0.4109, acc: 0.8381
loss: 0.4121, acc: 0.8377
loss: 0.4127, acc: 0.8379
loss: 0.4126, acc: 0.8380
loss: 0.4133, acc: 0.8373
loss: 0.4123, acc: 0.8375
loss: 0.4127, acc: 0.8373
> val_acc: 0.8288, val_f1: 0.8234
>> saved: peft/roberta_lora/mams//acc_0.8288_f1_0.8234_230828-0958
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
loss: 0.3603, acc: 0.8250
loss: 0.4244, acc: 0.8156
loss: 0.3752, acc: 0.8438
loss: 0.3806, acc: 0.8469
loss: 0.3877, acc: 0.8450
loss: 0.4011, acc: 0.8375
loss: 0.3981, acc: 0.8429
loss: 0.3866, acc: 0.8484
loss: 0.3917, acc: 0.8444
loss: 0.3887, acc: 0.8456
loss: 0.4022, acc: 0.8420
loss: 0.3950, acc: 0.8453
loss: 0.3915, acc: 0.8476
loss: 0.3861, acc: 0.8487
loss: 0.3864, acc: 0.8500
loss: 0.3904, acc: 0.8492
loss: 0.3855, acc: 0.8515
loss: 0.3837, acc: 0.8524
loss: 0.3884, acc: 0.8500
loss: 0.3882, acc: 0.8503
loss: 0.3913, acc: 0.8470
loss: 0.3882, acc: 0.8489
loss: 0.3887, acc: 0.8486
loss: 0.3872, acc: 0.8495
loss: 0.3849, acc: 0.8502
loss: 0.3830, acc: 0.8512
loss: 0.3842, acc: 0.8512
loss: 0.3863, acc: 0.8513
loss: 0.3892, acc: 0.8491
loss: 0.3874, acc: 0.8498
loss: 0.3873, acc: 0.8502
loss: 0.3868, acc: 0.8516
loss: 0.3868, acc: 0.8509
loss: 0.3877, acc: 0.8509
loss: 0.3868, acc: 0.8514
loss: 0.3874, acc: 0.8509
loss: 0.3839, acc: 0.8517
loss: 0.3895, acc: 0.8497
loss: 0.3901, acc: 0.8500
loss: 0.3894, acc: 0.8506
loss: 0.3878, acc: 0.8514
loss: 0.3906, acc: 0.8506
loss: 0.3901, acc: 0.8507
loss: 0.3886, acc: 0.8510
loss: 0.3903, acc: 0.8506
loss: 0.3891, acc: 0.8508
loss: 0.3931, acc: 0.8479
loss: 0.3948, acc: 0.8477
loss: 0.3952, acc: 0.8480
loss: 0.3949, acc: 0.8476
loss: 0.3951, acc: 0.8479
loss: 0.3942, acc: 0.8476
loss: 0.3951, acc: 0.8469
loss: 0.3952, acc: 0.8465
loss: 0.3974, acc: 0.8459
loss: 0.3975, acc: 0.8461
loss: 0.3975, acc: 0.8463
loss: 0.3971, acc: 0.8467
loss: 0.3964, acc: 0.8466
loss: 0.3943, acc: 0.8473
loss: 0.3945, acc: 0.8472
loss: 0.3929, acc: 0.8480
loss: 0.3911, acc: 0.8490
loss: 0.3909, acc: 0.8488
loss: 0.3898, acc: 0.8488
loss: 0.3912, acc: 0.8485
loss: 0.3903, acc: 0.8490
loss: 0.3891, acc: 0.8495
loss: 0.3888, acc: 0.8493
loss: 0.3883, acc: 0.8495
> val_acc: 0.8206, val_f1: 0.8132
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
loss: 0.4140, acc: 0.8625
loss: 0.3895, acc: 0.8750
loss: 0.3994, acc: 0.8625
loss: 0.3937, acc: 0.8656
loss: 0.3691, acc: 0.8725
loss: 0.3562, acc: 0.8781
loss: 0.3589, acc: 0.8777
loss: 0.3535, acc: 0.8797
loss: 0.3632, acc: 0.8736
loss: 0.3617, acc: 0.8750
loss: 0.3616, acc: 0.8744
loss: 0.3647, acc: 0.8719
loss: 0.3666, acc: 0.8688
loss: 0.3633, acc: 0.8696
loss: 0.3599, acc: 0.8675
loss: 0.3670, acc: 0.8648
loss: 0.3685, acc: 0.8636
loss: 0.3651, acc: 0.8646
loss: 0.3625, acc: 0.8658
loss: 0.3603, acc: 0.8669
loss: 0.3608, acc: 0.8664
loss: 0.3612, acc: 0.8668
loss: 0.3603, acc: 0.8660
loss: 0.3557, acc: 0.8674
loss: 0.3568, acc: 0.8670
loss: 0.3601, acc: 0.8666
loss: 0.3637, acc: 0.8644
loss: 0.3663, acc: 0.8629
loss: 0.3683, acc: 0.8619
loss: 0.3693, acc: 0.8617
loss: 0.3680, acc: 0.8617
loss: 0.3685, acc: 0.8613
loss: 0.3679, acc: 0.8614
loss: 0.3672, acc: 0.8608
loss: 0.3676, acc: 0.8605
loss: 0.3677, acc: 0.8608
loss: 0.3669, acc: 0.8603
loss: 0.3691, acc: 0.8584
loss: 0.3686, acc: 0.8585
loss: 0.3674, acc: 0.8588
loss: 0.3681, acc: 0.8588
loss: 0.3706, acc: 0.8574
loss: 0.3703, acc: 0.8574
loss: 0.3695, acc: 0.8577
loss: 0.3674, acc: 0.8586
loss: 0.3710, acc: 0.8576
loss: 0.3720, acc: 0.8573
loss: 0.3727, acc: 0.8573
loss: 0.3717, acc: 0.8577
loss: 0.3727, acc: 0.8570
loss: 0.3740, acc: 0.8561
loss: 0.3752, acc: 0.8558
loss: 0.3770, acc: 0.8554
loss: 0.3768, acc: 0.8557
loss: 0.3754, acc: 0.8561
loss: 0.3759, acc: 0.8558
loss: 0.3772, acc: 0.8554
loss: 0.3777, acc: 0.8553
loss: 0.3796, acc: 0.8542
loss: 0.3802, acc: 0.8542
loss: 0.3798, acc: 0.8546
loss: 0.3775, acc: 0.8553
loss: 0.3792, acc: 0.8547
loss: 0.3801, acc: 0.8546
loss: 0.3807, acc: 0.8543
loss: 0.3795, acc: 0.8547
loss: 0.3806, acc: 0.8544
loss: 0.3813, acc: 0.8541
loss: 0.3818, acc: 0.8538
loss: 0.3817, acc: 0.8541
> val_acc: 0.8273, val_f1: 0.8197
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
loss: 0.3457, acc: 0.8500
loss: 0.2915, acc: 0.8812
loss: 0.3045, acc: 0.8708
loss: 0.3076, acc: 0.8781
loss: 0.3127, acc: 0.8800
loss: 0.3142, acc: 0.8802
loss: 0.3230, acc: 0.8741
loss: 0.3065, acc: 0.8812
loss: 0.3227, acc: 0.8771
loss: 0.3271, acc: 0.8762
loss: 0.3245, acc: 0.8761
loss: 0.3174, acc: 0.8792
loss: 0.3224, acc: 0.8779
loss: 0.3219, acc: 0.8781
loss: 0.3132, acc: 0.8821
loss: 0.3093, acc: 0.8828
loss: 0.3190, acc: 0.8801
loss: 0.3166, acc: 0.8799
loss: 0.3203, acc: 0.8786
loss: 0.3232, acc: 0.8778
loss: 0.3240, acc: 0.8765
loss: 0.3252, acc: 0.8764
loss: 0.3277, acc: 0.8758
loss: 0.3318, acc: 0.8745
loss: 0.3374, acc: 0.8718
loss: 0.3374, acc: 0.8724
loss: 0.3402, acc: 0.8708
loss: 0.3427, acc: 0.8696
loss: 0.3417, acc: 0.8705
loss: 0.3411, acc: 0.8704
loss: 0.3417, acc: 0.8704
loss: 0.3425, acc: 0.8693
loss: 0.3455, acc: 0.8686
loss: 0.3460, acc: 0.8680
loss: 0.3448, acc: 0.8688
loss: 0.3473, acc: 0.8679
loss: 0.3464, acc: 0.8681
loss: 0.3438, acc: 0.8694
loss: 0.3435, acc: 0.8697
loss: 0.3451, acc: 0.8684
loss: 0.3450, acc: 0.8686
loss: 0.3441, acc: 0.8680
loss: 0.3425, acc: 0.8689
loss: 0.3431, acc: 0.8689
loss: 0.3449, acc: 0.8681
loss: 0.3462, acc: 0.8671
loss: 0.3463, acc: 0.8670
loss: 0.3461, acc: 0.8668
loss: 0.3466, acc: 0.8666
loss: 0.3454, acc: 0.8671
loss: 0.3460, acc: 0.8673
loss: 0.3462, acc: 0.8669
loss: 0.3470, acc: 0.8662
loss: 0.3470, acc: 0.8660
loss: 0.3477, acc: 0.8659
loss: 0.3477, acc: 0.8660
loss: 0.3470, acc: 0.8668
loss: 0.3464, acc: 0.8671
loss: 0.3463, acc: 0.8671
loss: 0.3471, acc: 0.8668
loss: 0.3482, acc: 0.8666
loss: 0.3484, acc: 0.8663
loss: 0.3478, acc: 0.8668
loss: 0.3469, acc: 0.8669
loss: 0.3466, acc: 0.8670
loss: 0.3468, acc: 0.8668
loss: 0.3465, acc: 0.8666
loss: 0.3454, acc: 0.8671
loss: 0.3465, acc: 0.8669
loss: 0.3465, acc: 0.8668
> val_acc: 0.8168, val_f1: 0.8060
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
loss: 0.3838, acc: 0.8063
loss: 0.4205, acc: 0.8125
loss: 0.3745, acc: 0.8458
loss: 0.3814, acc: 0.8484
loss: 0.3629, acc: 0.8625
loss: 0.3606, acc: 0.8594
loss: 0.3614, acc: 0.8589
loss: 0.3634, acc: 0.8586
loss: 0.3642, acc: 0.8604
loss: 0.3589, acc: 0.8631
loss: 0.3563, acc: 0.8653
loss: 0.3539, acc: 0.8661
loss: 0.3507, acc: 0.8668
loss: 0.3466, acc: 0.8679
loss: 0.3473, acc: 0.8679
loss: 0.3418, acc: 0.8711
loss: 0.3477, acc: 0.8684
loss: 0.3460, acc: 0.8701
loss: 0.3476, acc: 0.8688
loss: 0.3447, acc: 0.8716
loss: 0.3374, acc: 0.8738
loss: 0.3367, acc: 0.8741
loss: 0.3395, acc: 0.8745
loss: 0.3432, acc: 0.8740
loss: 0.3429, acc: 0.8740
loss: 0.3485, acc: 0.8726
loss: 0.3479, acc: 0.8727
loss: 0.3471, acc: 0.8737
loss: 0.3489, acc: 0.8728
loss: 0.3513, acc: 0.8719
loss: 0.3515, acc: 0.8724
loss: 0.3512, acc: 0.8730
loss: 0.3492, acc: 0.8737
loss: 0.3465, acc: 0.8735
loss: 0.3436, acc: 0.8741
loss: 0.3419, acc: 0.8748
loss: 0.3379, acc: 0.8764
loss: 0.3365, acc: 0.8775
loss: 0.3364, acc: 0.8772
loss: 0.3368, acc: 0.8770
loss: 0.3355, acc: 0.8770
loss: 0.3327, acc: 0.8777
loss: 0.3345, acc: 0.8770
loss: 0.3355, acc: 0.8760
loss: 0.3374, acc: 0.8757
loss: 0.3382, acc: 0.8754
loss: 0.3371, acc: 0.8762
loss: 0.3391, acc: 0.8749
loss: 0.3414, acc: 0.8737
loss: 0.3428, acc: 0.8730
loss: 0.3441, acc: 0.8724
loss: 0.3464, acc: 0.8714
loss: 0.3473, acc: 0.8712
loss: 0.3476, acc: 0.8712
loss: 0.3461, acc: 0.8719
loss: 0.3467, acc: 0.8717
loss: 0.3454, acc: 0.8719
loss: 0.3452, acc: 0.8712
loss: 0.3449, acc: 0.8715
loss: 0.3461, acc: 0.8710
loss: 0.3468, acc: 0.8704
loss: 0.3474, acc: 0.8698
loss: 0.3479, acc: 0.8697
loss: 0.3481, acc: 0.8698
loss: 0.3480, acc: 0.8694
loss: 0.3475, acc: 0.8695
loss: 0.3465, acc: 0.8699
loss: 0.3463, acc: 0.8700
loss: 0.3462, acc: 0.8699
loss: 0.3472, acc: 0.8693
> val_acc: 0.8108, val_f1: 0.8047
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
loss: 0.3306, acc: 0.8500
loss: 0.3058, acc: 0.8844
loss: 0.2992, acc: 0.8812
loss: 0.3285, acc: 0.8766
loss: 0.3299, acc: 0.8738
loss: 0.3200, acc: 0.8792
loss: 0.3160, acc: 0.8768
loss: 0.3072, acc: 0.8820
loss: 0.3121, acc: 0.8812
loss: 0.3084, acc: 0.8844
loss: 0.3124, acc: 0.8841
loss: 0.3091, acc: 0.8849
loss: 0.3157, acc: 0.8832
loss: 0.3143, acc: 0.8817
loss: 0.3091, acc: 0.8850
loss: 0.3213, acc: 0.8789
loss: 0.3248, acc: 0.8783
loss: 0.3242, acc: 0.8788
loss: 0.3232, acc: 0.8780
loss: 0.3241, acc: 0.8778
loss: 0.3210, acc: 0.8798
loss: 0.3229, acc: 0.8795
loss: 0.3260, acc: 0.8791
loss: 0.3280, acc: 0.8773
loss: 0.3328, acc: 0.8760
loss: 0.3317, acc: 0.8769
loss: 0.3349, acc: 0.8755
loss: 0.3361, acc: 0.8752
loss: 0.3364, acc: 0.8746
loss: 0.3408, acc: 0.8725
loss: 0.3455, acc: 0.8712
loss: 0.3424, acc: 0.8723
loss: 0.3411, acc: 0.8725
loss: 0.3443, acc: 0.8710
loss: 0.3479, acc: 0.8695
loss: 0.3525, acc: 0.8672
loss: 0.3542, acc: 0.8660
loss: 0.3516, acc: 0.8671
loss: 0.3515, acc: 0.8673
loss: 0.3497, acc: 0.8684
loss: 0.3478, acc: 0.8686
loss: 0.3483, acc: 0.8683
loss: 0.3504, acc: 0.8674
loss: 0.3516, acc: 0.8669
loss: 0.3511, acc: 0.8665
loss: 0.3494, acc: 0.8674
loss: 0.3508, acc: 0.8670
loss: 0.3514, acc: 0.8663
loss: 0.3489, acc: 0.8673
loss: 0.3494, acc: 0.8672
loss: 0.3498, acc: 0.8674
loss: 0.3508, acc: 0.8668
loss: 0.3513, acc: 0.8663
loss: 0.3517, acc: 0.8660
loss: 0.3510, acc: 0.8660
loss: 0.3524, acc: 0.8655
loss: 0.3529, acc: 0.8649
loss: 0.3517, acc: 0.8658
loss: 0.3508, acc: 0.8660
loss: 0.3498, acc: 0.8665
loss: 0.3498, acc: 0.8668
loss: 0.3493, acc: 0.8670
loss: 0.3492, acc: 0.8672
loss: 0.3483, acc: 0.8670
loss: 0.3472, acc: 0.8671
loss: 0.3502, acc: 0.8660
loss: 0.3500, acc: 0.8662
loss: 0.3503, acc: 0.8661
loss: 0.3490, acc: 0.8666
loss: 0.3481, acc: 0.8671
> val_acc: 0.8191, val_f1: 0.8136
>> early stop.
Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
>> test_acc: 0.8443, test_f1: 0.8403
>> test_acc: 0.8413, test_f1: 0.8357
>> test_acc: 0.8353, test_f1: 0.8280
>> test_acc: 0.8443, test_f1: 0.8403

>> avg_test_acc: 0.8403, avg_test_f1: 0.8347
>> max_test_acc: 0.8443, max_test_f1: 0.8403
